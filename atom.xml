<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>演好自己的戏，走好自己的路</title>
  
  <subtitle>花生人的苟且人生</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://jungler.com/"/>
  <updated>2022-06-23T10:50:18.877Z</updated>
  <id>http://jungler.com/</id>
  
  <author>
    <name>Dave Chen</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Kubernetes - metric server简介</title>
    <link href="http://jungler.com/2022/06/16/Kubernetes-metric-server%E7%AE%80%E4%BB%8B/"/>
    <id>http://jungler.com/2022/06/16/Kubernetes-metric-server简介/</id>
    <published>2022-06-16T08:24:28.000Z</published>
    <updated>2022-06-23T10:50:18.877Z</updated>
    
    <content type="html"><![CDATA[<p>关于metric server的一些随笔。</p><p>总得说来，metric server可以获取node和pod的使用了多少CPU或者memory的资源，其底层实现是通过cadvisor调用了runc的接口来读取例如CPU和memory的使用信息。</p><p>官网上的一个metric资源访问流。<br><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/resource-pipeline.png" alt=""></p><p><strong>安装</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml</span><br></pre></td></tr></table></figure><p>在测试场景下，我们还需要启动metric server的启动命令行上加上 <code>kubelet-insecure-tls</code>参数以表示不需要对kubelet的证书进行校验。</p><p><strong>验证</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root@a010735:~<span class="comment"># kubectl top nodes</span></span><br><span class="line">NAME      CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%</span><br><span class="line">a010735   372m         4%     6359Mi          40%</span><br><span class="line">root@a010735:~<span class="comment"># kubectl top pods</span></span><br><span class="line">NAME                          CPU(cores)   MEMORY(bytes)</span><br><span class="line">init-demo                     0m           1Mi</span><br><span class="line">php-apache-6db6dccd7f-569pp   1m           14Mi</span><br></pre></td></tr></table></figure><p><strong>源码分析（基于commit ID: bb893870b2）</strong>：</p><ul><li>API 注册：<br>apiserver启动后会注册metric server的endpoints, 项目地址为：<a href="https://github.com/kubernetes-sigs/metrics-server" target="_blank" rel="noopener">https://github.com/kubernetes-sigs/metrics-server</a>.</li></ul><p>对应的代码实现：<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//metrics-server/pkg/api/install.go</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Install builds the metrics for the metrics.k8s.io API, and then installs it into the given API metrics-server.</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">Install</span><span class="params">(m MetricsGetter, podMetadataLister cache.GenericLister, nodeLister corev1.NodeLister, server *genericapiserver.GenericAPIServer)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">node := newNodeMetrics(metrics.Resource(<span class="string">"nodemetrics"</span>), m, nodeLister)</span><br><span class="line">pod := newPodMetrics(metrics.Resource(<span class="string">"podmetrics"</span>), m, podMetadataLister)</span><br><span class="line">info := Build(pod, node)</span><br><span class="line"><span class="keyword">return</span> server.InstallAPIGroup(&amp;info)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>访问的API具体地址：</p><pre><code>- 访问某一个pod的metric数据</code></pre><p><a href="https://127.0.0.1:6443/apis/metrics.k8s.io/v1beta1/namespaces/default/pods/demo" target="_blank" rel="noopener">https://127.0.0.1:6443/apis/metrics.k8s.io/v1beta1/namespaces/default/pods/demo</a></p><pre><code>- 访问所有pods的metric数据</code></pre><p><a href="https://127.0.0.1:6443/apis/metrics.k8s.io/v1beta1/namespaces/default/pods" target="_blank" rel="noopener">https://127.0.0.1:6443/apis/metrics.k8s.io/v1beta1/namespaces/default/pods</a></p><pre><code>- 访问某一个node的metric数据</code></pre><p><a href="https://127.0.0.1:6443/apis/metrics.k8s.io/v1beta1/nodes/node1" target="_blank" rel="noopener">https://127.0.0.1:6443/apis/metrics.k8s.io/v1beta1/nodes/node1</a></p><pre><code>- 访问所有nodes的metric数据</code></pre><p><a href="https://127.0.0.1:6443/apis/metrics.k8s.io/v1beta1/nodes" target="_blank" rel="noopener">https://127.0.0.1:6443/apis/metrics.k8s.io/v1beta1/nodes</a></p><blockquote><p><strong><em>NOTE:</em></strong> API的定义另一个项目里：<a href="https://github.com/kubernetes/metrics" target="_blank" rel="noopener">https://github.com/kubernetes/metrics</a>, metric-server相当于实现了该项目定义的API。</p></blockquote><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//pkg/apis/metrics/register.go</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// GroupName is the group name use in this package</span></span><br><span class="line"><span class="keyword">const</span> GroupName = <span class="string">"metrics.k8s.io"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// SchemeGroupVersion is group version used to register these objects</span></span><br><span class="line"><span class="keyword">var</span> SchemeGroupVersion = schema.GroupVersion&#123;Group: GroupName, Version: runtime.APIVersionInternal&#125;</span><br></pre></td></tr></table></figure><ul><li>数据存储<br>metric server用来存储node和pod的metric数据的数据结构：<figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//metrics-server/pkg/storage/storage.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> storage <span class="keyword">struct</span> &#123;</span><br><span class="line">mu    sync.RWMutex</span><br><span class="line">pods  podStorage</span><br><span class="line">nodes nodeStorage</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>后面通过runc的接口获取到的数据会存储到这个数据结构中，并通过计算得到当前node或者pod的资源利用情况。metric server的定时任务每隔一定时间会调用scrape和store接口来刷新数据。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//metrics-server/pkg/server/server.go</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *server)</span> <span class="title">tick</span><span class="params">(ctx context.Context, startTime time.Time)</span></span> &#123;</span><br><span class="line">s.tickStatusMux.Lock()</span><br><span class="line">s.tickLastStart = startTime</span><br><span class="line">s.tickStatusMux.Unlock()</span><br><span class="line"></span><br><span class="line">ctx, cancelTimeout := context.WithTimeout(ctx, s.resolution)</span><br><span class="line"><span class="keyword">defer</span> cancelTimeout()</span><br><span class="line"></span><br><span class="line">klog.V(<span class="number">6</span>).InfoS(<span class="string">"Scraping metrics"</span>)</span><br><span class="line">data := s.scraper.Scrape(ctx)</span><br><span class="line"></span><br><span class="line">klog.V(<span class="number">6</span>).InfoS(<span class="string">"Storing metrics"</span>)</span><br><span class="line">s.storage.Store(data)</span><br><span class="line"></span><br><span class="line">collectTime := time.Since(startTime)</span><br><span class="line">tickDuration.Observe(<span class="keyword">float64</span>(collectTime) / <span class="keyword">float64</span>(time.Second))</span><br><span class="line">klog.V(<span class="number">6</span>).InfoS(<span class="string">"Scraping cycle complete"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>原始数据的收集的核心调用是通过<code>func (c *scraper) collectNode(ctx context.Context, node *corev1.Node) (*storage.MetricsBatch, error)</code>方法来获取数据并存入<code>storage.MetricsBatch</code>数据结构中。</p><p><code><br>// metrics-server/pkg/scraper/scraper.go</code></p><p>func (c <em>scraper) Scrape(baseCtx context.Context) </em>storage.MetricsBatch {<br>    …<br>    for _, node := range nodes {<br>        go func(node <em>corev1.Node) {<br>            // Prevents network congestion.<br>            sleepDuration := time.Duration(rand.Intn(delayMs)) </em> time.Millisecond<br>            time.Sleep(sleepDuration)<br>            // make the timeout a bit shorter to account for staggering, so we still preserve<br>            // the overall timeout<br>            ctx, cancelTimeout := context.WithTimeout(baseCtx, c.scrapeTimeout-sleepDuration)<br>            defer cancelTimeout()<br>            klog.V(2).InfoS(“Scraping node”, “node”, klog.KObj(node))<br>            <mark>m, err := c.collectNode(ctx, node)</mark><br>            if err != nil {<br>                klog.ErrorS(err, “Failed to scrape node”, “node”, klog.KObj(node))<br>            }<br>            responseChannel &lt;- m<br>        }(node)<br>    }<br>    …<br>}<br></p><p>这里<code>collectNode</code>方法是通过kubelet连接到各个不同的node，通过<code>/metrics/resource</code> URI来读取数据。</p><p><code><br>// metrics-server/pkg/scraper/client/resource/client.go</code></p><p>func (kc <em>kubeletClient) GetMetrics(ctx context.Context, node </em>corev1.Node) (*storage.MetricsBatch, error) {<br>    port := kc.defaultPort<br>    nodeStatusPort := int(node.Status.DaemonEndpoints.KubeletEndpoint.Port)<br>    …<br>    url := url.URL{<br>        Scheme: kc.scheme,<br>        Host:   net.JoinHostPort(addr, strconv.Itoa(port)),<br>        <strong>Path:   “/metrics/resource”,</strong><br>    }<br>    …<br>    return kc.getMetrics(ctx, url.String(), node.Name)<br>}<br></p><p><code>/metrics/resource</code>这个URI对应的处理逻辑在kubelet中实现。</p><p><code><br>//k8s.io/kubernetes/pkg/kubelet/server/server.go</code></p><p>const resourceMetricsPath = “/metrics/resource”<br>func (s *Server) InstallDefaultHandlers() {<br>        …<br>    s.addMetricsBucketMatcher(“metrics/resource”)<br>    resourceRegistry := compbasemetrics.NewKubeRegistry()<br>    resourceRegistry.CustomMustRegister(collectors.<mark>NewResourceMetricsCollector</mark>(s.resourceAnalyzer))<br>    s.restfulCont.Handle(resourceMetricsPath,<br>        compbasemetrics.HandlerFor(resourceRegistry, compbasemetrics.HandlerOpts{ErrorHandling: compbasemetrics.ContinueOnError}),<br>    )<br>    …<br>}<br></p><p><code>NewResourceMetricsCollector</code>返回<code>metrics.StableCollector</code>接口，metric主要通过实现接口中方法<code>CollectWithStability</code>来收集数据。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// k8s.io/kubernetes/pkg/kubelet/metrics/collectors/resource_metrics.go</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(rc *resourceMetricsCollector)</span> <span class="title">CollectWithStability</span><span class="params">(ch <span class="keyword">chan</span>&lt;- metrics.Metric)</span></span> &#123;</span><br><span class="line">...</span><br><span class="line">statsSummary, err := rc.provider.GetCPUAndMemoryStats()  <span class="comment">// 1)</span></span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">errorCount = <span class="number">1</span></span><br><span class="line">klog.ErrorS(err, <span class="string">"Error getting summary for resourceMetric prometheus endpoint"</span>)</span><br><span class="line"><span class="keyword">return</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">rc.collectNodeCPUMetrics(ch, statsSummary.Node) <span class="comment">// 2)</span></span><br><span class="line">rc.collectNodeMemoryMetrics(ch, statsSummary.Node) <span class="comment">// 3)</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> _, pod := <span class="keyword">range</span> statsSummary.Pods &#123;</span><br><span class="line"><span class="keyword">for</span> _, container := <span class="keyword">range</span> pod.Containers &#123;</span><br><span class="line">rc.collectContainerStartTime(ch, pod, container)</span><br><span class="line">rc.collectContainerCPUMetrics(ch, pod, container)</span><br><span class="line">rc.collectContainerMemoryMetrics(ch, pod, container)</span><br><span class="line">&#125;</span><br><span class="line">rc.collectPodCPUMetrics(ch, pod) <span class="comment">// 4)</span></span><br><span class="line">rc.collectPodMemoryMetrics(ch, pod) <span class="comment">// 5)</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>statsSummary, err := rc.provider.GetCPUAndMemoryStats() // 通过kubelet调用cadvisor方法读取CPU和memory的统计信息，包括node和active的容器。</li><li>rc.collectNodeCPUMetrics(ch, statsSummary.Node) // 生成cpu的metric，计算方法：float64(*s.CPU.UsageCoreNanoSeconds)/float64(time.Second)</li><li>rc.collectNodeMemoryMetrics(ch, statsSummary.Node) // 生成memory的metric，计算方法：float64(*s.Memory.WorkingSetBytes)</li><li>rc.collectPodCPUMetrics(ch, pod) // 生成pod的CPU metric，计算方法：float64(*pod.CPU.UsageCoreNanoSeconds)/float64(time.Second)</li><li>rc.collectPodMemoryMetrics(ch, pod) // 生成pod的memory metric，计算方法：float64(*pod.Memory.WorkingSetBytes) 对于pod上的每个容器也一并计算生成容器的开始时间以及CPU和memory的使用情况。</li></ol><p>继续深入看一下<code>statsSummary, err := rc.provider.GetCPUAndMemoryStats()</code>这个方法调用。总的说来，是通过调用cadvisor（cadvisor已经和kubelet集成）并最终调用runc的cgroup接口来读入数据。</p><p>cadvisor将数据存放在cache中，这部分在kubelet中实现，</p><p><code><br>// k8s.io/kubernetes/pkg/kubelet/stats/provider.go</code></p><p>func getCgroupInfo(cadvisor cadvisor.Interface, containerName string, updateStats bool) (*cadvisorapiv2.ContainerInfo, error) {<br>    …<br>    infoMap, err := <mark>cadvisor.ContainerInfoV2</mark>(containerName, cadvisorapiv2.RequestOptions{<br>        IdType:    cadvisorapiv2.TypeName,<br>        Count:     2, // 2 samples are needed to compute “instantaneous” CPU<br>        Recursive: false,<br>        MaxAge:    maxAge,<br>    })<br>    …<br>}<br></p><p>读取cache，开始和结束时间都置为空，实际上是读取了所有的数据。<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// github.com/google/cadvisor/manager/manager.go</span></span><br><span class="line"></span><br><span class="line">stats, err := m.memoryCache.RecentStats(name, nilTime, nilTime, options.Count)</span><br></pre></td></tr></table></figure></p><ul><li>Cadvisor与runc</li></ul><p>Cadvisor也启动了一个定时任务，每隔一段时间会将最新的数据刷到内存中。<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// github.com/google/cadvisor/manager/container.go</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(cd *containerData)</span> <span class="title">housekeepingTick</span><span class="params">(timer &lt;-<span class="keyword">chan</span> time.Time, longHousekeeping time.Duration)</span> <span class="title">bool</span></span></span><br></pre></td></tr></table></figure></p><p>可以去看看数据是如何从cgroup接口获取的。<br><code><br>func (cd *containerData) updateStats() error {<br>    stats, statsErr := <mark>cd.handler.GetStats()</mark><br>    …<br>    perfStatsErr := cd.perfCollector.UpdateStats(stats)</code></p><pre><code>resctrlStatsErr := cd.resctrlCollector.UpdateStats(stats)        ...err = cd.memoryCache.AddStats(&amp;cInfo, stats)</code></pre><p>}<br></p><p>这里handler根据底层runtime的不同调用有不同的具体实现，例如<code>containerd</code>或者<code>crio</code>，但是最后都会调入到<code>libcontainer</code>中去。</p><p><code><br>// github.com/google/cadvisor/container/libcontainer/handler.go</code></p><p>func (h <em>Handler) GetStats() (</em>info.ContainerStats, error) {<br>    …<br>    <mark>cgroupStats, err := h.cgroupManager.GetStats()</mark><br>    if err != nil {<br>        if !ignoreStatsError {<br>            return nil, err<br>        }<br>        klog.V(4).Infof(“Ignoring errors when gathering stats for root cgroup since some controllers don’t have stats on the root cgroup: %v”, err)<br>    }<br>    …<br>}<br><code></code></p><p>可以打印出<code>h.cgroupManager</code>的path可以发现就本质就是cgroup的各个子系统。Cgroup manager可以是systemd，fs或者fs2等，以fs为例。<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">paths: <span class="keyword">map</span>[<span class="keyword">string</span>]<span class="keyword">string</span> [</span><br><span class="line">        <span class="string">"cpu"</span>: <span class="string">"/sys/fs/cgroup/cpu,cpuacct"</span>,</span><br><span class="line">        <span class="string">"memory"</span>: <span class="string">"/sys/fs/cgroup/memory"</span>,</span><br><span class="line">        <span class="string">"cpuacct"</span>: <span class="string">"/sys/fs/cgroup/cpu,cpuacct"</span>,</span><br><span class="line">        <span class="string">"blkio"</span>: <span class="string">"/sys/fs/cgroup/blkio"</span>,</span><br><span class="line">        <span class="string">"devices"</span>: <span class="string">"/sys/fs/cgroup/devices"</span>,</span><br><span class="line">]</span><br></pre></td></tr></table></figure></p><p>libcontainer调用各个子系统的GetStats方法得到数据。<br><code><br>// opencontainers/runc/libcontainer/cgroups/fs/fs.go</code></p><p>func (m <em>manager) GetStats() (</em>cgroups.Stats, error) {<br>    m.mu.Lock()<br>    defer m.mu.Unlock()<br>    <mark>stats := cgroups.NewStats()</mark><br>    for _, sys := range subsystems {<br>        path := m.paths[sys.Name()]<br>        if path == “” {<br>            continue<br>        }<br>        if err := sys.GetStats(path, stats); err != nil {<br>            return nil, err<br>        }<br>    }<br>    return stats, nil<br>}<br></p><p>看看cpuacct是如何读取的数据，可以简单的理解为文件读取并做了一定的处理。<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//opencontainers/runc/libcontainer/cgroups/fs/cpuacct.go</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *CpuacctGroup)</span> <span class="title">GetStats</span><span class="params">(path <span class="keyword">string</span>, stats *cgroups.Stats)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">...</span><br><span class="line">totalUsage, err := fscommon.GetCgroupParamUint(path, <span class="string">"cpuacct.usage"</span>)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">percpuUsage, err := getPercpuUsage(path)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>比方说我们可以在系统上读取<code>cpuacct.usage</code>或者<code>cpuacct.usage_percpu</code>。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/sys/fs/cgroup/cpu,cpuacct<span class="comment"># cat cpuacct.usage</span></span><br><span class="line">5842504834476978</span><br></pre></td></tr></table></figure></p><ul><li>Metric 与 kubectl<br>当一个CLI的命令例如：<code>kubectl top node</code>发出之后，首先进入的入口定义在这里，<br><code><br>// k8s.io/kubectl/pkg/cmd/top/top.go</code></li></ul><p>func NewCmdTop(f cmdutil.Factory, streams genericclioptions.IOStreams) *cobra.Command {<br>    cmd := &amp;cobra.Command{<br>        Use:   “top”,<br>        Short: i18n.T(“Display resource (CPU/memory) usage”),<br>        Long:  topLong,<br>        Run:   cmdutil.DefaultSubCommandRun(streams.ErrOut),<br>    }</p><pre><code>// create subcommands&lt;mark&gt;cmd.AddCommand(NewCmdTopNode(f, nil, streams))&lt;/mark&gt;&lt;mark&gt;cmd.AddCommand(NewCmdTopPod(f, nil, streams))&lt;/mark&gt;return cmd</code></pre><p>}<br><code></code></p><p>调用api接口获取数据然后打印数据到终端，<br><code><br>// k8s.io/kubectl/pkg/metricsutil/metrics_printer.go</code></p><p>func printMetricsLine(out io.Writer, metrics *ResourceMetricsInfo) {<br>    printValue(out, metrics.Name)<br>    <mark>printAllResourceUsages(out, metrics)</mark><br>    fmt.Fprint(out, “\n”)<br>}<br></p><p>实际上就是计算了使用的资源并和机器上的可用资源相除取整并打印输出。<br><code><br>func printAllResourceUsages(out io.Writer, metrics <em>ResourceMetricsInfo) {<br>    for _, res := range MeasuredResources {<br>        quantity := metrics.Metrics[res]<br>        printSingleResourceUsage(out, res, quantity)<br>        fmt.Fprint(out, “\t”)<br>        if available, found := metrics.Available[res]; found {<br>            <mark>fraction := float64(quantity.MilliValue()) / float64(available.MilliValue()) </mark></em> 100<br>            fmt.Fprintf(out, “%d%%\t”, int64(fraction))<br>        }<br>    }<br>}<br></code></p><p><strong>Metric server的应用场景</strong></p><ul><li><p><a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough" target="_blank" rel="noopener">HPA (Horizontal Pod Autoscaler)</a>: 主要是解决当某种类型的workload对象占用资源超过一定数量之后需要增加更多的pod以缓解client端访问的压力。</p></li><li><p><a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler" target="_blank" rel="noopener">VPA(Vertical Pod Autoscaler)</a>：解决当pod的request的资源小于实际pod需要的资源数量之后，需要增加pod的request数量应该采取的措施。例如重新定义request的值，并创建一个新的pod。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;关于metric server的一些随笔。&lt;/p&gt;
&lt;p&gt;总得说来，metric server可以获取node和pod的使用了多少CPU或者memory的资源，其底层实现是通过cadvisor调用了runc的接口来读取例如CPU和memory的使用信息。&lt;/p&gt;
&lt;p&gt;官网
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="http://jungler.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes - HA and LB cluster setup</title>
    <link href="http://jungler.com/2021/11/09/Kubernetes-HA-and-LB-cluster-setup/"/>
    <id>http://jungler.com/2021/11/09/Kubernetes-HA-and-LB-cluster-setup/</id>
    <published>2021-11-09T03:07:45.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p>分布式系统的HA和LB一直是一个关键性技术点，Kubernetes也有许多方面的考虑和实现来支持HA/LB，比方说lease，网络流量方面的LB（kube-proxy），以及在整体架构上与业界比较通用的haproxy与keepalived解决方案的集成，其它第三方的项目包括kube-vip, nginx-ingress等。</p><p>本文记录一下在与haproxy以及keepalived做集成的一些关键性的问题和配置。一个最简单的实验需要至少3个控制节点外加一个计算节点。</p><p>考虑HA/LB主要考虑的是控制面上的一些服务，例如kube-apiserver，etcd，kube-scheduler以及kube-controller-manager。</p><ul><li>kube-apiserver是整个cluster的入口，各种状态查询，pod的CRUD都需要通过apiserver来接入，所以需要重点考虑，在我们的实验中我们引入haproxy对apiserver做负载均衡，在引入keepalived对haproxy做HA。</li><li>kube-scheduler 与kube-controller-manager相对于apiserver来说压力较小，例如scheduler只需要对pod的创建做出响应，所以直接采用lease机制即可。</li><li>etcd支持创建一个etcd的cluster，可以直接调用etcd的CLI添加一些新的member。etcd可以是和控制节点集成在一起（stacked etcd cluster）也可以独立在其它的节点上(external etcd cluster)，参考官方文档 - <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/" target="_blank" rel="noopener">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/</a></li></ul><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/kubeadm-ha-topology-stacked-etcd.svg" alt="ha-topology-stacked-etcd"></p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/kubeadm-ha-topology-external-etcd.svg" alt="ha-topology-external-etcd"></p><h2 id="lease机制"><a href="#lease机制" class="headerlink" title="lease机制"></a>lease机制</h2><p>首先来看看lease机制，或者说是leader-election机制，<a href="https://github.com/kubernetes/client-go/blob/master/examples/leader-election/main.go" target="_blank" rel="noopener">client-go例子</a>.<br>其本质是在竞争创建一个LeaseLock的object, 服务可以在多台机器上同时启动，但最终只有一个服务能成功抢占并创建这个lease对象。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// https://github.com/kubernetes/client-go/blob/9b0b23a8ade2b5323d6624146cea2ad7b8928f25/tools/leaderelection/leaderelection.go#L327-L341</span></span><br><span class="line"></span><br><span class="line">oldLeaderElectionRecord, oldLeaderElectionRawRecord, err := le.config.Lock.Get(ctx)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">if</span> !errors.IsNotFound(err) &#123;</span><br><span class="line">klog.Errorf(<span class="string">"error retrieving resource lock %v: %v"</span>, le.config.Lock.Describe(), err)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> err = le.config.Lock.Create(ctx, leaderElectionRecord); err != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Errorf(<span class="string">"error initially creating leader election record: %v"</span>, err)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">le.setObservedRecord(&amp;leaderElectionRecord)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>再来看看scheduler的相关代码，</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// go/src/k8s.io/kubernetes/cmd/kube-scheduler/app/server.go</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> cc.LeaderElection != <span class="literal">nil</span> &#123;</span><br><span class="line">cc.LeaderElection.Callbacks = leaderelection.LeaderCallbacks&#123;</span><br><span class="line">OnStartedLeading: <span class="function"><span class="keyword">func</span><span class="params">(ctx context.Context)</span></span> &#123;</span><br><span class="line"><span class="built_in">close</span>(waitingForLeader)</span><br><span class="line">sched.Run(ctx)</span><br><span class="line">&#125;,</span><br><span class="line">OnStoppedLeading: <span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123;</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> &lt;-ctx.Done():</span><br><span class="line"><span class="comment">// We were asked to terminate. Exit 0.</span></span><br><span class="line">klog.InfoS(<span class="string">"Requested to terminate, exiting"</span>)</span><br><span class="line">os.Exit(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">default</span>:</span><br><span class="line"><span class="comment">// We lost the lock.</span></span><br><span class="line">klog.Exitf(<span class="string">"leaderelection lost"</span>)</span><br><span class="line">&#125;</span><br><span class="line">&#125;,</span><br><span class="line">&#125;</span><br><span class="line">leaderElector, err := leaderelection.NewLeaderElector(*cc.LeaderElection)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">"couldn't create leader elector: %v"</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">leaderElector.Run(ctx)</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">"lost lease"</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>而<code>leaderElector.Run(ctx)</code>最后也是一样调用<code>le.acquire(ctx)</code>来尝试创建lease对象。</p><p>在多个scheduler或者controller manager的情况下，如何判断那个节点的服务是生效的？</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubectl get lease -A</span><br><span class="line"></span><br><span class="line">NAMESPACE          NAME                     HOLDER                                         AGE</span><br><span class="line">kube-node-lease   master1                   master1                                        13d</span><br><span class="line">kube-node-lease   master2                   master2                                        13d</span><br><span class="line">kube-node-lease   master3                   master3                                        13d</span><br><span class="line">kube-system       kube-controller-manager   master3_cb2b887c-71b4-4fca-9fea-9bab10279502   13d</span><br><span class="line">kube-system       kube-scheduler            master2_a741b6df-5833-4dc5-bf5d-6755709cbe1e   13d</span><br></pre></td></tr></table></figure><p>可以看到获取到lease的kube-scheduler是跑在master2这个节点上的。kube-controller-manager  是master3这个节点上的。</p><p>apiserver的HA/LB与haproxy和keepalived的集成主要是一些服务配置，</p><ul><li>haproxy需要在每一个controller上面安装，目的是一个机器上的haproxy宕掉之后，可以结合keepalived让其它节点上的haproxy继续工作。</li><li>keepalived需要在每一个controller上面安装，keepalived的目的是提供一个VIP，或者说是一个浮动IP。</li><li>以kubeadm的安装为例，首个节点需要control-plane-endpoint来指定控制节点的endpoint，即VIP+haproxy bind的端口。首个节点部署成功的时候会输出join其它控制节点命令。</li><li>其它控制节点在加入cluster时指定的endpoint不再是某个node上的物理IP，而是VIP+haproxy bind。这样只要保证VIP是可以联通的，某个节点到宕掉不影响整个cluster的工作。</li><li>加入其它结点之前需要先同步密钥和证书，这部分现在是手动做的，个人觉得上游社区是不会接受将其自动化处理的，因为这步操作会修改文件系统上的文件，上游应该会觉得这部分不应该归他们管吧。这部分文件包括（ca.crt，ca.key，sa.key，sa.pub，front-proxy-ca.crt，front-proxy-ca.key，etcd/ca.crt，etcd/ca.key） </li></ul><h2 id="haproxy与keepalived配置"><a href="#haproxy与keepalived配置" class="headerlink" title="haproxy与keepalived配置"></a>haproxy与keepalived配置</h2><p>下面列出haproxy以及keepalived的配置，每个节点上的配置保持相同，并对需要注意的点加以说明，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">/etc/haproxy/haproxy.cfg(see: https://github.com/chendave/initrepo/tree/master/ha)</span><br><span class="line"></span><br><span class="line">frontend kubernetes-apiserver</span><br><span class="line">    bind *:16443  [1]</span><br><span class="line">    mode tcp    [2]</span><br><span class="line">    option tcplog</span><br><span class="line">    default_backend kubernetes-apiserver</span><br><span class="line"></span><br><span class="line">backend kubernetes-apiserver</span><br><span class="line">    mode tcp</span><br><span class="line">    balance roundrobin  [3] </span><br><span class="line">    server master 10.169.180.51:6443 check [4]</span><br><span class="line">    server master1 10.169.212.212:6443 check</span><br><span class="line">    server master2 10.169.182.17:6443 check</span><br></pre></td></tr></table></figure><p>[1] 这里可以绑定任何不冲突的端口，后面我们用kubeadm在bootstrap其它节点时用的都是这个端口。<br>[2] 这里需要走tcp协议而不能用http <a href="https://serverfault.com/questions/611272/haproxy-http-vs-tcp" target="_blank" rel="noopener">haproxy-http-vs-tcp</a> 有做解释，apiserver走的是https协议，所以如果用http的话会出错。<br>[3] 参考<a href="https://www.haproxy.org/download/2.5/doc/configuration.txt" target="_blank" rel="noopener">configuration</a> 文档的说明，现在有多达10个算法，在我的实验里用了最普通的roundrobin。<br>[4]  后端对应的apiserver应该是物理IP加6443，无论前端bind的port是多少，6443是一定会在每个机器上存在的，这也是能balance的一个关键所在，注意VIP是独立的和服务原生的IP以及端口不是替换的关系。</p><blockquote><pre><code>roundrobin  Each server is used in turns, according to their weights.            This is the smoothest and fairest algorithm when the server&apos;s            processing time remains equally distributed. This algorithm            is dynamic, which means that server weights may be adjusted            on the fly for slow starts for instance. It is limited by            design to 4095 active servers per backend. Note that in some            large farms, when a server becomes up after having been down            for a very short time, it may sometimes take a few hundreds            requests for it to be re-integrated into the farm and start            receiving traffic. This is normal, though very rare. It is            indicated here in case you would have the chance to observe            it, so that you don&apos;t worry.</code></pre></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># netstat -anp | grep  6443</span></span><br><span class="line">tcp        0      0 0.0.0.0:16443           0.0.0.0:*               LISTEN      13015/haproxy</span><br><span class="line">tcp        0      0 10.169.180.90:16443     10.169.212.212:48064    ESTABLISHED 13015/haproxy</span><br><span class="line">tcp6       0      0 :::6443                 :::*                    LISTEN      21206/kube-apiserve</span><br><span class="line">tcp6       0      0 10.169.180.51:6443      10.169.180.51:40086     ESTABLISHED 21206/kube-apiserve</span><br></pre></td></tr></table></figure><p>再来看看keepalived的配置，每个机器上的配置稍有不通，主要在于如何合理的调整权重，可以参考github上的配置（<a href="https://github.com/chendave/initrepo/tree/master/ha）。" target="_blank" rel="noopener">https://github.com/chendave/initrepo/tree/master/ha）。</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">! Configuration File for keepalived</span><br><span class="line"></span><br><span class="line">global_defs &#123;</span><br><span class="line">   router_id master1   [1]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_script chk_haproxy &#123;</span><br><span class="line">    script &quot;/bin/bash -c &apos;if [[ $(netstat -nlp | grep 16443) ]]; then exit 0; else exit 1; fi&apos;&quot;  # haproxy 检测 [2]</span><br><span class="line">    interval 2  # 每2秒执行一次检测</span><br><span class="line">    weight 20 # 权重变化 [3]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">vrrp_instance VI-kube-master &#123;</span><br><span class="line">    state MASTER  [4]</span><br><span class="line">    interface eth0   [5]</span><br><span class="line">    virtual_router_id 50</span><br><span class="line">    priority 100 [6]</span><br><span class="line">    unicast_src_ip 10.169.180.51 [6]</span><br><span class="line">    unicast_peer &#123;</span><br><span class="line">        10.169.212.212      [7] peers</span><br><span class="line">        10.169.182.17</span><br><span class="line">    &#125;</span><br><span class="line">    advert_int 1</span><br><span class="line">    authentication &#123;</span><br><span class="line">        auth_type PASS</span><br><span class="line">        auth_pass 1111</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    track_script &#123;</span><br><span class="line">        chk_haproxy</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    virtual_ipaddress &#123;</span><br><span class="line">        10.169.180.90/22</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>[1] 保证每个机器的<code>router_id</code>互不相同。<br>[2] 引入keepalived的目的是某一个haproxy挂了之后，能将VIP挂在其它节点上，所以这里加了一个脚本来检测haproxy是否还活着。<br>[3] weight和priority一起决定最后的VIP挂在哪个物理机上，算法可以查看keepalived的说明。<br>通过weight和priority的调整，来做到当一个机器上的haproxy服务停止之后，VIP可以在其它机器上次权重的机器上的挂起。<br>[4] 标志当前的状态，据说关系不大。<br>[5] VIP挂在哪个物理的nic之下。<br>[6] 本机的物理IP地址。<br>[7] 之所以使用unicast的原因是很多情况下multicast被禁，例如在工作场所，所以往往配置为multicast的时候，对端的节点无法获知对端是否已经绑定了VIP，结果就是网络里可能有多个VIP造成我们的实验失败。</p><p>在master1的节点上可以看到VIP已经被挂起，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">root@master1 <span class="comment"># ip a</span></span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000</span><br><span class="line">…</span><br><span class="line">    inet 10.169.180.90/22 scope global secondary eth0</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line"></span><br><span class="line"><span class="comment"># systemctl status keepalived.service</span></span><br><span class="line">10月 30 18:16:23 master1 Keepalived_vrrp[32078]: VRRP_Instance(VI-kube-master) forcing a new MASTER election</span><br><span class="line">10月 30 18:16:24 master1 Keepalived_vrrp[32078]: VRRP_Instance(VI-kube-master) Transition to MASTER STATE</span><br><span class="line">10月 30 18:16:25 master1 Keepalived_vrrp[32078]: VRRP_Instance(VI-kube-master) Entering MASTER STATE</span><br><span class="line"></span><br><span class="line"><span class="comment"># ping -c 5 10.169.180.90</span></span><br><span class="line">PING 10.169.180.90 (10.169.180.90) 56(84) bytes of data.</span><br><span class="line">64 bytes from 10.169.180.90: icmp_seq=1 ttl=62 time=0.235 ms</span><br><span class="line">64 bytes from 10.169.180.90: icmp_seq=2 ttl=62 time=0.178 ms</span><br><span class="line">64 bytes from 10.169.180.90: icmp_seq=3 ttl=62 time=0.192 ms</span><br><span class="line">64 bytes from 10.169.180.90: icmp_seq=4 ttl=62 time=0.185 ms</span><br><span class="line">64 bytes from 10.169.180.90: icmp_seq=5 ttl=62 time=0.193 ms</span><br><span class="line"></span><br><span class="line">root@master3:~<span class="comment"># systemctl status keepalived.service</span></span><br><span class="line">Nov 08 08:13:17 master3 Keepalived_vrrp[6165]: Using LinkWatch kernel netlink reflector...</span><br><span class="line">Nov 08 08:13:17 master3 Keepalived_vrrp[6165]: VRRP_Instance(VI-kube-master) Entering BACKUP STATE</span><br><span class="line">Nov 08 08:13:17 master3 Keepalived_vrrp[6165]: VRRP_Script(chk_haproxy) succeeded</span><br><span class="line">Nov 08 08:13:18 master3 Keepalived_vrrp[6165]: VRRP_Instance(VI-kube-master) Changing effective priority from 80 to 100</span><br></pre></td></tr></table></figure><h2 id="kubeadm-bootstrap"><a href="#kubeadm-bootstrap" class="headerlink" title="kubeadm bootstrap"></a>kubeadm bootstrap</h2><p>看看我们用kubeadm来创建各个节点的命令。apiserver-cert-extra-sans的目的是让CLI可以在其它机器上也可以运行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --control-plane-endpoint=<span class="string">"10.169.180.90:16443"</span> --pod-network-cidr=10.244.0.0/16 --apiserver-cert-extra-sans=master2,10.169.180.90,master3 --upload-certs</span><br></pre></td></tr></table></figure><p>加入其它controller节点，至少三个，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 10.169.180.90:16443 --token q1vi54.rqy34szr6xn56s0k \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:dac15eaa52a6c9fff179693aea175df23719e6cb09d5a9281a3104148b13ebfb \</span><br><span class="line">        --control-plane --certificate-key 1fb83358db692a50f9d0dd4c6658af8d46e2ee8f1921f37b180cce2ec3f0d51b</span><br></pre></td></tr></table></figure></p><p>加入至少一个worker节点，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 10.169.180.90:16443 --token q1vi54.rqy34szr6xn56s0k \</span><br><span class="line">        --discovery-token-ca-cert-hash sha256:dac15eaa52a6c9fff179693aea175df23719e6cb09d5a9281a3104148b13ebfb</span><br></pre></td></tr></table></figure><p>检查是否多个控制节点都已经就绪.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root@master1:/etc/keepalived<span class="comment"># kubectl get node</span></span><br><span class="line">NAME                   STATUS                     ROLES                  AGE   VERSION</span><br><span class="line">master1                Ready                      control-plane,master   13d   v1.21.1</span><br><span class="line">master3                Ready                      control-plane,master   13d   v1.21.1</span><br><span class="line">master2                Ready                      control-plane,master   13d   v1.21.2</span><br><span class="line"></span><br><span class="line">root@master1:/etc/keepalived<span class="comment"># kubectl get pod -n kube-system -o wide | grep kube-apiserver</span></span><br><span class="line">kube-apiserver-master1            1/1     Running   3          10d   10.169.180.51    master1  &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-master3            1/1     Running   0          10d   10.169.212.212   master3  &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-apiserver-master2            1/1     Running   3          12d   10.169.182.17    master2  &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure><h2 id="etcd-相关"><a href="#etcd-相关" class="headerlink" title="etcd 相关"></a>etcd 相关</h2><p>kubeadm在处理etcd的时候并不复杂，简单来说就是创建static  pod所需要的yaml文件，并调用etcd的CLI将其它机器上的etcd的instance加入到etcd的cluster中去，</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// /go/src/k8s.io/kubernetes/cmd/kubeadm/app/cmd/phases/join/controlplanejoin.go （以stack的模式为例）</span></span><br><span class="line"><span class="keyword">if</span> err := etcdphase.CreateStackedEtcdStaticPodManifestFile(client, data.ManifestDir(), data.PatchesDir(), cfg.NodeRegistration.Name, &amp;cfg.ClusterConfiguration, &amp;cfg.LocalAPIEndpoint, data.DryRun(), data.CertificateWriteDir()); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> errors.Wrap(err, <span class="string">"error creating local etcd static pod manifest file"</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// /go/src/k8s.io/kubernetes/cmd/kubeadm/app/util/etcd/etcd.go</span></span><br><span class="line">cli, err := clientv3.New(clientv3.Config&#123;</span><br><span class="line">Endpoints:   c.Endpoints,</span><br><span class="line">DialTimeout: etcdTimeout,</span><br><span class="line">DialOptions: []grpc.DialOption&#123;</span><br><span class="line">grpc.WithBlock(), <span class="comment">// block until the underlying connection is up</span></span><br><span class="line">&#125;,</span><br><span class="line">TLS: c.TLS,</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>而etcd的同步是基于raft算法，一个从paxos算法衍生的分布式数据库同步算法（有机会可以去深入分析一下）。</p><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>几个基本的cases，</p><ol><li>停止某个节点上haproxy，VIP会浮动到次优节点，依然可以创建pods。</li><li>在apiserver里打上log，创建多个pods，请求被发送到多个不同的控制节点上。</li><li>停止某个节点上的scheduler服务，依然可以创建pods。</li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><hr><p><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/" target="_blank" rel="noopener">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/ha-topology/</a><br><a href="https://blog.csdn.net/chenleiking/article/details/84841394" target="_blank" rel="noopener">https://blog.csdn.net/chenleiking/article/details/84841394</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;分布式系统的HA和LB一直是一个关键性技术点，Kubernetes也有许多方面的考虑和实现来支持HA/LB，比方说lease，网络流量方面的LB（kube-proxy），以及在整体架构上与业界比较通用的haproxy与keepalived解决方案的集成，其它第三方的项目包括
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="http://jungler.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>kubemark + perf_test(clusterloader) 本地性能测试的注意事项</title>
    <link href="http://jungler.com/2021/09/21/kubemark-perf-test-clusterloader-%E6%9C%AC%E5%9C%B0%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%E7%9A%84%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/"/>
    <id>http://jungler.com/2021/09/21/kubemark-perf-test-clusterloader-本地性能测试的注意事项/</id>
    <published>2021-09-21T04:35:43.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/绍兴.jpg" alt="" title="6月13日在绍兴"><br>                                   <center>6月13日在绍兴</center></p><p>在用kubemark做k8s性能测试的时候踩过一些坑，整理记录一下，免得再次踩坑。</p><p>kubemark的架构描述以及原理参考[1],[2].</p><p>环境搭建以及跑benchmark需要注意的事项：</p><ul><li><p>Kubemark的master节点，也就是一个单一的all-in-one的节点最好加上一个”master”的后缀，老版本中需要这个信息来判断是否是控制节点，现在的版本应该没有这个要求了，但是最好还是加上这个后缀.<br>例如以kubeadm搭建的环境可以通过”node-name”这个参数来覆盖默认值。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm init --apiserver-advertise-address 10.10.36.51 --node-name node-master --pod-network-cidr=10.244.0.0/16</span><br></pre></td></tr></table></figure></li><li><p>client端需要支持无密码登陆到kubemark的master节点，clusterloader需要通过ssh连接到master节点获取节点信息。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i id_rsa.pub root@10.10.36.51</span><br></pre></td></tr></table></figure></li><li><p>需要开放的端口，当测试的机器在云上时，这个配置额外需要注意，因为有些端口是被禁的，而clusterloader则需要通过这些端口来收集数据，例如：</p></li></ul><table><thead><tr><th>组件</th><th>ports</th><th>协议</th></tr></thead><tbody><tr><td>etcd</td><td>2379-2381</td><td>tcp</td></tr><tr><td>kube-scheduler</td><td>10259</td><td>tcp</td></tr><tr><td>kube-controller-manager</td><td>10257</td><td>tcp</td></tr><tr><td>Kubelet API</td><td>10250</td><td>tcp</td></tr><tr><td>Kubernetes API server</td><td>6443</td><td>tcp</td></tr></tbody></table><ul><li><p>让scheduler和controller manager放行metrics endpoints，因为clusterloader会收集metrics数据。或者通过配置授权来让用户通过认证。</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">kube-scheduler.yaml</span><br><span class="line">   spec:</span><br><span class="line">     containers:</span><br><span class="line">     - command:</span><br><span class="line">       ...</span><br><span class="line">       - --bind-address=0.0.0.0</span><br><span class="line">       - --authorization-always-allow-paths=/healthz,/metrics</span><br></pre></td></tr></table></figure></li><li><p>跑benchmark之前需要设置的环境变量，详见测试脚本<a href="https://github.com/chendave/initrepo/blob/5812fc081916f4ed6bffff6c20c65cc12a37ff14/k8s/demo/kubemark/test.sh" title="install scripts" target="_blank" rel="noopener">安装脚本</a>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">KUBE_SSH_USER=root</span><br><span class="line">KUBEMARK_SSH_KEY=&quot;/root/.ssh/id_rsa&quot;</span><br></pre></td></tr></table></figure></li><li><p>hollow node的一些配置，</p><ul><li><p>设置用fake node的image service, 否则默认用的host上的image service。</p><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hollow-node.yaml</span><br><span class="line">  command: [</span><br><span class="line">    &quot;/kubemark&quot;,</span><br><span class="line">    &quot;--morph=kubelet&quot;,</span><br><span class="line">    &quot;--use-host-image-service=false&quot;,         // 这里！</span><br><span class="line">    &quot;--name=$(NODE_NAME)&quot;,</span><br><span class="line">    &quot;--kubeconfig=/kubeconfig/kubelet.kubeconfig&quot;,</span><br><span class="line">    &quot;--log-file=/var/log/kubelet-$(NODE_NAME).log&quot;,</span><br><span class="line">    &quot;--logtostderr=false&quot;</span><br><span class="line">    #&quot;--max-pods=1000&quot;   //设置最大支持pods数为1000</span><br><span class="line">  ]</span><br></pre></td></tr></table></figure></code></pre></li><li><p>而如果用host的image service时，需要和host上配置的runtime一致，因为测试框架默认用的是containerd，可以看hollow-node.yaml里mount的containerd，如果host上设置的是其它runtime，则起pods的时候会找不到对应的服务。</p></li><li><p>fakenodes默认也是只支持110个pods，如果需要跑更多的pods需要修改hollow node的启动参数。</p><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hollow-node.yaml</span><br><span class="line">      command: [</span><br><span class="line">        &quot;/kubemark&quot;,</span><br><span class="line">        &quot;--morph=kubelet&quot;,</span><br><span class="line">        &quot;--use-host-image-service=false&quot;,</span><br><span class="line">        &quot;--name=$(NODE_NAME)&quot;,</span><br><span class="line">        &quot;--kubeconfig=/kubeconfig/kubelet.kubeconfig&quot;,</span><br><span class="line">        &quot;--log-file=/var/log/kubelet-$(NODE_NAME).log&quot;,</span><br><span class="line">        &quot;--logtostderr=false&quot;</span><br><span class="line">        #&quot;--max-pods=1000&quot;   //设置最大支持pods数为1000</span><br><span class="line">      ]</span><br></pre></td></tr></table></figure></code></pre></li><li><p>hollow node无需配置太多资源，20m cpu以及20M memory足矣。</p>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hollow-node.yaml</span><br><span class="line"> ...</span><br><span class="line"> resources:</span><br><span class="line">   requests:</span><br><span class="line">     cpu: 20m</span><br><span class="line">     memory: 20M</span><br></pre></td></tr></table></figure></li></ul></li><li><p>设置部分组件的<code>profiling</code>为<code>true</code>, 否则cpu或者mem的pprof文件可能为空。</p></li></ul><table><thead><tr><th>组件</th><th>设置</th></tr></thead><tbody><tr><td>kube-controller-manager</td><td>profiling=true</td></tr><tr><td>kube-api</td><td>无需设置</td></tr><tr><td>kube-scheduling</td><td>无需设置</td></tr><tr><td>etcd</td><td>enable-pprof=true<br> listen-client-urls=<a href="https://0.0.0.0:2379,http://0.0.0.0:2379" target="_blank" rel="noopener">https://0.0.0.0:2379,http://0.0.0.0:2379</a></td></tr></tbody></table><ul><li><p>clusterloader的参数必须指定kubemark的master节点的IP地址，client端需要知道这个信息以收集kubemark对应的cluster的信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go run cmd/clusterloader.go ... --masterip=10.253.2.39 ...--logtostderr=false</span><br></pre></td></tr></table></figure></li><li><p>默认etcd的listen端口一般是2381，而代码里的默认值是2382，所以需要告诉clusterloader真实的etcd的监听端口是多少。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">go run cmd/clusterloader.go ... --etcd-insecure-port=2381 ...--logtostderr=false</span><br></pre></td></tr></table></figure></li><li><p>测试框架里有默认的throughput的threshold值，但是实际上跑下来经常会超过默认值，可以修改测试的schema以免跑出来一些warning信息。</p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--- a/clusterloader2/testing/node-throughput/config.yaml</span></span><br><span class="line"><span class="comment">+++ b/clusterloader2/testing/node-throughput/config_origin.yaml</span></span><br><span class="line">@@ -17,16 +17,19 @@ steps:</span><br><span class="line">     Method: APIResponsiveness</span><br><span class="line">     Params:</span><br><span class="line">       action: reset</span><br><span class="line"><span class="addition">+      threshold: 20s</span></span><br><span class="line">   - Identifier: PodStartupLatency</span><br><span class="line">     Method: PodStartupLatency</span><br><span class="line">     Params:</span><br><span class="line">       action: start</span><br><span class="line">       +      threshold: 20s</span><br></pre></td></tr></table></figure></li><li><p>load测试过程中master节点要打taint，单纯的cardon-ed不会阻止某些测试，例如daemonset scheduler到master，会造成期待的与实际起来的pods数量不一致。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes kubemark-master node-role.kubernetes.io/master=:NoSchedule</span><br></pre></td></tr></table></figure></li><li><p>本地测试时，一般是不支持PV的dynamic provision的，然而各个cloud的provider可能支持，测试框架中默认支持，所以本地测试的时候需要把pv相关的测试disable。</p><pre><code><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--- a/clusterloader2/testing/load/statefulset.yaml</span></span><br><span class="line"><span class="comment">+++ b/clusterloader2/testing/load/statefulset_origin.yaml</span></span><br><span class="line"><span class="meta">@@ -1,5 +1,5 @@</span></span><br><span class="line"> &#123;&#123;$HostNetworkMode := DefaultParam .CL2_USE_HOST_NETWORK_PODS false&#125;&#125;</span><br><span class="line"><span class="deletion">-&#123;&#123;$EnablePVs := DefaultParam .CL2_ENABLE_PVS true&#125;&#125;</span></span><br><span class="line"><span class="addition">+&#123;&#123;$EnablePVs := DefaultParam .CL2_ENABLE_PVS false&#125;&#125;</span></span><br></pre></td></tr></table></figure></code></pre></li><li><p>load测试需要disable掉daemonset [3]</p><pre><code><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--- a/clusterloader2/testing/load/modules/reconcile-objects.yaml</span></span><br><span class="line"><span class="comment">+++ b/clusterloader2/testing/load/modules/reconcile-objects_origin.yaml</span></span><br><span class="line"><span class="meta">@@ -44,10 +44,10 @@</span></span><br><span class="line"></span><br><span class="line"> ## CL2 params</span><br><span class="line"> &#123;&#123;$CHECK_IF_PODS_ARE_UPDATED := DefaultParam .CL2_CHECK_IF_PODS_ARE_UPDATED true&#125;&#125;</span><br><span class="line"><span class="deletion">-&#123;&#123;$DISABLE_DAEMONSETS := DefaultParam .CL2_DISABLE_DAEMONSETS false&#125;&#125;</span></span><br><span class="line"><span class="addition">+&#123;&#123;$DISABLE_DAEMONSETS := DefaultParam .CL2_DISABLE_DAEMONSETS true&#125;&#125;</span></span><br><span class="line"> &#123;&#123;$ENABLE_DNSTESTS := DefaultParam .CL2_ENABLE_DNSTESTS false&#125;&#125;</span><br><span class="line"> &#123;&#123;$ENABLE_NETWORKPOLICIES := DefaultParam .CL2_ENABLE_NETWORKPOLICIES false&#125;&#125;</span><br><span class="line"><span class="deletion">-&#123;&#123;$ENABLE_PVS := DefaultParam .CL2_ENABLE_PVS true&#125;&#125;</span></span><br><span class="line"><span class="addition">+&#123;&#123;$ENABLE_PVS := DefaultParam .CL2_ENABLE_PVS false&#125;&#125;</span></span><br></pre></td></tr></table></figure></code></pre></li></ul><p>其它的一些说明：</p><ol><li><p>单纯的cpu或者mem的profiling，可以不用借助clusterloader，可以直接通过调用url即可获取一段时间的数据，例如：</p><ul><li><p>打开scheduler的不安全端口</p><pre><code><figure class="highlight diff"><figcaption><span>kube-scheduler_updated.yaml kube-scheduler_origin.yaml</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;     - --port=7788</span><br><span class="line"><span class="comment">---</span></span><br><span class="line">&gt;     - --port=0</span><br></pre></td></tr></table></figure></code></pre><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -k http://localhost:7788/debug/pprof/profile?seconds=30 -o cpu2.pprof</span><br></pre></td></tr></table></figure></li><li><p>配置好用户授权后，也可以通过https端口来后去数据</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -k https://localhost:10259/debug/pprof/profile?seconds=30 -o cpu2.pprof</span><br></pre></td></tr></table></figure></li></ul></li></ol><ol start="2"><li><p>火焰图</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; go tool pprof http://localhost:7788/debug/pprof/profile?seconds=30 //生成数据，可以在/root/pprof/下找到压缩包</span><br><span class="line">&gt; web     //通过网页load数据</span><br></pre></td></tr></table></figure><p>web页面上view的icon下面可以找到<code>flame graph</code>，点击即可load火焰图，注意有些组件需要配置非安全端口或者配置好授权之后才可以拉到数据。</p></li><li><p>分析cpu或者mem的profiling数据时，主要有这么几个数据 [4]<br>flat: 当前函数的耗时（不包含调用的子函数），所以这个数据一般不大。<br>flat%: 占总耗时的百分比。<br>sum%: 函数占CPU耗时的累计百分比。<br>cum: 当前函数加上其调用的子函数总共的耗时<br>cum%: 当前函数加上其调用的子函数总共的耗时占总耗时的百分比，这个参考意义更大一些。</p></li><li><p>获取apiserver的pprof数据需要指定端口为6443，参考: <a href="https://github.com/kubernetes/perf-tests/pull/1937" target="_blank" rel="noopener">https://github.com/kubernetes/perf-tests/pull/1937</a>.</p></li><li><p>对于etcd的profiling来说，除了上面说的需要enable profiling之外，还需要指定cert以及key，对应的参数为: <code>--etcd-certificate</code>与<code>--etcd-key</code>，可以用curl来做个实验，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -k --cert /etc/kubernetes/pki/etcd/healthcheck-client.crt --key /etc/kubernetes/pki/etcd/healthcheck-client.key https://localhost:2379/debug/pprof/profile?seconds=10 -o cpu2.pprof</span><br></pre></td></tr></table></figure><p>/etc/kubernetes/pki/etcd下面的证书都可以用。</p></li></ol><h2 id="reference："><a href="#reference：" class="headerlink" title="reference："></a>reference：</h2><p>[1] <a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scalability/kubemark-guide.md" target="_blank" rel="noopener">https://github.com/kubernetes/community/blob/master/contributors/devel/sig-scalability/kubemark-guide.md</a><br>[2] <a href="https://github.com/kubernetes/kubernetes/tree/master/test/kubemark/pre-existing" target="_blank" rel="noopener">https://github.com/kubernetes/kubernetes/tree/master/test/kubemark/pre-existing</a><br>[3] <a href="https://github.com/kubernetes/perf-tests/issues/1878" target="_blank" rel="noopener">https://github.com/kubernetes/perf-tests/issues/1878</a><br>[4] <a href="https://xiazemin.github.io/MyBlog/golang/2020/03/26/cum.html" target="_blank" rel="noopener">https://xiazemin.github.io/MyBlog/golang/2020/03/26/cum.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://github.com/chendave/chendave.github.io/raw/master/css/images/绍兴.jpg&quot; alt=&quot;&quot; title=&quot;6月13日在绍兴&quot;&gt;&lt;br&gt;                      
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="http://jungler.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>kubelet若干配置-待更新</title>
    <link href="http://jungler.com/2021/07/25/kubelet%E8%8B%A5%E5%B9%B2%E9%85%8D%E7%BD%AE-%E5%BE%85%E6%9B%B4%E6%96%B0/"/>
    <id>http://jungler.com/2021/07/25/kubelet若干配置-待更新/</id>
    <published>2021-07-25T07:03:16.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/西湖.jpg" alt="" title="5月3日在西湖"><br>                                   <center>5月3日在西湖</center></p><p>记录一下，kubelet的若干配置，这些配置日常工作经常用到，记录待查。</p><ul><li>修改runtime<br>修改K8S的runtime只需要修改kubelet的一些参数即可，假设kubelet是通过systemD管理并通过kubeadm安装的，如果通过binary启动修改起来则更简单（直接修改启动参数即可）。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">systemctl status kubelet</span><br><span class="line">   Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)</span><br><span class="line">  Drop-In: /etc/systemd/system/kubelet.service.d</span><br><span class="line">           └─10-kubeadm.conf</span><br></pre></td></tr></table></figure><p>可以看到它的服务对应的配置文件为<code>10-kubeadm.conf</code>,</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat 10-kubeadm.conf</span><br><span class="line">[Service]</span><br><span class="line">Environment=&quot;KUBELET_KUBECONFIG_ARGS=--bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf&quot;</span><br><span class="line">...</span><br><span class="line">EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env</span><br><span class="line">...</span><br><span class="line">EnvironmentFile=-/etc/default/kubelet</span><br></pre></td></tr></table></figure><p>可以直接修改环境变量或者在引用的环境变量文件中定义加入下面这行，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KUBELET_EXTRA_ARGS=--container-runtime=remote --container-runtime-endpoint=&apos;unix:///run/containerd/containerd.sock&apos; --image-service-endpoint=&apos;unix:///run/containerd/containerd.sock&apos;</span><br></pre></td></tr></table></figure><p>目前来说，默认的runtime还是docker，我们希望将其修改为containerd，CRIO的修改类似。</p><p>然后重新load服务的配置文件，并重启kubelet服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl daemon-reload</span><br><span class="line">systemctl restart kubelet</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># kubectl get node -o wide</span><br><span class="line">NAME           STATUS     ROLES                  AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME</span><br><span class="line">dave-desktop   Ready      control-plane,master   72d   v1.21.1   10.169.180.51    &lt;none&gt;        Ubuntu 18.04.3 LTS   4.15.0-46-generic   containerd://1.2.6</span><br></pre></td></tr></table></figure><p>可以看到node的runtime已经修改为containerd了。</p><p><strong>NOTE</strong><br>默认的containerd的配置（通过Docker安装的默认配置）是没有enable cri服务的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/containerd/config.toml | grep disabled_plugins</span><br><span class="line">disabled_plugins = [&quot;cri&quot;]</span><br></pre></td></tr></table></figure><p>这里可以直接修改此配置文件，或者通过下面的命令来生成默认的containerd的配置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">containerd config default &gt; /etc/containerd/config.toml</span><br></pre></td></tr></table></figure><p>reload，重启containerd的服务即可。</p><p>否则，会遇到类似这样的错误而导致kubelet无法启动成功。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;Failed to run kubelet&quot; err=&quot;failed to run Kubelet: failed to create kubelet: get remote runtime typed version failed: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService&quot;</span><br></pre></td></tr></table></figure></p><ul><li>修改cgroup driver</li></ul><p>方法类似，例如我们修改driver为systemd，只需要在上面<code>KUBELET_EXTRA_ARGS</code>的配置里加上</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">--cgroup-driver=&quot;systemd&quot;</span><br></pre></td></tr></table></figure><p>reload， 重启服务即可。</p><ul><li>修改node上可以run的pod的数量</li></ul><p>默认一个node上可以跑110个pods，但这个是可以修改的。例如，我们希望修改这个配置为1500，那么只需要在10-kubeadm.conf中添加一个环境变量，像这样，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Environment=&quot;KUBELET_NODE_MAX_PODS=--max-pods=1500&quot;</span><br></pre></td></tr></table></figure><p>并将这个环境变量加入到服务启动的参数列表中去，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ExecStart=/usr/bin/kubelet $KUBELET_KUBECONFIG_ARGS $KUBELET_CONFIG_ARGS $KUBELET_KUBEADM_ARGS $KUBELET_EXTRA_ARGS $KUBELET_NODE_MAX_PODS</span><br></pre></td></tr></table></figure><p>reload， 重启服务即可。</p><p>我们来检查一下，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">kubectl describe node ip-10-253-2-22</span><br><span class="line"></span><br><span class="line">Capacity:</span><br><span class="line">  cpu:                64</span><br><span class="line">  ephemeral-storage:  508184812Ki</span><br><span class="line">  hugepages-1Gi:      0</span><br><span class="line">  hugepages-2Mi:      0</span><br><span class="line">  memory:             261120516Ki</span><br><span class="line">  pods:               1500</span><br><span class="line">Allocatable:</span><br><span class="line">  cpu:                64</span><br><span class="line">  ephemeral-storage:  468343121964</span><br><span class="line">  hugepages-1Gi:      0</span><br><span class="line">  hugepages-2Mi:      0</span><br><span class="line">  memory:             261018116Ki</span><br><span class="line">  pods:               1500</span><br></pre></td></tr></table></figure><p>可以看到Capacity以及Allocatable对应的pods数量都修改为1500。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://github.com/chendave/chendave.github.io/raw/master/css/images/西湖.jpg&quot; alt=&quot;&quot; title=&quot;5月3日在西湖&quot;&gt;&lt;br&gt;                       
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="http://jungler.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes-从APIServer到Kubelet</title>
    <link href="http://jungler.com/2021/02/16/Kubernetes-%E4%BB%8EAPIServer%E5%88%B0Kubelet/"/>
    <id>http://jungler.com/2021/02/16/Kubernetes-从APIServer到Kubelet/</id>
    <published>2021-02-16T12:40:54.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/崇明.jpg" alt="" title="2月13日在崇明东平森林公园"><br>                                   <center>2月13日在崇明东平森林公园</center></p><p>总的来说，Kubelet需要监听APIServer的events, 例如pod的创建事件，然后根据具体的事件去调用CRI的接口完成containers的创建，启动等。</p><p>这里记录一下部分核心的代码以备后查。<br>Kubelet启动后定义了对三个消息源的监听，分别是HTTP，File以及APIServer，以APIServer为例，</p><ul><li>APIServer表示来自于API Server的更新 - Apiserver Source identifies updates from Kubernetes API Server.</li><li>file更新来自于一个文件，比方说static pod对应的manifest文件- Filesource idenitified updates from a file.</li><li>http 更新来自于web page, 通过web page传入的static pod配置 - HTTPSource identifies updates from querying a web page.</li></ul><p>在创建pod的时候，这部分信息会记录在pod的yaml文件中，以这样的annotation呈现：</p><blockquote><p>kubernetes.io/config.source: file</p></blockquote><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">/<span class="keyword">go</span>/src/k8s.io/kubernetes/pkg/kubelet/kubelet.<span class="keyword">go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">makePodSourceConfig</span><span class="params">(kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *Dependencies, nodeName types.NodeName)</span> <span class="params">(*config.PodConfig, error)</span></span> &#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">if</span> kubeDeps.KubeClient != <span class="literal">nil</span> &#123;</span><br><span class="line">klog.Infof(<span class="string">"Watching apiserver"</span>)</span><br><span class="line">config.NewSourceApiserver(kubeDeps.KubeClient, nodeName, cfg.Channel(kubetypes.ApiserverSource))</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/<span class="keyword">go</span>/src/k8s.io/kubernetes/pkg/kubelet/config/apiserver.<span class="keyword">go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">newSourceApiserverFromLW</span><span class="params">(lw cache.ListerWatcher, updates <span class="keyword">chan</span>&lt;- <span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line">send := <span class="function"><span class="keyword">func</span><span class="params">(objs []<span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> pods []*v1.Pod</span><br><span class="line"><span class="keyword">for</span> _, o := <span class="keyword">range</span> objs &#123;</span><br><span class="line">pods = <span class="built_in">append</span>(pods, o.(*v1.Pod))</span><br><span class="line">&#125;</span><br><span class="line">updates &lt;- kubetypes.PodUpdate&#123;Pods: pods, Op: kubetypes.SET, Source: kubetypes.ApiserverSource&#125;</span><br><span class="line">&#125;</span><br><span class="line">r := cache.NewReflector(lw, &amp;v1.Pod&#123;&#125;, cache.NewUndeltaStore(send, cache.MetaNamespaceKeyFunc), <span class="number">0</span>)</span><br><span class="line"><span class="keyword">go</span> r.Run(wait.NeverStop)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>cfg.Channel(kubetypes.ApiserverSource)</code>生成了一个chan，并且定义了一个goroutine来轮询此chan，此chan的输入本质上来自于APIServer通过Reflector的list-watch机制捕获的一些消息。添加，删除等。</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(m *Mux)</span> <span class="title">Channel</span><span class="params">(source <span class="keyword">string</span>)</span> <span class="title">chan</span> <span class="title">interface</span></span>&#123;&#125; &#123;</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(source) == <span class="number">0</span> &#123;</span><br><span class="line"><span class="built_in">panic</span>(<span class="string">"Channel given an empty name"</span>)</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">newChannel := <span class="built_in">make</span>(<span class="keyword">chan</span> <span class="keyword">interface</span>&#123;&#125;)</span><br><span class="line">m.sources[source] = newChannel</span><br><span class="line"><span class="keyword">go</span> wait.Until(<span class="function"><span class="keyword">func</span><span class="params">()</span></span> &#123; m.listen(source, newChannel) &#125;, <span class="number">0</span>, wait.NeverStop)</span><br><span class="line"><span class="keyword">return</span> newChannel</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过Reflector的list-watch机制，监听APIServer来获取Cache的变更，<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">/<span class="keyword">go</span>/src/k8s.io/kubernetes/vendor/k8s.io/client-<span class="keyword">go</span>/tools/cache/reflector.<span class="keyword">go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(r *Reflector)</span> <span class="title">watchHandler</span><span class="params">(start time.Time, w watch.Interface, resourceVersion *<span class="keyword">string</span>, errc <span class="keyword">chan</span> error, stopCh &lt;-<span class="keyword">chan</span> <span class="keyword">struct</span>&#123;&#125;)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">switch</span> event.Type &#123;</span><br><span class="line"><span class="keyword">case</span> watch.Added:</span><br><span class="line">err := r.store.Add(event.Object)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">utilruntime.HandleError(fmt.Errorf(<span class="string">"%s: unable to add watch event object (%#v) to store: %v"</span>, r.name, event.Object, err))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> watch.Modified:</span><br><span class="line">err := r.store.Update(event.Object)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">utilruntime.HandleError(fmt.Errorf(<span class="string">"%s: unable to update watch event object (%#v) to store: %v"</span>, r.name, event.Object, err))</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">case</span> watch.Deleted:</span><br><span class="line"><span class="comment">// <span class="doctag">TODO:</span> Will any consumers need access to the "last known</span></span><br><span class="line"><span class="comment">// state", which is passed in event.Object? If so, may need</span></span><br><span class="line"><span class="comment">// to change this.</span></span><br><span class="line">err := r.store.Delete(event.Object)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line">utilruntime.HandleError(fmt.Errorf(<span class="string">"%s: unable to delete watch event object (%#v) from store: %v"</span>, r.name, event.Object, err))</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里的store初始化为<code>UndeltaStore</code>， 因此会触发<code>PushFunc</code>，也是就是之前在<code>apiserver.go</code>里定义的send方法，<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/<span class="keyword">go</span>/src/k8s.io/kubernetes/pkg/kubelet/config/apiserver.<span class="keyword">go</span></span><br><span class="line">send := <span class="function"><span class="keyword">func</span><span class="params">(objs []<span class="keyword">interface</span>&#123;&#125;)</span></span> &#123;</span><br><span class="line"><span class="keyword">var</span> pods []*v1.Pod</span><br><span class="line"><span class="keyword">for</span> _, o := <span class="keyword">range</span> objs &#123;</span><br><span class="line">pods = <span class="built_in">append</span>(pods, o.(*v1.Pod))</span><br><span class="line">&#125;</span><br><span class="line">updates &lt;- kubetypes.PodUpdate&#123;Pods: pods, Op: kubetypes.SET, Source: kubetypes.ApiserverSource&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/<span class="keyword">go</span>/src/k8s.io/kubernetes/vendor/k8s.io/client-<span class="keyword">go</span>/tools/cache/undelta_store.<span class="keyword">go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(u *UndeltaStore)</span> <span class="title">Add</span><span class="params">(obj <span class="keyword">interface</span>&#123;&#125;)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="keyword">if</span> err := u.Store.Add(obj); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">u.PushFunc(u.Store.List())</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>监听到newChannel上有输入时，调用Merge方法来过滤掉一些多余重复的变更，并规范化为一个PodUpdate类型的结构体，发送给<code>podStorage</code>chan型成员变量updates。<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(s *podStorage)</span> <span class="title">Merge</span><span class="params">(source <span class="keyword">string</span>, change <span class="keyword">interface</span>&#123;&#125;)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">switch</span> s.mode &#123;</span><br><span class="line"><span class="keyword">case</span> PodConfigNotificationIncremental:</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(removes.Pods) &gt; <span class="number">0</span> &#123;</span><br><span class="line">s.updates &lt;- *removes</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(adds.Pods) &gt; <span class="number">0</span> &#123;</span><br><span class="line">s.updates &lt;- *adds</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(updates.Pods) &gt; <span class="number">0</span> &#123;</span><br><span class="line">s.updates &lt;- *updates</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(deletes.Pods) &gt; <span class="number">0</span> &#123;</span><br><span class="line">s.updates &lt;- *deletes</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>podStorage用在PodConfig的初始化步骤里，<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/<span class="keyword">go</span>/src/k8s.io/kubernetes/pkg/kubelet/config/config.<span class="keyword">go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewPodConfig</span><span class="params">(mode PodConfigNotificationMode, recorder record.EventRecorder)</span> *<span class="title">PodConfig</span></span> &#123;</span><br><span class="line">updates := <span class="built_in">make</span>(<span class="keyword">chan</span> kubetypes.PodUpdate, <span class="number">50</span>)</span><br><span class="line">storage := newPodStorage(updates, mode, recorder)</span><br><span class="line">podConfig := &amp;PodConfig&#123;</span><br><span class="line">pods:    storage,</span><br><span class="line">mux:     config.NewMux(storage),  <span class="comment">// podStorage转为会Mux类型，所以可以调用去Merge方法对变更进行规范化。</span></span><br><span class="line">updates: updates,<span class="comment">// updates是我们需要的主要结构体，后续需要根据这个结构来分析不同的事件。</span></span><br><span class="line">sources: sets.String&#123;&#125;,</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> podConfig</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">/<span class="keyword">go</span>/src/k8s.io/kubernetes/pkg/kubelet/kubelet.<span class="keyword">go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewMainKubelet</span><span class="params">(kubeCfg *kubeletconfiginternal.KubeletConfiguration,</span></span></span><br><span class="line"><span class="function"><span class="params">...</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">if</span> kubeDeps.PodConfig == <span class="literal">nil</span> &#123;</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">var</span> err error</span></span></span><br><span class="line"><span class="function"><span class="params">kubeDeps.PodConfig, err = makePodSourceConfig(kubeCfg, kubeDeps, nodeName)</span></span></span><br><span class="line"><span class="function"><span class="title">if</span> <span class="title">err</span> != <span class="title">nil</span></span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, err</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>makePodSourceConfig</code>调用了<code>NewPodConfig</code>来创建<code>podconfig</code>，并赋值给<code>kubeDeps.PodConfig</code>，接着在<code>RunKubelet</code>方法中赋值给podCfg，</p><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">/<span class="keyword">go</span>/src/k8s.io/kubernetes/cmd/kubelet/app/server.<span class="keyword">go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">RunKubelet</span><span class="params">(kubeServer *options.KubeletServer, kubeDeps *kubelet.Dependencies, runOnce <span class="keyword">bool</span>)</span> <span class="title">error</span></span> &#123;</span><br><span class="line">        ...</span><br><span class="line">podCfg := kubeDeps.PodConfig</span><br><span class="line">...</span><br><span class="line"><span class="comment">// process pods and exit.</span></span><br><span class="line"><span class="keyword">if</span> runOnce &#123;</span><br><span class="line"><span class="keyword">if</span> _, err := k.RunOnce(podCfg.Updates()); err != <span class="literal">nil</span> &#123;  </span><br><span class="line"><span class="keyword">return</span> fmt.Errorf(<span class="string">"runonce failed: %v"</span>, err)</span><br><span class="line">&#125;</span><br><span class="line">klog.Info(<span class="string">"Started kubelet as runonce"</span>)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">startKubelet(k, podCfg, &amp;kubeServer.KubeletConfiguration, kubeDeps, kubeServer.EnableCAdvisorJSONEndpoints, kubeServer.EnableServer)</span><br><span class="line">klog.Info(<span class="string">"Started kubelet"</span>)</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>startKubelet方法启动kubelet,<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">startKubelet</span><span class="params">(k kubelet.Bootstrap, podCfg *config.PodConfig, kubeCfg *kubeletconfiginternal.KubeletConfiguration, kubeDeps *kubelet.Dependencies, enableCAdvisorJSONEndpoints, enableServer <span class="keyword">bool</span>)</span></span> &#123;</span><br><span class="line"><span class="comment">// start the kubelet</span></span><br><span class="line"><span class="keyword">go</span> k.Run(podCfg.Updates())  <span class="comment">//podCfg.Updates()获取updates结构体</span></span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这里就很清楚了，updates结构体被用在syncLoop方法中，也就是通过各个handler来调用底层的CRI来实现pod的增删改等操作。<br><figure class="highlight golang"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">/<span class="keyword">go</span>/src/k8s.io/kubernetes/pkg/kubelet/kubelet.<span class="keyword">go</span></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span> <span class="title">Run</span><span class="params">(updates &lt;-<span class="keyword">chan</span> kubetypes.PodUpdate)</span></span> &#123;</span><br><span class="line">...</span><br><span class="line"><span class="comment">// Start the pod lifecycle event generator.</span></span><br><span class="line">kl.pleg.Start()</span><br><span class="line">kl.syncLoop(updates, kl)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(kl *Kubelet)</span> <span class="title">syncLoopIteration</span><span class="params">(configCh &lt;-<span class="keyword">chan</span> kubetypes.PodUpdate, handler SyncHandler,</span></span></span><br><span class="line"><span class="function"><span class="params">syncCh &lt;-<span class="keyword">chan</span> time.Time, housekeepingCh &lt;-<span class="keyword">chan</span> time.Time, plegCh &lt;-<span class="keyword">chan</span> *pleg.PodLifecycleEvent)</span> <span class="title">bool</span></span> &#123;</span><br><span class="line">...</span><br><span class="line"><span class="keyword">select</span> &#123;</span><br><span class="line"><span class="keyword">case</span> u, open := &lt;-configCh:</span><br><span class="line"><span class="comment">// Update from a config source; dispatch it to the right handler</span></span><br><span class="line"><span class="comment">// callback.</span></span><br><span class="line"><span class="keyword">if</span> !open &#123;</span><br><span class="line">klog.Errorf(<span class="string">"Update channel is closed. Exiting the sync loop."</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">switch</span> u.Op &#123;</span><br><span class="line"><span class="keyword">case</span> kubetypes.ADD:</span><br><span class="line">...</span><br><span class="line">klog.V(<span class="number">2</span>).Infof(<span class="string">"SyncLoop (ADD, %q): %q"</span>, u.Source, format.Pods(u.Pods))</span><br><span class="line"><span class="comment">// After restarting, kubelet will get all existing pods through</span></span><br><span class="line"><span class="comment">// ADD as if they are new pods. These pods will then go through the</span></span><br><span class="line"><span class="comment">// admission process and *may* be rejected. This can be resolved</span></span><br><span class="line"><span class="comment">// once we have checkpointing.</span></span><br><span class="line">handler.HandlePodAdditions(u.Pods)</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://github.com/chendave/chendave.github.io/raw/master/css/images/崇明.jpg&quot; alt=&quot;&quot; title=&quot;2月13日在崇明东平森林公园&quot;&gt;&lt;br&gt;                
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="http://jungler.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Ceph 日志迁移</title>
    <link href="http://jungler.com/2018/12/31/%E6%97%A5%E5%BF%97%E8%BF%81%E7%A7%BB/"/>
    <id>http://jungler.com/2018/12/31/日志迁移/</id>
    <published>2018-12-31T03:08:57.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p>18年最后一天，也是本命年的最后一天了。这一年没有什么大的收获，每天都是浑浑噩噩的过，当初誓要对未来的职业规划做个改变，眼看着又要回到原点了。随着年龄的增加，心态也越来越淡定起来，很多事情即便你愿意去努力，愿意去改变，结果也未必是有什么大的不同。人各有命，也许我的命就是“独善其身”吧。</p><p>过去一段时间一直在看一些benchmark，尤其是关注一些硬件选型对软件性能的影响，这其中就涉及到Ceph journal放在不同类型的盘上对Ceph性能的影响，这里记录的是Ceph在使用filestore的场景下，如何更改journal所在的盘，Ceph版本是”Jewel”。一般的建议是Ceph journal是放在SSD或者NVME上，OSD与journal的比例一般遵循：</p><ul><li>SSD 4-5:1</li><li>NVME 12-18:1</li></ul><p>先来看看为什么要用Journal？</p><blockquote><p>   Speed: The journal enables the Ceph OSD Daemon to commit small writes quickly. Ceph writes small, random i/o to the journal sequentially, which tends to speed up bursty workloads by allowing the backing filesystem more time to coalesce writes. The Ceph OSD Daemon’s journal, however, can lead to spiky performance with short spurts of high-speed writes followed by periods without any write progress as the filesystem catches up to the journal.<br>   Consistency: Ceph OSD Daemons require a filesystem interface that guarantees atomic compound operations. Ceph OSD Daemons write a description of the operation to the journal and apply the operation to the filesystem. This enables atomic updates to an object (for example, placement group metadata). Every few seconds–between filestore max sync interval and filestore min sync interval–the Ceph OSD Daemon stops writes and synchronizes the journal with the filesystem, allowing Ceph OSD Daemons to trim operations from the journal and reuse the space. On failure, Ceph OSD Daemons replay the journal starting after the last synchronization operation.</p></blockquote><p>简言之就是速度和一致性，正因为速度的考量，所以这里最好使用快存储设备，例如SSD； 一致性我一直理解类似为数据库的日志，可以用来做数据恢复。</p><p>默认情况下，Jewel版本的Ceph journal是放在HDD上，且在我系统上看到的是在OSD所在的盘上划分了一个5G大小的分区来用作Journal，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lsblk</span><br><span class="line">...</span><br><span class="line">sdb      8:16   0   1.8T  0 disk</span><br><span class="line">├─sdb2   8:18   0     5G  0 part</span><br><span class="line">└─sdb1   8:17   0   1.8T  0 part /var/lib/ceph/osd/ceph-0</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>所以我们需要找到这个分区，将其修改为SSD或者NVME的分区，看看Journal放在哪里：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /var/lib/ceph/osd/ceph-0</span></span><br><span class="line"><span class="comment"># ls -l journal</span></span><br><span class="line">lrwxrwxrwx 1 root root 58 12月 26 14:04 journal -&gt; /dev/disk/by-partuuid/389057e5-a099-43b6-952e-ad0bff2e7893</span><br><span class="line"><span class="comment"># ls -l /dev/disk/by-partuuid/389057e5-a099-43b6-952e-ad0bff2e7893</span></span><br><span class="line">lrwxrwxrwx 1 root root 10 11月 13 16:15 /dev/disk/by-partuuid/389057e5-a099-43b6-952e-ad0bff2e7893 -&gt; ../../sdb2</span><br></pre></td></tr></table></figure><p>接下来所要做的不外乎是将其链接到SSD/NVME的一块分区，这里假设sde是一块SSD盘:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ceph节点上有三块OSD，所以从sde（SSD盘符）上划分出三块5G大小的分区，为了方便比较，大小也设置为5G</span></span><br><span class="line"><span class="comment"># parted /dev/sde</span></span><br><span class="line">(parted) mklabel gpt</span><br><span class="line">(parted) mkpart journal-0 1 5G</span><br><span class="line">(parted) mkpart journal-1 5G 10G</span><br><span class="line">(parted) mkpart journal-2 10G 15G</span><br><span class="line"><span class="comment"># 修改owner和group，否则后面可能会有权限问题</span></span><br><span class="line"><span class="comment"># sudo chown ceph:ceph /dev/sde1</span></span><br><span class="line"><span class="comment"># sudo chown ceph:ceph /dev/sde2</span></span><br><span class="line"><span class="comment"># sudo chown ceph:ceph /dev/sde3</span></span><br><span class="line"><span class="comment"># cd /var/lib/ceph/osd/ceph-0</span></span><br><span class="line"><span class="comment"># ceph osd set noout （开启noout以避免rebalance。）</span></span><br><span class="line"><span class="comment"># systemctl stop ceph-osd@0</span></span><br><span class="line"><span class="comment"># ceph-osd -i 0 --flush-journal</span></span><br><span class="line"><span class="comment"># rm /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line"><span class="comment"># 链接journal盘到SSD</span></span><br><span class="line"><span class="comment"># ln -s  /var/lib/ceph/osd/&lt;osd-id&gt;/journal /dev/&lt;ssd-partition-for-journal&gt;</span></span><br><span class="line"><span class="comment"># OSD目录下有一个“journal_uuid”，这个文件需要手动更新，应该是一个bug</span></span><br><span class="line"><span class="comment"># echo $partuuid &gt; journal_uuid</span></span><br><span class="line"><span class="comment"># ceph-osd -i 0 --mkjournal</span></span><br><span class="line"><span class="comment"># systemctl start ceph-osd@0</span></span><br><span class="line"><span class="comment"># ceph osd unset noout</span></span><br></pre></td></tr></table></figure><p>做完上面的步骤，用“ceph-disk list”确认一下是否修改成功，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ceph-disk list</span><br><span class="line">...</span><br><span class="line">/dev/sda :</span><br><span class="line">/dev/sda2 ceph journal</span><br><span class="line">/dev/sda1 ceph data, active, cluster ceph, osd.4, journal /dev/sde3</span><br><span class="line">/dev/sdb :</span><br><span class="line">/dev/sdb2 ceph journal</span><br><span class="line">/dev/sdb1 ceph data, active, cluster ceph, osd.1, journal /dev/sde2</span><br><span class="line">/dev/sdc :</span><br><span class="line">/dev/sdc2 ceph journal</span><br><span class="line">/dev/sdc1 ceph data, active, cluster ceph, osd.0, journal /dev/sde1...<span class="comment"># ls -l /var/lib/ceph/osd/ceph-0/journal</span></span><br><span class="line">lrwxrwxrwx 1 root root 58 11月 13 16:01 /var/lib/ceph/osd/ceph-0/journal -&gt; /dev/disk/by-partuuid/8fa35d73-d973-4ff3-b103-a370a10bf4a1</span><br><span class="line"><span class="comment"># ls -l /dev/disk/by-partuuid/8fa35d73-d973-4ff3-b103-a370a10bf4a1</span></span><br><span class="line">lrwxrwxrwx 1 root root 10 11月 13 16:09 /dev/disk/by-partuuid/8fa35d73-d973-4ff3-b103-a370a10bf4a1 -&gt; ../../sde1</span><br></pre></td></tr></table></figure><p>OSD-0所对应的journal已经修改为SSD盘上的一个分区了，接下来就可以跑一些benchmark做一些对比实验了。</p><p>上面描述的是在一个已有的OSD的基础上做修改，如果是新创建一个OSD，则可以直接journal的位置，例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy osd create &#123;node-name&#125;:&#123;disk&#125;[:&#123;path/to/journal&#125;]</span><br><span class="line">ceph-deploy osd create osdserver1:sdb:/dev/ssd1</span><br></pre></td></tr></table></figure><p>跑benchmark过程中发现rados benchmark默认参数下跑出来的结果差别不大，究其原因可以从下面的回复中得到答案。</p><blockquote><p>Hi Dave,<br>The SSD journal will help boost iops &amp; latency which will be more apparent for small block sizes. The rados benchmark default block size is 4M, use the -b option to specify the size. Try at 4k, 32k, 64k …<br>As a side note, this is a rados level test, the rbd image size is not relevant here.</p></blockquote><blockquote><p>Maged.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;18年最后一天，也是本命年的最后一天了。这一年没有什么大的收获，每天都是浑浑噩噩的过，当初誓要对未来的职业规划做个改变，眼看着又要回到原点了。随着年龄的增加，心态也越来越淡定起来，很多事情即便你愿意去努力，愿意去改变，结果也未必是有什么大的不同。人各有命，也许我的命就是“独
      
    
    </summary>
    
    
      <category term="Ceph" scheme="http://jungler.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack 归档 - 虚拟机临时存储与块存储</title>
    <link href="http://jungler.com/2018/11/04/OpenStack-%E5%BD%92%E6%A1%A3-%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%B4%E6%97%B6%E5%AD%98%E5%82%A8%E4%B8%8E%E5%9D%97%E5%AD%98%E5%82%A8/"/>
    <id>http://jungler.com/2018/11/04/OpenStack-归档-虚拟机临时存储与块存储/</id>
    <published>2018-11-04T07:06:04.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p>总体来说，虚拟机内部的存储分临时存储与可拔插的块存储两部分，所谓临时存储既是指存储空间会随着虚拟机的创建而产生，删除而消亡。而块存储(volume)则可以将用户的数据保存下来，并可以attach到不通的虚机机上。</p><h2 id="默认情况"><a href="#默认情况" class="headerlink" title="默认情况"></a>默认情况</h2><p>默认情况创建一个虚机只有一个盘，mount到root分区，看下下面的例子。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nova boot default --image cirros-0.3.5-x86_64-disk --flavor m1.small --nic net-name=private</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ sudo fdisk -l</span><br><span class="line"></span><br><span class="line">Disk /dev/vda: 21.5 GB, 21474836480 bytes</span><br><span class="line">255 heads, 63 sectors/track, 2610 cylinders, total 41943040 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/vda1   *       16065    41929649    20956792+  83  Linux</span><br><span class="line">$ df -h</span><br><span class="line">Filesystem                Size      Used Available Use% Mounted on</span><br><span class="line">/dev                    998.3M         0    998.3M   0% /dev</span><br><span class="line">/dev/vda1                23.2M     18.0M      4.0M  82% /</span><br><span class="line">tmpfs                  1001.8M         0   1001.8M   0% /dev/shm</span><br><span class="line">tmpfs                   200.0K     68.0K    132.0K  34% /run</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ lsblk</span><br><span class="line">NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  20G  0 disk</span><br><span class="line">`-vda1 253:1    0  20G  0 part /</span><br></pre></td></tr></table></figure><h2 id="块设备"><a href="#块设备" class="headerlink" title="块设备"></a>块设备</h2><p>如果希望数据能持久的保存下来，即便虚拟机被删之后，还能找到在之前的数据，可以给虚拟机添加一个块设备。块设备由<em>Cinder</em>服务提供，可以将其理解为一块U盘，可以动态的拔插到的你的电脑上。如下，我们给虚拟机添加一个块设备<em>volume</em>。</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/attach_volume.png" alt=""></p><p>再来看看虚拟机里的存储空间，我们会发现多出来一块盘。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">$ lsblk</span><br><span class="line">NAME   MAJ:MIN RM SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0  20G  0 disk</span><br><span class="line">`-vda1 253:1    0  20G  0 part /</span><br><span class="line">vdb    253:16   0   1G  0 disk</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ sudo fdisk -l</span><br><span class="line"></span><br><span class="line">Disk /dev/vda: 21.5 GB, 21474836480 bytes</span><br><span class="line">255 heads, 63 sectors/track, 2610 cylinders, total 41943040 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">   Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/vda1   *       16065    41929649    20956792+  83  Linux</span><br><span class="line"></span><br><span class="line">Disk /dev/vdb: 1073 MB, 1073741824 bytes</span><br><span class="line">16 heads, 63 sectors/track, 2080 cylinders, total 2097152 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br></pre></td></tr></table></figure><h2 id="Ephemeral-与Swap"><a href="#Ephemeral-与Swap" class="headerlink" title="Ephemeral 与Swap"></a>Ephemeral 与Swap</h2><p>另外，可以根据需要在虚拟机里创建一些临时的分区/盘，但这些盘同样会时随着虚拟机的生命周期消亡而消亡。<br>首先创建一个flavor,</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ nova flavor-create --ephemeral 20 --swap 512 testeph 7 512 1 1</span><br><span class="line">+----+---------+-----------+------+-----------+------+-------+-------------+-----------+</span><br><span class="line">| ID | Name    | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |</span><br><span class="line">+----+---------+-----------+------+-----------+------+-------+-------------+-----------+</span><br><span class="line">| 7  | testeph | 512       | 1    | 20        | 512  | 1     | 1.0         | True      |</span><br><span class="line">+----+---------+-----------+------+-----------+------+-------+-------------+-----------+</span><br></pre></td></tr></table></figure><p>根据此flavor创建一个虚拟机:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ nova boot --image cirros-0.3.5-x86_64-disk --flavor 7 --nic net-name=private --ephemeral size=1 emph1 </span><br><span class="line"></span><br><span class="line">$ sudo fdisk -l</span><br><span class="line"></span><br><span class="line">Disk /dev/vda: 1073 MB, 1073741824 bytes</span><br><span class="line">255 heads, 63 sectors/track, 130 cylinders, total 2097152 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">Device Boot      Start         End      Blocks   Id  System</span><br><span class="line">/dev/vda1   *       16065     2088449     1036192+  83  Linux</span><br><span class="line"></span><br><span class="line">Disk /dev/vdb: 1073 MB, 1073741824 bytes</span><br><span class="line">16 heads, 63 sectors/track, 2080 cylinders, total 2097152 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">Disk /dev/vdb doesn&apos;t contain a valid partition table</span><br><span class="line"></span><br><span class="line">Disk /dev/vdc: 536 MB, 536870912 bytes</span><br><span class="line">16 heads, 63 sectors/track, 1040 cylinders, total 1048576 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x00000000</span><br><span class="line"></span><br><span class="line">$ lsblk</span><br><span class="line">NAME   MAJ:MIN RM    SIZE RO TYPE MOUNTPOINT</span><br><span class="line">vda    253:0    0      1G  0 disk</span><br><span class="line">`-vda1 253:1    0 1011.9M  0 part /</span><br><span class="line">vdb    253:16   0      1G  0 disk /mnt</span><br><span class="line">vdc    253:32   0    512M  0 disk</span><br></pre></td></tr></table></figure><p>可以看出，swap和ephemeral 是以独立的虚拟磁盘来呈现的。<em>disk.eph0</em> 与 <em>disk.swap</em> 都存放在虚拟机的目录下，这意味着虚拟机删除之后，这些文件也将随之被删除。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ls /opt/stack/data/nova/instances/29cc9a74-bebc-429d-a0b8-58fbfe89b2cd</span><br><span class="line">console.log  disk  disk.eph0  disk.info  disk.swap</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;总体来说，虚拟机内部的存储分临时存储与可拔插的块存储两部分，所谓临时存储既是指存储空间会随着虚拟机的创建而产生，删除而消亡。而块存储(volume)则可以将用户的数据保存下来，并可以attach到不通的虚机机上。&lt;/p&gt;
&lt;h2 id=&quot;默认情况&quot;&gt;&lt;a href=&quot;#默认
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>（转）初探Openstack Neutron DVR</title>
    <link href="http://jungler.com/2018/10/21/%E5%88%9D%E6%8E%A2Openstack-Neutron-DVR/"/>
    <id>http://jungler.com/2018/10/21/初探Openstack-Neutron-DVR/</id>
    <published>2018-10-21T07:26:18.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章总结的很好了，偷懒直接转过来，以便日后不时查看。</p><p>首先看一下，没有使用DVR的问题在哪里：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/issues.png" alt=""></p><p>从图中可以明显看到东西向和南北向的流量会集中到网络节点，这会使网络节点成为瓶颈。</p><p>那如果启用的DVR，情况会变成如下：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/dvr.png" alt=""></p><p>对于东西向的流量， 流量会直接在计算节点之间传递。<br>对于南北向的流量，如果有floating ip，流量就直接走计算节点。如果没有floating ip，则会走网络节点。</p><p>我的实验环境如下：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/lab.png" alt=""></p><p>然后起了两个私有网络和一个DVR 路由器，拓扑如下:</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/network.png" alt=""></p><p>注:<br>可以看到每个网络与DVR连接时有两个接口，以private1为例，有10.0.1.1和10.0.1.6。</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/interface1.png" alt=""></p><p>可以看到10.0.1.6是centralized_snat的网关，这个地址是在网络节点上的。<br>10.0.1.1是router_interface_distributed地址，它是在每一个计算节点上的。虚机获取到的默认网关就是这个IP。</p><p>虚机情况如下：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/interface2.png" alt=""></p><p>注:<br>虚机privateX-computeY-VM表示，此虚机起在privateX网络，computeY节点上。在compute1节点的两台虚机拥有floating ip。</p><p>下面分析三种情况下traffic的是怎么走的：</p><ol><li>东西向流量：以private1-compute1-VM和private2-compute2-VM之间的通信为例。</li><li>南北向流量：<br> a) 带floating ip， 以private1-compute1-VM对外通信为例。<br> b) 不带floating ip， 以private1-compute2-VM对外通信为例。</li></ol><p>第一种情况 – 东西向流量<br>首先我们看一下虚机private1-compute1-VM的IP和路由:</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/we1.png" alt=""></p><p>再看一下虚机private2-compute2-VM的IP和路由:</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/we2.png" alt=""></p><p>我们在private1-compute1-VM中ping 10.0.2.5(private2-compute2-VM的IP)。</p><p>当我们ping了之后，在首先会查询private1-compute1-VM的路由表，会将包发送到网关10.0.1.1。那么会首先会发送10.0.1.1的arp请求。<br>arp请求会发送到br-int上。<br>我们可以看到10.0.1.5的port id是4e843b99开头的：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/interface3.png" alt=""></p><p>最终会转发到br-int的qvo4e843b99-fb:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">root@dvr-compute1:~<span class="comment"># ovs-vsctl show</span></span><br><span class="line">67f121bd-cca7-41c2-95ab-23ed85d1305b</span><br><span class="line">    Bridge br-tun</span><br><span class="line">        Port patch-int</span><br><span class="line">            Interface patch-int</span><br><span class="line">                <span class="built_in">type</span>: patch</span><br><span class="line">                options: &#123;peer=patch-tun&#125;</span><br><span class="line">        Port <span class="string">"vxlan-0ae09f91"</span></span><br><span class="line">            Interface <span class="string">"vxlan-0ae09f91"</span></span><br><span class="line">                <span class="built_in">type</span>: vxlan</span><br><span class="line">                options: &#123;df_default=<span class="string">"true"</span>, in_key=flow, local_ip=<span class="string">"10.224.159.141"</span>, out_key=flow, remote_ip=<span class="string">"10.224.159.145"</span>&#125;</span><br><span class="line">        Port <span class="string">"vxlan-0ae09f88"</span></span><br><span class="line">            Interface <span class="string">"vxlan-0ae09f88"</span></span><br><span class="line">                <span class="built_in">type</span>: vxlan</span><br><span class="line">                options: &#123;df_default=<span class="string">"true"</span>, in_key=flow, local_ip=<span class="string">"10.224.159.141"</span>, out_key=flow, remote_ip=<span class="string">"10.224.159.136"</span>&#125;</span><br><span class="line">        Port br-tun</span><br><span class="line">            Interface br-tun</span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">    Bridge br-int</span><br><span class="line">        fail_mode: secure</span><br><span class="line">        Port <span class="string">"qvo111517d8-c5"</span></span><br><span class="line">            tag: 2</span><br><span class="line">            Interface <span class="string">"qvo111517d8-c5"</span></span><br><span class="line">        Port patch-tun</span><br><span class="line">            Interface patch-tun</span><br><span class="line">                <span class="built_in">type</span>: patch</span><br><span class="line">                options: &#123;peer=patch-int&#125;</span><br><span class="line">        Port <span class="string">"qr-001d0ed9-01"</span></span><br><span class="line">            tag: 2</span><br><span class="line">            Interface <span class="string">"qr-001d0ed9-01"</span></span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">        Port br-int</span><br><span class="line">            Interface br-int</span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">        Port <span class="string">"qr-ddbdc784-d7"</span></span><br><span class="line">            tag: 1</span><br><span class="line">            Interface <span class="string">"qr-ddbdc784-d7"</span></span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">        Port <span class="string">"qvo4e843b99-fb"</span></span><br><span class="line">            tag: 1</span><br><span class="line">            Interface <span class="string">"qvo4e843b99-fb"</span></span><br><span class="line">    Bridge br-ex</span><br><span class="line">        Port br-ex</span><br><span class="line">            Interface br-ex</span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">        Port <span class="string">"fg-081d537b-06"</span></span><br><span class="line">            Interface <span class="string">"fg-081d537b-06"</span></span><br><span class="line">                <span class="built_in">type</span>: internal</span><br><span class="line">    ovs_version: <span class="string">"2.0.2"</span></span><br></pre></td></tr></table></figure><p>而端口qvo4e843b99-fb是属于vlan 1的，arp广播包会转发到”qr-ddbdc784-d7”和”patch-tun”。</p><p>首先看”qr-ddbdc784-d7”，这是interface_distributed的接口：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/interface4.png" alt=""></p><p>这个接口是在compute node的的DVR中的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">root@dvr-compute1:~<span class="comment"># ip netns</span></span><br><span class="line">fip-fbd46644-c70f-4227-a414-862a00cbd1d2</span><br><span class="line">&lt;font color=DarkRed size=5&gt;qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa&lt;/font&gt;</span><br><span class="line">qdhcp-401f678d-4518-446c-9a33-cd2fb054c104</span><br><span class="line">qdhcp-db755841-0764-4a8f-b962-8df008ce6330</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root@dvr-compute1:~<span class="comment"># ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ifconfig</span></span><br><span class="line">lo        Link encap:Local Loopback  </span><br><span class="line">          inet addr:127.0.0.1  Mask:255.0.0.0</span><br><span class="line">          inet6 addr: ::1/128 Scope:Host</span><br><span class="line">          UP LOOPBACK RUNNING  MTU:65536  Metric:1</span><br><span class="line">          RX packets:0 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0 </span><br><span class="line">          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">qr-001d0ed9-01 Link encap:Ethernet  HWaddr fa:16:3e:69:b4:05  </span><br><span class="line">          inet addr:10.0.2.1  Bcast:10.0.2.255  Mask:255.255.255.0</span><br><span class="line">          inet6 addr: fe80::f816:3eff:fe69:b405/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING  MTU:1500  Metric:1</span><br><span class="line">          RX packets:35 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:14 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0 </span><br><span class="line">          RX bytes:3510 (3.5 KB)  TX bytes:1092 (1.0 KB)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;font color=DarkRed size=5&gt;</span><br><span class="line">qr-ddbdc784-d7 Link encap:Ethernet  HWaddr fa:16:3e:66:13:af  </span><br><span class="line">          inet addr:10.0.1.1  Bcast:10.0.1.255  Mask:255.255.255.0</span><br><span class="line">          inet6 addr: fe80::f816:3eff:fe66:13af/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING  MTU:1500  Metric:1</span><br><span class="line">          RX packets:401 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:378 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:0 </span><br><span class="line">          RX bytes:38224 (38.2 KB)  TX bytes:36224 (36.2 KB)</span><br><span class="line">&lt;/font&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rfp-0fbb351e<span class="_">-a</span> Link encap:Ethernet  HWaddr ea:5c:56:9a:36:9c  </span><br><span class="line">          inet addr:169.254.31.28  Bcast:0.0.0.0  Mask:255.255.255.254</span><br><span class="line">          inet6 addr: fe80::e85c:56ff:fe9a:369c/64 Scope:Link</span><br><span class="line">          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</span><br><span class="line">          RX packets:12 errors:0 dropped:0 overruns:0 frame:0</span><br><span class="line">          TX packets:12 errors:0 dropped:0 overruns:0 carrier:0</span><br><span class="line">          collisions:0 txqueuelen:1000 </span><br><span class="line">          RX bytes:1116 (1.1 KB)  TX bytes:1116 (1.1 KB)</span><br></pre></td></tr></table></figure><p>接口qr-ddbdc784-d7拥有10.0.1.1。所以他会响应ARP请求。</p><p>回过头来看”patch-tun”, ARP请求转发到这个接口后，会转发到br-tun。看一下br-tun上的flow, 目前我们只需要看红色部分，他会将目标地址为10.0.1.1的ARP请求丢弃：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">root@dvr-compute1:~<span class="comment"># ovs-ofctl dump-flows br-tun </span></span><br><span class="line">NXST_FLOW reply (xid=0x4): </span><br><span class="line">。。。</span><br><span class="line">cookie=0x0, duration=64720.432s, table=1, n_packets=4, n_bytes=168, idle_age=64607, priority=3,arp,dl_vlan=1,arp_tpa=10.0.1.1 actions=drop </span><br><span class="line">cookie=0x0, duration=62666.766s, table=1, n_packets=2, n_bytes=84, idle_age=62576, priority=3,arp,dl_vlan=2,arp_tpa=10.0.2.1 actions=drop </span><br><span class="line">。。。</span><br></pre></td></tr></table></figure><p>回到我们虚机，当获取到了10.0.1.1的MAC地址后，会发出如下的包：<br>Dest IP: 10.0.2.5<br>Souce IP: 10.0.1.5<br>Dest MAC: MAC of 10.0.1.1<br>Source MAC: MAC of 10.0.1.5</p><p>之后包被转发到compute1的qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa 的namespace：<br>这里利用了内核的高级路由到了，首先看一下ip rule：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip rule<br>0: from all lookup local<br>32766: from all lookup main<br>32767: from all lookup default<br>32768: from 10.0.1.5 lookup 16<br>32769: from 10.0.2.3 lookup 16<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772673: from 10.0.2.1/24 lookup 167772673 </p><p>可以看到会先查找main表：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip route list table main<br>10.0.1.0/24 dev qr-ddbdc784-d7 proto kernel scope link src 10.0.1.1<br>10.0.2.0/24 dev qr-001d0ed9-01 proto kernel scope link src 10.0.2.1<br>169.254.31.28/31 dev rfp-0fbb351e-a proto kernel scope link src 169.254.31.28</p><p>在main表中满足以下路由:<br>10.0.2.0/24 dev qr-001d0ed9-01 proto kernel scope link src 10.0.2.1<br>因此会从qr-001d0ed9-01转发出去。</p><p>之后需要去查询10.0.2.5的MAC地址， MAC是由neutron使用静态ARP的方式设定的：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip nei<br>10.0.1.5 dev qr-ddbdc784-d7 lladdr fa:16:3e:da:75:6d PERMANENT<br>10.0.2.3 dev qr-001d0ed9-01 lladdr fa:16:3e:a4:fc:98 PERMANENT<br>10.0.1.6 dev qr-ddbdc784-d7 lladdr fa:16:3e:9f:55:67 PERMANENT<br>10.0.2.2 dev qr-001d0ed9-01 lladdr fa:16:3e:13:55:66 PERMANENT</p><p><font color="DarkRed" size="4">10.0.2.5 dev qr-001d0ed9-01 lladdr fa:16:3e:51:99:b8 PERMANENT</font><br>10.0.1.4 dev qr-ddbdc784-d7 lladdr fa:16:3e:da:e3:6e PERMANENT<br>10.0.1.7 dev qr-ddbdc784-d7 lladdr fa:16:3e:14:b8:ec PERMANENT<br>169.254.31.29 dev rfp-0fbb351e-a lladdr 42:0d:9f:49:63:c6 STALE</p><p>由于Neutron知道所有虚机的信息，因此他可以事先设定好静态ARP。<br>至此，我们的ICMP包会变成以下形式从qr-001d0ed9-01转发出去：<br>Dest IP: 10.0.2.5<br>Souce IP: 10.0.1.5<br>Dest MAC: MAC of 10.0.2.5<br>Source MAC: MAC of 10.0.2.1</p><p>当包转发到”br-tun”后，进开始查询openflow表。<br>首先我们看一下br-tun的接口状况：<br>root@dvr-compute1:~# ovs-ofctl show br-tun<br>OFPT_FEATURES_REPLY (xid=0x2): dpid:0000e2b7aa5da34a<br>n_tables:254, n_buffers:256<br>capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP<br>actions: OUTPUT SET_VLAN_VID SET_VLAN_PCP STRIP_VLAN SET_DL_SRC SET_DL_DST SET_NW_SRC SET_NW_DST SET_NW_TOS SET_TP_SRC SET_TP_DST ENQUEUE<br> 1(patch-int): addr:76:ae:9f:b3:bf:c6<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> 3(vxlan-0ae09f88): addr:92:61:e9:43:dd:99<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> 4(vxlan-0ae09f91): addr:2e:cc:c0:4a:4e:d4<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> LOCAL(br-tun): addr:e2:b7:aa:5d:a3:4a<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br>OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0</p><p>首先我们看一下br-tun的flowtable，首先会进入table 0，由于包是从br-int发过来的，因此in_port是patch-int(1)，之后会查询表1：<br> cookie=0x0, duration=66172.51s, table=0, n_packets=58, n_bytes=5731, idle_age=20810, hard_age=65534, priority=1,in_port=3 actions=resubmit(,4)<br> cookie=0x0, duration=67599.526s, table=0, n_packets=273, n_bytes=24999, idle_age=1741, hard_age=65534, priority=1,in_port=1 actions=resubmit(,1)<br> cookie=0x0, duration=64437.052s, table=0, n_packets=28, n_bytes=2980, idle_age=20799, priority=1,in_port=4 actions=resubmit(,4)<br> cookie=0x0, duration=67601.704s, table=0, n_packets=5, n_bytes=390, idle_age=65534, hard_age=65534, priority=0 actions=drop</p><p>表1，这张表中会丢弃目标地址是interface_distributed接口的ARP和目的MAC是interface_distributed的包。以防止虚机发送给本地IR的包不会被转发到网络中。<br>我们的ICMP包会命中一下flow，它会把源MAC地址改为全局唯一和计算节点绑定的MAC:<br> cookie=0x0, duration=66135.811s, table=1, n_packets=140, n_bytes=13720, idle_age=65534, hard_age=65534, priority=1,dl_vlan=1,dl_src=fa:16:3e:66:13:af actions=mod_dl_src:fa:16:3f:fe:49:e9,resubmit(,2)<br> cookie=0x0, duration=64082.141s, table=1, n_packets=2, n_bytes=200, idle_age=64081, priority=1,dl_vlan=2,dl_src=fa:16:3e:69:b4:05 actions=mod_dl_src:fa:16:3f:fe:49:e9,resubmit(,2)<br> cookie=0x0, duration=66135.962s, table=1, n_packets=1, n_bytes=98, idle_age=65301, hard_age=65534, priority=2,dl_vlan=1,dl_dst=fa:16:3e:66:13:af actions=drop<br> cookie=0x0, duration=64082.297s, table=1, n_packets=0, n_bytes=0, idle_age=64082, priority=2,dl_vlan=2,dl_dst=fa:16:3e:69:b4:05 actions=drop<br> cookie=0x0, duration=66136.115s, table=1, n_packets=4, n_bytes=168, idle_age=65534, hard_age=65534, priority=3,arp,dl_vlan=1,arp_tpa=10.0.1.1 actions=drop<br> cookie=0x0, duration=64082.449s, table=1, n_packets=2, n_bytes=84, idle_age=63991, priority=3,arp,dl_vlan=2,arp_tpa=10.0.2.1 actions=drop<br> cookie=0x0, duration=67599.22s, table=1, n_packets=123, n_bytes=10687, idle_age=1741, hard_age=65534, priority=0 actions=resubmit(,2)</p><p>这个全局唯一和计算节点绑定的MAC地址，是由neutron全局分配的，数据库中可以看到这个MAC是每个host一个：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/mac1.png" alt=""></p><p>它的base MAC是可以在neutron.conf中配置的：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/mac2.png" alt=""></p><p>继续查询流表2，表2是VXLAN表，如果是广播包就会查询表22，如果是单播包就查询表20：<br> cookie=0x0, duration=67601.554s, table=2, n_packets=176, n_bytes=16981, idle_age=20810, hard_age=65534, priority=0,dl_dst=00:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,20)<br> cookie=0x0, duration=67601.406s, table=2, n_packets=92, n_bytes=7876, idle_age=1741, hard_age=65534, priority=0,dl_dst=01:00:00:00:00:00/01:00:00:00:00:00 actions=resubmit(,22)</p><p>ICMP包是单播包，因此会查询表20，由于开启了L2 pop功能，在表20中会事先学习到应该转发到哪个VTEP。<br> cookie=0x0, duration=64076.431s, table=20, n_packets=0, n_bytes=0, idle_age=64076, priority=2,dl_vlan=2,dl_dst=fa:16:3e:13:55:66 actions=strip_vlan,set_tunnel:0x3eb,output:3<br> cookie=0x0, duration=66130.899s, table=20, n_packets=152, n_bytes=14728, idle_age=65534, hard_age=65534, priority=2,dl_vlan=1,dl_dst=fa:16:3e:9f:55:67 actions=strip_vlan,set_tunnel:0x3e9,output:3<br> cookie=0x0, duration=66560.59s, table=20, n_packets=7, n_bytes=552, idle_age=65534, hard_age=65534, priority=2,dl_vlan=1,dl_dst=fa:16:3e:da:e3:6e actions=strip_vlan,set_tunnel:0x3e9,output:2<br> cookie=0x0, duration=64436.717s, table=20, n_packets=0, n_bytes=0, idle_age=64436, priority=2,dl_vlan=1,dl_dst=fa:16:3e:14:b8:ec actions=strip_vlan,set_tunnel:0x3e9,output:4<br> cookie=0x0, duration=64015.308s, table=20, n_packets=0, n_bytes=0, idle_age=64015, priority=2,dl_vlan=2,dl_dst=fa:16:3e:51:99:b8 actions=strip_vlan,set_tunnel:0x3eb,output:4<br> cookie=0x0, duration=64032.699s, table=20, n_packets=9, n_bytes=917, idle_age=20810, priority=2,dl_vlan=2,dl_dst=fa:16:3e:bb:cf:66 actions=strip_vlan,set_tunnel:0x3eb,output:3<br> cookie=0x0, duration=67600.802s, table=20, n_packets=8, n_bytes=784, idle_age=65534, hard_age=65534, priority=0 actions=resubmit(,22)</p><p>注：<br>由于L2 POP并不是本文的重点。因此不在此细说。如果有兴趣可以看以下blog:<br><a href="http://assafmuller.com/category/overlays/" target="_blank" rel="noopener">http://assafmuller.com/category/overlays/</a></p><p>此时包会变成如下形式：<br>Dest IP: 10.0.2.5<br>Souce IP: 10.0.1.5<br>Dest MAC: MAC of 10.0.2.5<br>Source MAC: fa:16:3f:fe:49:e9</p><p>之后包会从port 4发出：<br>root@dvr-compute1:~# ovs-vsctl show<br>67f121bd-cca7-41c2-95ab-23ed85d1305b<br>    Bridge br-tun<br>        Port patch-int<br>            Interface patch-int<br>                type: patch<br>                options: {peer=patch-tun}<br>        Port “vxlan-0ae09f91”<br>            Interface “vxlan-0ae09f91”<br>                type: vxlan<br>                options: {df_default=”true”, in_key=flow, local_ip=”10.224.159.141”, out_key=flow, remote_ip=”10.224.159.145”}<br>        Port “vxlan-0ae09f88”<br>            Interface “vxlan-0ae09f88”<br>                type: vxlan<br>                options: {df_default=”true”, in_key=flow, local_ip=”10.224.159.141”, out_key=flow, remote_ip=”10.224.159.136”}<br>        Port br-tun<br>            Interface br-tun<br>                type: internal</p><p>OVS会将此包进行VXLAN封装，将L2帧分装到VXLAN中，包头如下：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/vlan.png" alt=""></p><p>OVS会将此包进行VXLAN封装，将L2帧分装到VXLAN中，包头如下：<br>本文并不想具体讨论VXLAN是如何封装的，简单的说就是讲二层帧封到了一个UDP包中。</p><p>之后compute2会收到这个包，在compute2的br-tun上查询流表:<br>首先看一下接口情况：<br>root@dvr-compute2:~# ovs-ofctl show br-tun<br>OFPT_FEATURES_REPLY (xid=0x2): dpid:000062e9fb8b8f42<br>n_tables:254, n_buffers:256<br>capabilities: FLOW_STATS TABLE_STATS PORT_STATS QUEUE_STATS ARP_MATCH_IP<br>actions: OUTPUT SET_VLAN_VID SET_VLAN_PCP STRIP_VLAN SET_DL_SRC SET_DL_DST SET_NW_SRC SET_NW_DST SET_NW_TOS SET_TP_SRC SET_TP_DST ENQUEUE<br> 1(patch-int): addr:02:dc:f1:96:db:bd<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> 3(vxlan-0ae09f88): addr:b6:4b:d0:83:07:52<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> 4(vxlan-0ae09f8d): addr:12:e5:36:2c:1a:36<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br> LOCAL(br-tun): addr:62:e9:fb:8b:8f:42<br>     config:     0<br>     state:      0<br>     speed: 0 Mbps now, 0 Mbps max<br>OFPT_GET_CONFIG_REPLY (xid=0x4): frags=normal miss_send_len=0</p><p>在table0中可以看到，如果包是从外部发来的就会去查询表4：<br> cookie=0x0, duration=66293.658s, table=0, n_packets=31, n_bytes=3936, idle_age=22651, hard_age=65534, priority=1,in_port=3 actions=resubmit(,4)<br> cookie=0x0, duration=69453.368s, table=0, n_packets=103, n_bytes=9360, idle_age=22651, hard_age=65534, priority=1,in_port=1 actions=resubmit(,1)<br> cookie=0x0, duration=66292.808s, table=0, n_packets=20, n_bytes=1742, idle_age=3598, hard_age=65534, priority=1,in_port=4 actions=resubmit(,4)<br> cookie=0x0, duration=69455.675s, table=0, n_packets=5, n_bytes=390, idle_age=65534, hard_age=65534, priority=0 actions=drop</p><p>在表4中，会将tun_id对应的改为本地vlan id，之后查询表9:<br> cookie=0x0, duration=65937.871s, table=4, n_packets=32, n_bytes=3653, idle_age=22651, hard_age=65534, priority=1,tun_id=0x3eb actions=mod_vlan_vid:3,resubmit(,9)<br> cookie=0x0, duration=66294.732s, table=4, n_packets=19, n_bytes=2025, idle_age=3598, hard_age=65534, priority=1,tun_id=0x3e9 actions=mod_vlan_vid:2,resubmit(,9)<br> cookie=0x0, duration=69455.115s, table=4, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=0 actions=drop</p><p>在表9中，如果发现包的源地址是全局唯一并与计算节点绑定的MAC地址，就将其转发到br-int:<br> cookie=0x0, duration=69453.507s, table=9, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=1,dl_src=fa:16:3f:fe:49:e9 actions=output:1<br> cookie=0x0, duration=69453.782s, table=9, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=1,dl_src=fa:16:3f:72:3f:a7 actions=output:1<br> cookie=0x0, duration=69453.23s, table=9, n_packets=56, n_bytes=6028, idle_age=3598, hard_age=65534, priority=0 actions=resubmit(,10)</p><p>由于我们的源MAC为fa:16:3f:fe:49:e9，我们的ICMP包就被转发到了br-int，之后查询br-int的流表：<br>在表0中，如果是全局唯一并与计算节点绑定的MAC地址就查询表1，否则就正常转发：<br> cookie=0x0, duration=70039.903s, table=0, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=2,in_port=6,dl_src=fa:16:3f:72:3f:a7 actions=resubmit(,1)<br> cookie=0x0, duration=70039.627s, table=0, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=2,in_port=6,dl_src=fa:16:3f:fe:49:e9 actions=resubmit(,1)<br> cookie=0x0, duration=70040.053s, table=0, n_packets=166, n_bytes=15954, idle_age=4184, hard_age=65534, priority=1 actions=NORMAL</p><p>在表1中，事先设定好了flow，如果目的MAC是发送给private2-compute2-VM，就将源MAC改为private2的网关MAC地址：<br> cookie=0x0, duration=66458.695s, table=1, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=4,dl_vlan=3,dl_dst=fa:16:3e:51:99:b8 actions=strip_vlan,mod_dl_src:fa:16:3e:69:b4:05,output:12<br> cookie=0x0, duration=66877.515s, table=1, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=4,dl_vlan=2,dl_dst=fa:16:3e:14:b8:ec actions=strip_vlan,mod_dl_src:fa:16:3e:66:13:af,output:9<br> cookie=0x0, duration=66877.369s, table=1, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=2,ip,dl_vlan=2,nw_dst=10.0.1.0/24 actions=strip_vlan,mod_dl_src:fa:16:3e:66:13:af,output:9<br> cookie=0x0, duration=66458.559s, table=1, n_packets=0, n_bytes=0, idle_age=65534, hard_age=65534, priority=2,ip,dl_vlan=3,nw_dst=10.0.2.0/24 actions=strip_vlan,mod_dl_src:fa:16:3e:69:b4:05,output:12</p><p>还可以看到下面两条rule是网段flow的rule，他们的output是一个list，会将此包转发到所有连接到此network上。<br>如果所有的虚机的rule都已经事先设定好的话，这两条rule应该并没有实际作用，等到代码稳定后，这两条rule应该会被删除。</p><p>经过br-int的流表后，包会变成如下形式：<br>Dest IP: 10.0.2.5<br>Souce IP: 10.0.1.5<br>Dest MAC: MAC of 10.0.2.5<br>Source MAC: fa:16:3e:69:b4:05(MAC of 10.0.2.1 网关地址)</p><p>至此，虚机private2-compute2-VM就会收到来自private1-compute1-VM的包了。从通信的过程可以看到，跨网段的东西向流量没有经过网络节点。</p><p>第二种情况 – 南北向流量(虚机有floating ip)<br>以虚机private1-compute1-VM对外通信为例，此虚机拥有floating ip:</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/interface-last.png" alt=""></p><p>比如我们在虚机中ping 8.8.8.8 。首先在虚机中查询路由，和第一种情况一样，虚机会发送给网关。发送的包如下：<br>Dest IP: 8.8.8.8<br>Souce IP: 10.0.1.5<br>Dest MAC: MAC of 10.0.1.1<br>Source MAC: MAC of 10.0.1.5</p><p>查看ip rule:<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip rule<br>0: from all lookup local<br>32766: from all lookup main<br>32767: from all lookup default<br>32768: from 10.0.1.5 lookup 16<br>32769: from 10.0.2.3 lookup 16<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772673: from 10.0.2.1/24 lookup 167772673</p><p>在main表中没有合适的路由：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip route list table main<br>10.0.1.0/24 dev qr-ddbdc784-d7 proto kernel scope link src 10.0.1.1<br>10.0.2.0/24 dev qr-001d0ed9-01 proto kernel scope link src 10.0.2.1<br>169.254.31.28/31 dev rfp-0fbb351e-a proto kernel scope link src 169.254.31.28</p><p>由于包是从10.0.1.5发来的之后会查看table 16:<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip route list table 16<br>default via 169.254.31.29 dev rfp-0fbb351e-a<br>包会命中这条路由。</p><p>路由之后会通过netfilter的POSTROUTING链中进行SNAT：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa iptables -nvL -t nat<br>。。。<br>Chain neutron-l3-agent-float-snat (1 references)<br> pkts bytes target prot opt in out source destination<br>    0 0 SNAT all – <em> </em> 10.0.2.3 0.0.0.0/0 to:172.24.4.7<br>    0 0 SNAT all – <em> </em> 10.0.1.5 0.0.0.0/0 to:172.24.4.5<br>。。。</p><p>之后就可以看到包会通过rfp-0fbb351e-a发送给169.254.31.29。</p><p>端口rfp-0fbb351e-a和fpr-0fbb351e-a是一对veth pair。在fip namespace中你可以看到这个接口：<br>root@dvr-compute1:~# ip netns exec fip-fbd46644-c70f-4227-a414-862a00cbd1d2 ifconfig<br>fg-081d537b-06 Link encap:Ethernet  HWaddr fa:16:3e:a4:eb:6b<br>          inet addr:172.24.4.6  Bcast:172.24.4.255  Mask:255.255.255.0<br>          inet6 addr: fe80::f816:3eff:fea4:eb6b/64 Scope:Link<br>          UP BROADCAST RUNNING  MTU:1500  Metric:1<br>          RX packets:0 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:50 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:0 (0.0 B)  TX bytes:2512 (2.5 KB)</p><p>fpr-0fbb351e-a Link encap:Ethernet  HWaddr 42:0d:9f:49:63:c6<br>          inet addr:169.254.31.29  Bcast:0.0.0.0  Mask:255.255.255.254<br>          inet6 addr: fe80::400d:9fff:fe49:63c6/64 Scope:Link<br>          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1<br>          RX packets:12 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:12 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:1000<br>          RX bytes:1116 (1.1 KB)  TX bytes:1116 (1.1 KB)</p><p>lo        Link encap:Local Loopback<br>          inet addr:127.0.0.1  Mask:255.0.0.0<br>          inet6 addr: ::1/128 Scope:Host<br>          UP LOOPBACK RUNNING  MTU:65536  Metric:1<br>          RX packets:13 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:13 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:1250 (1.2 KB)  TX bytes:1250 (1.2 KB)</p><p>到了fip的namespace之后，会查询路由， 这里有通往公网的默认路由：<br>root@dvr-compute1:~# ip netns exec fip-fbd46644-c70f-4227-a414-862a00cbd1d2 ip route<br>default via 172.24.4.1 dev fg-081d537b-06<br>169.254.31.28/31 dev fpr-0fbb351e-a proto kernel scope link src 169.254.31.29<br>172.24.4.0/24 dev fg-081d537b-06 proto kernel scope link src 172.24.4.6<br>172.24.4.5 via 169.254.31.28 dev fpr-0fbb351e-a<br>172.24.4.7 via 169.254.31.28 dev fpr-0fbb351e-a</p><p>通过fg-081d537b-06 发送到br-ex。这是从虚机发送到公网的过程。</p><p>反过来，从外网发起连接到虚机时，在fip的namespace会做arp代理：<br>root@dvr-compute1:~# ip netns exec fip-fbd46644-c70f-4227-a414-862a00cbd1d2 sysctl net.ipv4.conf.fg-081d537b-06.proxy_arp<br>net.ipv4.conf.fg-081d537b-06.proxy_arp = 1</p><p>可以看到接口的arp代理是打开的，对于floating ip 有以下两条路由：<br>root@dvr-compute1:~# ip netns exec fip-fbd46644-c70f-4227-a414-862a00cbd1d2 ip route<br>。。。<br>172.24.4.5 via 169.254.31.28 dev fpr-0fbb351e-a<br>172.24.4.7 via 169.254.31.28 dev fpr-0fbb351e-a<br>。。。</p><p>ARP会去通过VETH Pair到IR的namespace中去查询，在IR中可以看到，接口rfp-0fbb351e-a配置了floating ip:<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip addr<br>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default<br>    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00<br>    inet 127.0.0.1/8 scope host lo<br>       valid_lft forever preferred_lft forever<br>    inet6 ::1/128 scope host<br>       valid_lft forever preferred_lft forever<br>2: rfp-0fbb351e-a: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000<br>    link/ether ea:5c:56:9a:36:9c brd ff:ff:ff:ff:ff:ff<br>    inet 169.254.31.28/31 scope global rfp-0fbb351e-a<br>       valid_lft forever preferred_lft forever<br>    inet 172.24.4.5/32 brd 172.24.4.5 scope global rfp-0fbb351e-a<br>       valid_lft forever preferred_lft forever<br>    inet 172.24.4.7/32 brd 172.24.4.7 scope global rfp-0fbb351e-a<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::e85c:56ff:fe9a:369c/64 scope link<br>       valid_lft forever preferred_lft forever<br>17: qr-ddbdc784-d7: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default<br>    link/ether fa:16:3e:66:13:af brd ff:ff:ff:ff:ff:ff<br>    inet 10.0.1.1/24 brd 10.0.1.255 scope global qr-ddbdc784-d7<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::f816:3eff:fe66:13af/64 scope link<br>       valid_lft forever preferred_lft forever<br>19: qr-001d0ed9-01: &lt;BROADCAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default<br>    link/ether fa:16:3e:69:b4:05 brd ff:ff:ff:ff:ff:ff<br>    inet 10.0.2.1/24 brd 10.0.2.255 scope global qr-001d0ed9-01<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::f816:3eff:fe69:b405/64 scope link<br>       valid_lft forever preferred_lft forever</p><p>因此fip的namespace会对这两个floating ip进行ARP回应。</p><p>外部发起目标地址为floating ip的请求后，fip会将其转发到IR中，IR的RPOROUTING链中规则如下：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa iptables -nvL -t nat<br>。。。<br>Chain neutron-l3-agent-PREROUTING (1 references)<br> pkts bytes target prot opt in out source destination<br>    0 0 REDIRECT tcp – <em> </em> 0.0.0.0/0 169.254.169.254 tcp dpt:80 redir ports 9697<br>    0 0 DNAT all – <em> </em> 0.0.0.0/0 172.24.4.7 to:10.0.2.3<br>    0 0 DNAT all – <em> </em> 0.0.0.0/0 172.24.4.5 to:10.0.1.5<br>。。。</p><p>这条DNAT规则会将floating ip地址转换为内部地址，之后进行路由查询：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip route<br>10.0.1.0/24 dev qr-ddbdc784-d7 proto kernel scope link src 10.0.1.1<br>10.0.2.0/24 dev qr-001d0ed9-01 proto kernel scope link src 10.0.2.1<br>169.254.31.28/31 dev rfp-0fbb351e-a proto kernel scope link src 169.254.31.28</p><p>目的地址是10.0.1.0/24网段的，因此会从qr-ddbdc784-d7转发出去。之后就会转发到br-int再到虚机。</p><p>第三种情况 – 南北向流量(虚机没有floating ip)<br>在虚机没有floating ip的情况下，从虚机发出的包会首先到IR，IR中查询路由：<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip rule<br>0: from all lookup local<br>32766: from all lookup main<br>32767: from all lookup default<br>32768: from 10.0.1.5 lookup 16<br>32769: from 10.0.2.3 lookup 16<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772417: from 10.0.1.1/24 lookup 167772417<br>167772673: from 10.0.2.1/24 lookup 167772673</p><p>会先查询main表，之后查询167772417表。<br>root@dvr-compute1:~# ip netns exec qrouter-0fbb351e-a65b-4790-a409-8fb219ce16aa ip route list table 167772417<br>default via 10.0.1.6 dev qr-ddbdc784-d7</p><p>这个表会将其转发给10.0.1.6,而这个IP就是在network node上的router_centralized_snat接口。</p><p>在network node的snat namespace中，我们可以看到这个接口：<br>stack@dvr-controller:/root$ sudo ip netns exec snat-0fbb351e-a65b-4790-a409-8fb219ce16aa ifconfig<br>lo        Link encap:Local Loopback<br>          inet addr:127.0.0.1  Mask:255.0.0.0<br>          inet6 addr: ::1/128 Scope:Host<br>          UP LOOPBACK RUNNING  MTU:65536  Metric:1<br>          RX packets:0 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)</p><p>qg-4d15b7f6-cb Link encap:Ethernet  HWaddr fa:16:3e:24:0b:6b<br>          inet addr:172.24.4.4  Bcast:172.24.4.255  Mask:255.255.255.0<br>          inet6 addr: fe80::f816:3eff:fe24:b6b/64 Scope:Link<br>          UP BROADCAST RUNNING  MTU:1500  Metric:1<br>          RX packets:5 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:144 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:210 (210.0 B)  TX bytes:13320 (13.3 KB)</p><p>sg-427653e4-a3 Link encap:Ethernet  HWaddr fa:16:3e:9f:55:67<br>          inet addr:10.0.1.6  Bcast:10.0.1.255  Mask:255.255.255.0<br>          inet6 addr: fe80::f816:3eff:fe9f:5567/64 Scope:Link<br>          UP BROADCAST RUNNING  MTU:1500  Metric:1<br>          RX packets:167 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:52 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:16260 (16.2 KB)  TX bytes:4460 (4.4 KB)</p><p>sg-5df1ec71-d3 Link encap:Ethernet  HWaddr fa:16:3e:13:55:66<br>          inet addr:10.0.2.2  Bcast:10.0.2.255  Mask:255.255.255.0<br>          inet6 addr: fe80::f816:3eff:fe13:5566/64 Scope:Link<br>          UP BROADCAST RUNNING  MTU:1500  Metric:1<br>          RX packets:34 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:12 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:0<br>          RX bytes:3412 (3.4 KB)  TX bytes:952 (952.0 B)</p><p>stack@dvr-controller:/root$ sudo ip netns exec snat-0fbb351e-a65b-4790-a409-8fb219ce16aa iptables -nvL -t nat<br>。。。<br>Chain neutron-l3-agent-snat (1 references)<br> pkts bytes target prot opt in out source destination<br>    0 0 SNAT all – <em> </em> 10.0.1.0/24 0.0.0.0/0 to:172.24.4.4<br>    0 0 SNAT all – <em> </em> 10.0.2.0/24 0.0.0.0/0 to:172.24.4.4<br>。。。</p><p>这里就和以前的L3类似，会将没有floating ip的包SNAT成一个172.24.4.4(DVR的网关臂)。这个过程是和以前L3类似的，不再累述。</p><p>stack@dvr-controller:/root$ sudo ip netns exec snat-0fbb351e-a65b-4790-a409-8fb219ce16aa iptables -nvL -t nat<br>。。。<br>Chain neutron-l3-agent-snat (1 references)<br> pkts bytes target prot opt in out source destination<br>    0 0 SNAT all – <em> </em> 10.0.1.0/24 0.0.0.0/0 to:172.24.4.4<br>    0 0 SNAT all – <em> </em> 10.0.2.0/24 0.0.0.0/0 to:172.24.4.4<br>。。。</p><p>这里就和以前的L3类似，会将没有floating ip的包SNAT成一个172.24.4.4(DVR的网关臂)。这个过程是和以前L3类似的，不再累述。</p><hr><p>原文：<a href="https://blog.csdn.net/matt_mao/article/details/39180135" target="_blank" rel="noopener">https://blog.csdn.net/matt_mao/article/details/39180135</a> </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇文章总结的很好了，偷懒直接转过来，以便日后不时查看。&lt;/p&gt;
&lt;p&gt;首先看一下，没有使用DVR的问题在哪里：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;https://github.com/chendave/chendave.github.io/raw/master/css/i
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>Python基础-__name__ == &#39;__main__&#39;</title>
    <link href="http://jungler.com/2018/10/06/Python%E5%9F%BA%E7%A1%80-name-main/"/>
    <id>http://jungler.com/2018/10/06/Python基础-name-main/</id>
    <published>2018-10-06T07:44:32.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p>今年国庆节过的比较悠闲，中间只有一天来了一个短途去了苏州，其余大部分时间都是在家休息，这是我第二次去苏州，印象中对苏州园林的印象还一直来自于中学课本，园林应该是一个大的公园，有竹林，小桥，流水，假山，凉亭。算得上是一个自然景观吧。这次去亲身体验发现我的假设大部分还是对的，只是它不是一座公园，而是有钱人的私家宅院。读万卷书，行万里路，可见没有实践的永远不能保证是完全正确的，想象和现实总是有着各种差距。</p><p>看了Pence的演讲，即便我无意于或者不忍于贬低自己的祖国，但却又无法说服自己这些个烂疮实实在在的存在于这个国度。无奈，无力，失语。但愿正如Pence引用的那句话，Heaven not only see the future, but also give us the hope!</p><hr><p>回过头来看看Python为什么需要__name__ == ‘__main__’？ 先看看两段代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># executor.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entry</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"implement something here..."</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> (<span class="string">"do something here..."</span>)</span><br><span class="line"><span class="keyword">print</span> __name__</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># caller.py</span></span><br><span class="line"><span class="keyword">import</span> executor</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>如果直接运行第一段程序，我们将得到下面的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;python executor.py</span><br><span class="line"><span class="keyword">do</span> something here...</span><br><span class="line">__main__</span><br></pre></td></tr></table></figure><p>运行第二段程序输出下面的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;python caller.py</span><br><span class="line"><span class="keyword">do</span> something here...</span><br><span class="line">executor</span><br></pre></td></tr></table></figure><p>这里我们得出两条结论：<br>1）Python里可以直接在模块里写不属于任何function的语句，例如第一段程序里的两个print语句，这个有点类似shell，而不同于java或者c语言，这些语句无论在本模块被执行或者被import到其它python文件中去，都会被自动执行，这有时不是我们希望看到的。而实际上这些语句常常被理解为Python的main函数。我们可以忽略所谓的main函数，因为这对于我们的理解无益，但是我们可以将其改写为下面的形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># executor.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">entry</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"implement something here..."</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">print</span> (<span class="string">"do something here..."</span>)</span><br><span class="line">    <span class="keyword">print</span> __name__</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>这样以第一种方式直接执行此文件，得到的将是相同的结果。</p><p>2）__name__被解释为不同的名字，第一种方式被解释为__main__, 而第二种被解释为executor，所以当我们将程序改写为上面的形式的时候，main中的定义的语句这次将得不到执行。这时候将没有任何输出。但同时可以通过显示的调用<em>executor.main()</em>来让这两行语句得到执行。</p><p>所以，python程序中if __name__ == ‘__main__’ 这行语句的目的就是希望有些语句能有条件的得到执行，或者说只有本模块直接运行时才会被调用。这样保证了此模块被其它模块引入时这些语句不会被调用，即便它时所谓的main函数。<br>那什么时候需要用这样的语句呢？一个典型的例子比如在做模块的单元测试的时候。</p><hr><p>[1] <a href="https://interactivepython.org/runestone/static/CS152f17/Functions/mainfunction.html" target="_blank" rel="noopener">https://interactivepython.org/runestone/static/CS152f17/Functions/mainfunction.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;今年国庆节过的比较悠闲，中间只有一天来了一个短途去了苏州，其余大部分时间都是在家休息，这是我第二次去苏州，印象中对苏州园林的印象还一直来自于中学课本，园林应该是一个大的公园，有竹林，小桥，流水，假山，凉亭。算得上是一个自然景观吧。这次去亲身体验发现我的假设大部分还是对的，只
      
    
    </summary>
    
    
      <category term="Python" scheme="http://jungler.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>系统重启后恢复OpenStack网络设置-tips</title>
    <link href="http://jungler.com/2018/09/24/%E7%B3%BB%E7%BB%9F%E9%87%8D%E5%90%AF%E5%90%8E%E6%81%A2%E5%A4%8DOpenStack%E7%BD%91%E7%BB%9C%E8%AE%BE%E7%BD%AE-tips/"/>
    <id>http://jungler.com/2018/09/24/系统重启后恢复OpenStack网络设置-tips/</id>
    <published>2018-09-24T09:22:59.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p>眼看着今天就要过去了，一个月就要过去了，马上一年也就要过去了，可你又能怎样？<br>昨天在焦虑，今天还在焦虑，明天将继续焦虑，何时能停止？<br>既然无力挣扎，那就闭着眼睛过吧，时间最终会给我们答案，尘过尘，土归土，看谈一些就好，看空一些就好。</p><p>不管OpenStack是不是还有些把玩的价值，但终归割舍不下，我一直把它看作一本书，一本可以提升自己的书，至于它能带给你什么？ Who knows? Who cares?</p><p>所以，当实验室里每次断电之后，虚拟网络都将无法工作，我还是会继续去<em>stack</em>一下，终于有一天我无法再次忍受，我要去看个究竟，当愚钝的翻过一行一行脚本之后，我好像找到了答案。</p><p>先看看<em>lib/neutron_plugins/services/l3</em>里的这个函数：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">function</span> _configure_neutron_l3_agent &#123;</span><br><span class="line"></span><br><span class="line">    cp <span class="variable">$NEUTRON_DIR</span>/etc/l3_agent.ini.sample <span class="variable">$Q_L3_CONF_FILE</span></span><br><span class="line"></span><br><span class="line">    iniset <span class="variable">$Q_L3_CONF_FILE</span> DEFAULT debug <span class="variable">$ENABLE_DEBUG_LOG_LEVEL</span></span><br><span class="line">    iniset <span class="variable">$Q_L3_CONF_FILE</span> AGENT root_helper <span class="string">"<span class="variable">$Q_RR_COMMAND</span>"</span></span><br><span class="line">    <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$Q_USE_ROOTWRAP_DAEMON</span>"</span> == <span class="string">"True"</span> ]]; <span class="keyword">then</span></span><br><span class="line">        iniset <span class="variable">$Q_L3_CONF_FILE</span> AGENT root_helper_daemon <span class="string">"<span class="variable">$Q_RR_DAEMON_COMMAND</span>"</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">    _neutron_setup_interface_driver <span class="variable">$Q_L3_CONF_FILE</span></span><br><span class="line"></span><br><span class="line">    neutron_plugin_configure_l3_agent <span class="variable">$Q_L3_CONF_FILE</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># If we've given a PUBLIC_INTERFACE to take over, then we assume</span></span><br><span class="line">    <span class="comment"># that we can own the whole thing, and privot it into the OVS</span></span><br><span class="line">    <span class="comment"># bridge. If we are not, we're probably on a single interface</span></span><br><span class="line">    <span class="comment"># machine, and we just setup NAT so that fixed guests can get out.</span></span><br><span class="line">    <span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> ]]; <span class="keyword">then</span>   <span class="comment"># *看这里!* </span></span><br><span class="line">        _move_neutron_addresses_route <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> <span class="string">"<span class="variable">$OVS_PHYSICAL_BRIDGE</span>"</span> True False <span class="string">"inet"</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> [[ $(ip -f inet6 a s dev <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> | grep -c <span class="string">'global'</span>) != 0 ]]; <span class="keyword">then</span></span><br><span class="line">            _move_neutron_addresses_route <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> <span class="string">"<span class="variable">$OVS_PHYSICAL_BRIDGE</span>"</span> False False <span class="string">"inet6"</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> <span class="variable">$default_v4_route_devs</span>; <span class="keyword">do</span></span><br><span class="line">            sudo iptables -t nat -A POSTROUTING -o <span class="variable">$d</span> -s <span class="variable">$FLOATING_RANGE</span> -j MASQUERADE</span><br><span class="line">        <span class="keyword">done</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从这个函数可以看出，如果绑定了某一个物理网卡（例如在<em>localrc</em>中配置了”PUBLIC_INTERFACE”），那么将会调用”_move_neutron_addresses_route”来做进一步处理，否则就做一个源地址伪装（MASQUERADE）就算完了。</p><p>核心就是这段代码了，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">_move_neutron_addresses_route <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> <span class="string">"<span class="variable">$OVS_PHYSICAL_BRIDGE</span>"</span> True False <span class="string">"inet"</span></span><br></pre></td></tr></table></figure></p><p>来重点看下绑定物理网卡的处理方式，函数有点长，我们调重点的看一下部分核心代码，<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># lib/neutron-legacy</span></span><br><span class="line"><span class="comment"># _move_neutron_addresses_route() - Move the primary IP to the OVS bridge</span></span><br><span class="line"><span class="comment"># on startup, or back to the public interface on cleanup. If no IP is</span></span><br><span class="line"><span class="comment"># configured on the interface, just add it as a port to the OVS bridge.</span></span><br><span class="line"><span class="keyword">function</span> _move_neutron_addresses_route &#123;</span><br><span class="line">...</span><br><span class="line">    <span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$from_intf</span>"</span> &amp;&amp; -n <span class="string">"<span class="variable">$to_intf</span>"</span> ]]; <span class="keyword">then</span></span><br><span class="line">...</span><br><span class="line">        DEFAULT_ROUTE_GW=$(ip -f <span class="variable">$af</span> r | awk <span class="string">"/default.+<span class="variable">$from_intf</span>\s/ &#123; print \$3; exit &#125;"</span>) </span><br><span class="line">        IP_BRD=$(ip -f <span class="variable">$af</span> a s dev <span class="variable">$from_intf</span> scope global primary | grep inet | awk <span class="string">'&#123; print $2, $3, $4; exit &#125;'</span>) <span class="comment">#①</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> [ <span class="string">"<span class="variable">$DEFAULT_ROUTE_GW</span>"</span> != <span class="string">""</span> ]; <span class="keyword">then</span></span><br><span class="line">            ADD_DEFAULT_ROUTE=<span class="string">"sudo ip -f <span class="variable">$af</span> r replace default via <span class="variable">$DEFAULT_ROUTE_GW</span> dev <span class="variable">$to_intf</span>"</span> <span class="comment"># ②</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$add_ovs_port</span>"</span> == <span class="string">"True"</span> ]]; <span class="keyword">then</span></span><br><span class="line">            ADD_OVS_PORT=<span class="string">"sudo ovs-vsctl --may-exist add-port <span class="variable">$to_intf</span> <span class="variable">$from_intf</span>"</span> <span class="comment"># ③</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">...</span><br><span class="line">        <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$IP_BRD</span>"</span> != <span class="string">""</span> ]]; <span class="keyword">then</span></span><br><span class="line">            IP_DEL=<span class="string">"sudo ip addr del <span class="variable">$IP_BRD</span> dev <span class="variable">$from_intf</span>"</span> <span class="comment"># ④</span></span><br><span class="line">            IP_REPLACE=<span class="string">"sudo ip addr replace <span class="variable">$IP_BRD</span> dev <span class="variable">$to_intf</span>"</span> <span class="comment"># ⑤ </span></span><br><span class="line">            IP_UP=<span class="string">"sudo ip link set <span class="variable">$to_intf</span> up"</span> <span class="comment"># ⑥</span></span><br><span class="line">            <span class="keyword">if</span> [[ <span class="string">"<span class="variable">$af</span>"</span> == <span class="string">"inet"</span> ]]; <span class="keyword">then</span></span><br><span class="line">                IP=$(<span class="built_in">echo</span> <span class="variable">$IP_BRD</span> | awk <span class="string">'&#123; print $1; exit &#125;'</span> | grep -o -E <span class="string">'(.*)/'</span> | cut -d <span class="string">"/"</span> -f1)</span><br><span class="line">                ARP_CMD=<span class="string">"arping -A -c 3 -w 4.5 -I <span class="variable">$to_intf</span> <span class="variable">$IP</span> "</span> <span class="comment"># ⑦</span></span><br><span class="line">            <span class="keyword">fi</span></span><br><span class="line">        <span class="keyword">fi</span></span><br><span class="line">...</span><br><span class="line">        <span class="variable">$DEL_OVS_PORT</span>; <span class="variable">$IP_DEL</span>; <span class="variable">$IP_REPLACE</span>; <span class="variable">$IP_UP</span>; <span class="variable">$ADD_OVS_PORT</span>; <span class="variable">$ADD_DEFAULT_ROUTE</span>; <span class="variable">$ARP_CMD</span></span><br></pre></td></tr></table></figure></p><p>先来看下网络的路由配置情况，<em>eno2</em>将会作为<em>PUBLIC_INTERFACE</em>被绑定到<em>br-ex</em>上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ip route</span><br><span class="line">default via 192.168.18.1 dev eno2  proto static  metric 100</span><br><span class="line">169.254.0.0/16 dev docker0  scope link  metric 1000 linkdown</span><br><span class="line">172.17.0.0/16 dev docker0  proto kernel  scope link  src 172.17.0.1 linkdown</span><br><span class="line">192.168.16.0/21 dev eno2  proto kernel  scope link  src 192.168.18.24  metric 100</span><br></pre></td></tr></table></figure><ol><li><p>这两句是获取系统当前的一些网络配置，<em>DEFAULT_ROUTE_GW</em>是物理机的默认路由，<em>IP_BRD</em>得到的是物理网卡上的主IP地址配置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ip -f inet r | awk <span class="string">"/default.+eno2\s/ &#123; print \$3; exit &#125;"</span></span><br><span class="line">192.168.18.1</span><br><span class="line"></span><br><span class="line">$ ip -f inet a s dev eno2 scope global primary | grep inet | awk <span class="string">'&#123; print $2, $3, $4; exit &#125;'</span></span><br><span class="line">192.168.18.24/21 brd 192.168.23.255</span><br></pre></td></tr></table></figure></li><li><p>将默认路由替换为目标网卡，这里当然是替换为OVS的bridge <em>br-ex</em>，这一步之后默认的路由的IP地址虽然没变，但是device已经改为<em>br-ex</em>了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip -f inet r replace default via 192.168.18.1 dev br-ex</span><br></pre></td></tr></table></figure></li><li><p>将物理网卡绑定到<em>br-ex</em>上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ovs-vsctl --may-exist add-port br-ex eno2</span><br></pre></td></tr></table></figure></li><li><p>这一步将物理网卡上的主IP地址删除，之后该地址将配置到<em>br-ex</em>上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip addr del 192.168.18.24/21 brd 192.168.23.255 dev eno2</span><br></pre></td></tr></table></figure></li><li><p>将物理网卡上的主IP地址配置到<em>br-ex</em>上。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip addr replace 192.168.18.24/21 brd 192.168.23.255 dev br-ex</span><br></pre></td></tr></table></figure></li><li><p>不解释</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip link <span class="built_in">set</span> br-ex up</span><br></pre></td></tr></table></figure></li><li><p>设置ARP请求的一些参数，arping没用过，具体在干什么，还是不太了解。</p></li></ol><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ arping -A -c 3 -w 4.5 -I br-ex 192.168.18.24</span><br></pre></td></tr></table></figure><p>之后就是顺序执行这几个命令了，一句话来解释这段代码干的事情就是这段注释:</p><blockquote><p>Move the primary IP to the OVS bridge on startup, or back to the<br>public interface on cleanup. If no IP is configured on the interface,<br>just add it as a port to the OVS bridge.</p></blockquote><p>如果系统reboot了，网络挂了，不行就顺序再来一便吧，但问题是为什么网络不能自动恢复？这是个大问题啊？！</p><p>希望国庆节之后能对DVR来做个了结吧。</p><p>p.s. 天又晚了，码字确实费时间。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;眼看着今天就要过去了，一个月就要过去了，马上一年也就要过去了，可你又能怎样？&lt;br&gt;昨天在焦虑，今天还在焦虑，明天将继续焦虑，何时能停止？&lt;br&gt;既然无力挣扎，那就闭着眼睛过吧，时间最终会给我们答案，尘过尘，土归土，看谈一些就好，看空一些就好。&lt;/p&gt;
&lt;p&gt;不管OpenS
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>DevStack激活DVR相关配置</title>
    <link href="http://jungler.com/2018/08/11/DevStack%E6%BF%80%E6%B4%BBDVR%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE/"/>
    <id>http://jungler.com/2018/08/11/DevStack激活DVR相关配置/</id>
    <published>2018-08-11T08:57:25.000Z</published>
    <updated>2022-06-19T11:49:19.349Z</updated>
    
    <content type="html"><![CDATA[<p>这些日子脑子里经常会想到《可可西里》电影里的一个画面，从刚陷入到流沙时的垂死挣扎越陷越深到后来彻底绝望放弃，电影表达了个人在恶劣的自然环境下的无能无力，换个角度想想，我们绝大多数人何尝不是已经陷入了深不见底的绝望之中，只不过这里不是沙漠戈壁而是同样现实的社会，我们挣扎着，不想认命，不想就此结束，但是结果常常是在岁月的流逝中，一步步走向属于我们这代人的结局。</p><p>DVR(Distributed virtual router)已经理解的差不多了，一句话来总结DVR就是DVR可以网络流量的负载均衡，以解决过往所有流量需要网络节点参与从而可能造成性能瓶颈的问题。这里分享一下如何在DevStack中做配置来激活DVR，</p><ul><li><p>controller节点(网络节点)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Base configuration</span></span><br><span class="line">HOST_IP=192.168.20.132</span><br><span class="line">SERVICE_HOST=192.168.20.132</span><br><span class="line">MYSQL_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">RABBIT_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">GLANCE_HOSTPORT=<span class="variable">$SERVICE_HOST</span>:9292</span><br><span class="line">ADMIN_PASSWORD=abc123</span><br><span class="line">DATABASE_PASSWORD=abc123</span><br><span class="line">RABBIT_PASSWORD=abc123</span><br><span class="line">SERVICE_PASSWORD=abc123</span><br><span class="line">DATABASE_TYPE=mysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># Neutron options</span></span><br><span class="line">Q_USE_SECGROUP=True</span><br><span class="line">FLOATING_RANGE=<span class="string">"192.168.23.1/21"</span></span><br><span class="line">Q_FLOATING_ALLOCATION_POOL=start=192.168.23.120,end=192.168.23.150</span><br><span class="line">IPV4_ADDRS_SAFE_TO_USE=<span class="string">"10.0.0.0/22"</span></span><br><span class="line">PUBLIC_NETWORK_GATEWAY=<span class="string">"192.168.18.1"</span></span><br><span class="line">PUBLIC_INTERFACE=eno1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Open vSwitch provider networking configuration</span></span><br><span class="line">Q_USE_PROVIDERNET_FOR_PUBLIC=True</span><br><span class="line">OVS_PHYSICAL_BRIDGE=br-ex</span><br><span class="line">PUBLIC_BRIDGE=br-ex</span><br><span class="line">OVS_BRIDGE_MAPPINGS=public:br-ex</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multi-node cluster</span></span><br><span class="line">MULTI_HOST=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># MISC</span></span><br><span class="line">LOGFILE=/opt/stack/logs/stack.sh.log</span><br><span class="line">NOVA_VNC_ENABLED=True</span><br><span class="line">NOVNCPROXY_URL=<span class="string">"http://<span class="variable">$SERVICE_HOST</span>:6080/vnc_auto.html"</span></span><br><span class="line">VNCSERVER_LISTEN=<span class="variable">$HOST_IP</span></span><br><span class="line">VNCSERVER_PROXYCLIENT_ADDRESS=<span class="variable">$VNCSERVER_LISTEN</span></span><br><span class="line">GIT_BASE=https://git.openstack.org</span><br><span class="line"></span><br><span class="line"><span class="comment"># Settings for DVR networking, DVR depends on vxlan and ml2/ovs,</span></span><br><span class="line"><span class="comment"># this is not verified in the latest version.</span></span><br><span class="line"><span class="comment"># The setting on the network node.</span></span><br><span class="line">Q_DVR_MODE=dvr_snat</span><br><span class="line">Q_PLUGIN=ml2</span><br><span class="line">Q_ML2_TENANT_NETWORK_TYPE=vxlan</span><br></pre></td></tr></table></figure></li><li><p>计算节点（这里需要注意<strong>ENABLED_SERVICES</strong>中需要将neutron相关的服务都加上，DHCP服务除外）</p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Basic configuration</span></span><br><span class="line"><span class="comment"># Reference: https://docs.openstack.org/devstack/latest/guides/neutron.html</span></span><br><span class="line">HOST_IP=192.168.18.79</span><br><span class="line">SERVICE_HOST=192.168.20.132</span><br><span class="line">MYSQL_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">RABBIT_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">GLANCE_HOSTPORT=<span class="variable">$SERVICE_HOST</span>:9292</span><br><span class="line">ADMIN_PASSWORD=abc123</span><br><span class="line">DATABASE_PASSWORD=abc123</span><br><span class="line">MYSQL_PASSWORD=abc123</span><br><span class="line">RABBIT_PASSWORD=abc123</span><br><span class="line">SERVICE_PASSWORD=abc123</span><br><span class="line">DATABASE_TYPE=mysql</span><br><span class="line">MULTI_HOST=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Neutron options</span></span><br><span class="line">FLOATING_RANGE=<span class="string">"192.168.23.1/21"</span></span><br><span class="line">Q_FLOATING_ALLOCATION_POOL=start=192.168.23.120,end=192.168.23.150</span><br><span class="line">IPV4_ADDRS_SAFE_TO_USE=<span class="string">"10.0.0.0/22"</span></span><br><span class="line">PUBLIC_NETWORK_GATEWAY=<span class="string">"192.168.18.1"</span></span><br><span class="line">PUBLIC_INTERFACE=eno1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Services that a compute node runs</span></span><br><span class="line">ENABLED_SERVICES=n-cpu,q-agt,n-api-meta,c-vol,placement-client,placement-api,neutron,q-l3,q-meta</span><br><span class="line"></span><br><span class="line"><span class="comment"># Misc configuration</span></span><br><span class="line">LOGFILE=/opt/stack/logs/stack.sh.log</span><br><span class="line">NOVA_VNC_ENABLED=True</span><br><span class="line">NOVNCPROXY_URL=<span class="string">"http://<span class="variable">$SERVICE_HOST</span>:6080/vnc_auto.html"</span></span><br><span class="line">VNCSERVER_LISTEN=<span class="variable">$HOST_IP</span></span><br><span class="line">VNCSERVER_PROXYCLIENT_ADDRESS=<span class="variable">$VNCSERVER_LISTEN</span></span><br><span class="line">GIT_BASE=https://git.openstack.org</span><br><span class="line"></span><br><span class="line"><span class="comment"># Settings for DVR networking</span></span><br><span class="line">Q_DVR_MODE=dvr</span><br><span class="line">Q_PLUGIN=ml2</span><br><span class="line">Q_ML2_TENANT_NETWORK_TYPE=vxlan</span><br></pre></td></tr></table></figure><p>这里或许有部分参数是可选的，但是加上也没有关系。</p><hr><p>[1] <a href="https://github.com/chendave/initrepo/tree/master/openstack/localrc/dvr" target="_blank" rel="noopener">https://github.com/chendave/initrepo/tree/master/openstack/localrc/dvr</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这些日子脑子里经常会想到《可可西里》电影里的一个画面，从刚陷入到流沙时的垂死挣扎越陷越深到后来彻底绝望放弃，电影表达了个人在恶劣的自然环境下的无能无力，换个角度想想，我们绝大多数人何尝不是已经陷入了深不见底的绝望之中，只不过这里不是沙漠戈壁而是同样现实的社会，我们挣扎着，不
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>Neutron接口命名规则</title>
    <link href="http://jungler.com/2018/07/28/neutron%E6%8E%A5%E5%8F%A3%E5%91%BD%E5%90%8D%E6%BD%9C%E8%A7%84%E5%88%99/"/>
    <id>http://jungler.com/2018/07/28/neutron接口命名潜规则/</id>
    <published>2018-07-28T11:38:12.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p>这些日子上海挺热的，前些日子去杭州才发现，杭州比上海还要热个几度，大热天爆晒36+，带着老婆孩子去西溪湿地去暴走，这个天当然不适合旅游，只是往年都要找个日子去杭州来个短游，今年正好得去杭州去拜访吉利集团，可以理解为为了所谓的旅游而旅游吧。</p><p><em>Neutron</em>已经看了有些日子了，计划不久的将来对有无DVR情况下南北与东西流量做个总结，当作一个铺垫吧，这里对Neutron里的网络接口命名做个小结，当看到<em>tap, qbr, qvb, qvo, qr-, qg-, br</em>前缀命令的接口设备有没有一点小晕呢？其实这些设备本质上都是一样的，但是应用的场景又各不相同，不同的名称前缀代表了不同的含义，所以熟悉了之后只看这些前缀也就略知一二了。</p><p><em>tap-</em><br>这个就是tap设备，每个虚拟机都对应一个tap设备，tap设备需要挂在linux bridge上或者OVS上，OpenStack里虚拟机的tap设备挂在linux bridge上，DHCP namespace里的tap设备挂在OVS上。<br>例如下面的tap设备”tap0cf5c0e2-26”来自于DHCP namespace并挂在OVS上。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ovs-vsctl show</span><br><span class="line">    Bridge br-int</span><br><span class="line">        Controller <span class="string">"tcp:127.0.0.1:6633"</span></span><br><span class="line">            is_connected: <span class="literal">true</span></span><br><span class="line">        fail_mode: secure</span><br><span class="line">..</span><br><span class="line">        Port <span class="string">"tap0cf5c0e2-26"</span></span><br><span class="line">            tag: 1</span><br><span class="line">            Interface <span class="string">"tap0cf5c0e2-26"</span></span><br><span class="line">                <span class="built_in">type</span>: internal</span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip netns <span class="built_in">exec</span> qdhcp-2f0982cf-3f10-4ae5-96de-1e70d289fbf0 ip a</span><br><span class="line">64: tap0cf5c0e2-26: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:fb:9b:53 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.0.0.2/26 brd 10.0.0.63 scope global tap0cf5c0e2-26</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fd7d:9d2b:8fb7:0:f816:3eff:fefb:9b53/64 scope global</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p><em>qvb-</em>，<em>qvo-</em>与<em>qbr-</em><br>qvb与qvo是一对veth pair，可以在系统上看到这一对veth pair，其中qvb设备挂在linux bridge上，qvo设备挂在OVS上。<br>我们可以通过在系统上输入ip a命令来查看这些veth pair的信息，例如我的系统上可以看到下面的设备：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">70: qvo285c68b1-9d@qvb285c68b1-9d: &lt;BROADCAST,MULTICAST,PROMISC,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master ovs-system state UP group default qlen 1000</span><br></pre></td></tr></table></figure></p><p>qbr用来定义命名一个linux bridge。</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/linux-bridge.png" alt=""></p><p><em>gr-</em>与<em>qg-</em><br>qr设备用于连接租户网络（租户内部IP地址），qg设备用于连接外部网络（通过floating IP连接外部网络）。<br>例如：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip netns <span class="built_in">exec</span> qrouter-3b1a4673-4ada-4988-a11b-86fcacfb0ea0 ip a</span><br><span class="line">65: qr-f937ae2f-ec: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:ac:b9:00 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.0.0.1/26 brd 10.0.0.63 scope global qr-f937ae2f-ec</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::f816:3eff:feac:b900/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">66: qg-4386c8fb-38: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:0d:5a:4d brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.42.16/24 brd 192.168.42.255 scope global qg-4386c8fb-38</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.42.11/32 brd 192.168.42.11 scope global qg-4386c8fb-38</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这些日子上海挺热的，前些日子去杭州才发现，杭州比上海还要热个几度，大热天爆晒36+，带着老婆孩子去西溪湿地去暴走，这个天当然不适合旅游，只是往年都要找个日子去杭州来个短游，今年正好得去杭州去拜访吉利集团，可以理解为为了所谓的旅游而旅游吧。&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Neutro
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>虚拟机网络不通的几个思路[2]</title>
    <link href="http://jungler.com/2018/07/01/%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%B8%8D%E9%80%9A%E7%9A%84%E5%87%A0%E4%B8%AA%E6%80%9D%E8%B7%AF%5B2%5D/"/>
    <id>http://jungler.com/2018/07/01/虚拟机网络不通的几个思路[2]/</id>
    <published>2018-07-01T09:14:52.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/stock.png" alt=""></p><p>2018年六月份已经过去，刚过去的两个月是入市以来回撤最大的两个月，虽然资金量不大，但依然对未来抱有希望，说不定哪天走狗屎运咸鱼翻身了呢。本质上来说，股市也好，人生中的各种选择也好，都是一种赌博，就看你的筹码有多大，未来到底看的有多清，当然我从不屑于直接参与赌博，只是厌倦了一辈子都是如此谨谨慎慎，按部就班的生活。</p><p>2018年下半年祝福自己依然有颗勇敢的心，怀抱希望的走下去，Get busy living, Or get busy dying!</p><p>回来继续做些笔记，之前提到过OpenStack 虚拟机网络不通的几个思路，这篇笔记对其中iptable的规则来做个深入的了解：</p><blockquote><p>sudo iptables -t nat -A POSTROUTING -s 192.168.42.0/24 -o eno1 -j MASQUERADE</p></blockquote><h3 id="为什么？？？"><a href="#为什么？？？" class="headerlink" title="为什么？？？"></a>为什么？？？</h3><p>首先我们假设floating IP的地址和物理机的IP地址不在同一个网段，而是一个自定义的网段（后面我会探讨不同的应用场景，比如floating IP和物理机器的IP地址在需要在同一个网段的情况如何做网络配置），比如：</p><ul><li>物理机的IP地址为: 192.168.18.0/24</li><li>floating IP地址定义为: 192.168.42.0/24<br>这里floating IP地址其实是一个不存在的网络，是需要通过OpenStack去创建的一个网络。和之前的定义不一样，这里我们只对<strong>192.168.42.0/24</strong>网段的IP进行地址伪装，伪装为eno1的IP地址出去，<strong>192.168.42.0/24</strong>是floating IP地址。如果没有上面定义的这个规则，虚拟机则无法访问外网(公司内网)，因为外部网络是不知道floating IP地址存在的，通过这个规则，从虚拟机出来的流量将得以访问外网。</li></ul><p>假设OpenStack的环境是通过DevStack安装起来的，那么你可以看到DevStack脚本已经帮你创建好了这个规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ sudo iptables -t nat -S</span><br><span class="line">-P PREROUTING ACCEPT</span><br><span class="line">-P INPUT ACCEPT</span><br><span class="line">-P OUTPUT ACCEPT</span><br><span class="line">-P POSTROUTING ACCEPT</span><br><span class="line">-N DOCKER</span><br><span class="line">-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER</span><br><span class="line">-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER</span><br><span class="line">-A POSTROUTING -s 192.168.42.0/24 -o eno1 -j MASQUERADE  &lt;- 看这里</span><br><span class="line">-A DOCKER -i docker0 -j RETURN</span><br></pre></td></tr></table></figure><h3 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h3><p>我们来想这样一个场景，物理机上有多个网卡，比方说<strong>eno1</strong>连接的是公司内部的网络，<strong>eno2</strong>连的是公网，通过在DevStack的localrc文件里做如下的配置：</p><blockquote><p>FLAT_INTERFACE=eno1</p></blockquote><p><strong>注：</strong></p><table><tr><td bgcolor="DarkGray">新版本中已经不建议采用nova network, 而是用neutron来提供网络服务，这样FLAT_INTERFACE将不再起作用，取而代之的是PUBLIC_INTERFACE这个属性，而这两个属性所代表的意思是不同的，参考后面的源码分析。</td></tr></table><p>DevStack会自动配置好了iptables，我们假设网络没有绑定任何物理网卡，那么应该走的是系统的默认路由，一般情况下，物理机上会配置一个默认路由，像下面这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip route</span><br><span class="line">default via 192.168.18.1 dev eno1  proto static  metric 100</span><br><span class="line">default via 192.168.8.1 dev eno2  proto static  metric 101</span><br><span class="line">...</span><br><span class="line">192.168.16.0/21 dev eno1  proto kernel  scope link  src 192.168.20.132</span><br><span class="line">192.168.42.0/24 dev br-ex  proto kernel  scope link  src 192.168.42.1</span><br></pre></td></tr></table></figure><p>这样虚拟机通过floating IP应该可以访问公司的内网了，但是有一天，虚拟机需要访问外部的公网了，怎么办？<br>修改物理机的默认路由？例如默认路由修改为：</p><blockquote><p>default via 192.168.8.1 dev eno2  proto static  metric 100<br>default via 192.168.18.1 dev eno1  proto static  metric 101</p></blockquote><p>是否可以了呢？做个测试，在虚拟机里ping 8.8.8.8，然后通过tcpdump可以来检测eno2看的网络请求:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ sudo tcpdump -n -i eno2 icmp</span><br><span class="line">tcpdump: verbose output suppressed, use -v or -vv <span class="keyword">for</span> full protocol decode</span><br><span class="line">listening on eno2, link-type EN10MB (Ethernet), capture size 262144 bytes</span><br><span class="line">18:26:57.424083 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 1, length 64</span><br><span class="line">18:26:58.432816 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 2, length 64</span><br><span class="line">18:26:59.440803 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 3, length 64</span><br><span class="line">18:27:00.448842 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 4, length 64</span><br><span class="line">18:27:01.456825 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 5, length 64</span><br><span class="line">18:27:02.464842 IP 192.168.42.20 &gt; 8.8.8.8: ICMP <span class="built_in">echo</span> request, id 5155, seq 6, length 64</span><br></pre></td></tr></table></figure><p>虽然eno2上能检测测ping请求，但是没有reply，虚拟机无法ping通外部网络！<br>所以，手动修改iptables的规则就该上场了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables -t nat -A POSTROUTING -s 192.168.42.0/24 -o eno2 -j MASQUERADE</span><br></pre></td></tr></table></figure><p>再ping一次试试：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ ping 8.8.8.8</span><br><span class="line">PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=1 ttl=38 time=99.6 ms</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=2 ttl=38 time=97.7 ms</span><br><span class="line">64 bytes from 8.8.8.8: icmp_seq=3 ttl=38 time=83.0 ms</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>看看系统上的iptables的规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ sudo iptables -t nat -S</span><br><span class="line">-P PREROUTING ACCEPT</span><br><span class="line">-P INPUT ACCEPT</span><br><span class="line">-P OUTPUT ACCEPT</span><br><span class="line">-P POSTROUTING ACCEPT</span><br><span class="line">-N DOCKER</span><br><span class="line">-A PREROUTING -m addrtype --dst-type LOCAL -j DOCKER</span><br><span class="line">-A OUTPUT ! -d 127.0.0.0/8 -m addrtype --dst-type LOCAL -j DOCKER</span><br><span class="line">-A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE</span><br><span class="line">-A POSTROUTING -s 192.168.42.0/24 -o eno1 -j MASQUERADE</span><br><span class="line">-A POSTROUTING -s 192.168.42.0/24 -o eno2 -j MASQUERADE   &lt;- 看这里</span><br><span class="line">-A DOCKER -i docker0 -j RETURN</span><br></pre></td></tr></table></figure><p>看来虽然我们对网段<strong>192.168.42.0/24</strong>加了两个MASQUERADE规则，但它们并不互斥，这周就到这里吧。</p><h3 id="源码分析"><a href="#源码分析" class="headerlink" title="源码分析"></a>源码分析</h3><p>这里的源码来自于<strong>pike release</strong>版本，上面所讲的都是我们能看到的现象，我们看看代码里是如何实现的。<br>打开DevStack里这个文件，我们可以看的一清二楚：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /lib/neutron_plugins/services/l3</span></span><br><span class="line">98 default_v4_route_devs=$(ip -4 route | grep ^default | awk <span class="string">'&#123;print $5&#125;'</span>)</span><br><span class="line">...</span><br><span class="line">126     <span class="keyword">if</span> [[ -n <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> ]]; <span class="keyword">then</span></span><br><span class="line">127         _move_neutron_addresses_route <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> <span class="string">"<span class="variable">$OVS_PHYSICAL_BRIDGE</span>"</span> True False <span class="string">"inet"</span></span><br><span class="line">128</span><br><span class="line">129         <span class="keyword">if</span> [[ $(ip -f inet6 a s dev <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> | grep -c <span class="string">'global'</span>) != 0 ]]; <span class="keyword">then</span></span><br><span class="line">130             _move_neutron_addresses_route <span class="string">"<span class="variable">$PUBLIC_INTERFACE</span>"</span> <span class="string">"<span class="variable">$OVS_PHYSICAL_BRIDGE</span>"</span> False False <span class="string">"inet6"</span></span><br><span class="line">131         <span class="keyword">fi</span></span><br><span class="line">132     <span class="keyword">else</span></span><br><span class="line">133         <span class="keyword">for</span> d <span class="keyword">in</span> <span class="variable">$default_v4_route_devs</span>; <span class="keyword">do</span></span><br><span class="line">134             sudo iptables -t nat -A POSTROUTING -o <span class="variable">$d</span> -s <span class="variable">$FLOATING_RANGE</span> -j MASQUERADE</span><br><span class="line">135         <span class="keyword">done</span></span><br><span class="line">136     <span class="keyword">fi</span></span><br><span class="line">137 &#125;</span><br></pre></td></tr></table></figure></p><p>所以，当有设置这个<strong>PUBLIC_INTERFACE</strong>这个属性时，将会对应的物理网卡绑定到bridge上去，如果没有设置，则会对系统的默认路由对应的所有的所有物理网卡设备定义MASQUERADE规则，以实现虚拟机与外网访问的目的。</p><p>从这里我们可以看出，配置OpenStack需要至少配置两个不同的网络的原因所在:</p><ul><li>集群内部需要一个可以连接的外部网络，每个节点都有配置一个该网段的IP地址，网络节点需要将此网卡绑定到bridge上，这样虚拟机可以通过此网络访问外网，floating IP也可以配置在同一个网络里，这样其它物理节点可以直接访问虚拟机（比如虚拟机提供一些web服务那么可以直接在其它节点访问）。</li><li>还需要一个可以让终端用户去访问的管理机器的网络，此网络可以是内部管理网即OpenStack API服务所在的网络。<br>实际生产环境比这个要复杂的多，比如需要一个存储网络，租户网络，部署(Provision)网络等。</li></ul><p>附:</p><ol><li>查看某个table上的rules<br>sudo iptables -nL -t nat –line-number</li></ol><ol start="2"><li>删除某个规则<br>sudo iptables -D POSTROUTING 3 -t nat</li></ol><hr><p>[1] <a href="https://blog.csdn.net/chenwei8280/article/details/79601885" target="_blank" rel="noopener">https://blog.csdn.net/chenwei8280/article/details/79601885</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://github.com/chendave/chendave.github.io/raw/master/css/images/stock.png&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;2018年六月份已经过去，刚过去的两个月是入市以来回撤最大的两个
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>PG 异常状态- active+undersized+degraded</title>
    <link href="http://jungler.com/2018/06/23/PG%E5%BC%82%E5%B8%B8%E7%8A%B6%E6%80%81-active+undersized+degraded/"/>
    <id>http://jungler.com/2018/06/23/PG异常状态-active+undersized+degraded/</id>
    <published>2018-06-23T08:53:31.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p>自己搭的3个OSD节点的集群的健康状态经常处在”WARN”状态，replicas设置为3，OSD节点数量大于3，存放的data数量也不多，<strong>ceph -s</strong> 不是期待的health ok，而是<strong>active+undersized+degraded</strong>。被这个问题困扰有段时间，因为对Ceph不太了解而一直没有找到解决方案，直到最近发邮件到社区才得到解决[1]。</p><h3 id="PG状态的含义"><a href="#PG状态的含义" class="headerlink" title="PG状态的含义"></a>PG状态的含义</h3><p>PG的非正常状态说明可以参考[2]，<strong>undersized</strong>与<strong>degraded</strong>的含义记录于此：</p><blockquote><p>undersized<br>The placement group has fewer copies than the configured pool replication level.<br>degraded<br>Ceph has not replicated some objects in the placement group the correct number of times yet.<br>这两种状态一般同时出现，大概的意思就是有些PG没有满足设定的replicas数量要求，PG中的部分objects亦如此。看下PG的详细信息：</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">ceph health detail</span><br><span class="line">HEALTH_WARN 2 pgs degraded; 2 pgs stuck degraded; 2 pgs stuck unclean; 2 pgs </span><br><span class="line">stuck undersized; 2 pgs undersized</span><br><span class="line">pg 17.58 is stuck unclean <span class="keyword">for</span> 61033.947719, current state </span><br><span class="line">active+undersized+degraded, last acting [2,0]</span><br><span class="line">pg 17.16 is stuck unclean <span class="keyword">for</span> 61033.948201, current state </span><br><span class="line">active+undersized+degraded, last acting [0,2]</span><br><span class="line">pg 17.58 is stuck undersized <span class="keyword">for</span> 61033.343824, current state </span><br><span class="line">active+undersized+degraded, last acting [2,0]</span><br><span class="line">pg 17.16 is stuck undersized <span class="keyword">for</span> 61033.327566, current state </span><br><span class="line">active+undersized+degraded, last acting [0,2]</span><br><span class="line">pg 17.58 is stuck degraded <span class="keyword">for</span> 61033.343835, current state </span><br><span class="line">active+undersized+degraded, last acting [2,0]</span><br><span class="line">pg 17.16 is stuck degraded <span class="keyword">for</span> 61033.327576, current state </span><br><span class="line">active+undersized+degraded, last acting [0,2]</span><br><span class="line">pg 17.16 is active+undersized+degraded, acting [0,2]</span><br><span class="line">pg 17.58 is active+undersized+degraded, acting [2,0]</span><br></pre></td></tr></table></figure><h3 id="解决办法"><a href="#解决办法" class="headerlink" title="解决办法"></a>解决办法</h3><p>虽然设定的拷贝数量是3，但是PG 17.58与17.58却只有两个拷贝，分别存放在OSD 0与OSD 2上。<br>而究其原因则是我们的OSD所在的磁盘不是同质的，从而每个OSD的weight不同，而Ceph对异质OSD的支持不是很好。从而导致部分PG无法满足我们设定的备份数量限制。</p><p>OSD状态树：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ceph osd tree</span><br><span class="line">ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY</span><br><span class="line">-1 5.89049 root default</span><br><span class="line">-2 1.81360     host ceph3</span><br><span class="line">2 1.81360         osd.2       up  1.00000          1.00000</span><br><span class="line">-3 0.44969     host ceph4</span><br><span class="line">3 0.44969         osd.3       up  1.00000          1.00000</span><br><span class="line">-4 3.62720     host ceph1</span><br><span class="line">0 1.81360         osd.0       up  1.00000          1.00000</span><br><span class="line">1 1.81360         osd.1       up  1.00000          1.00000</span><br></pre></td></tr></table></figure><p>解决办法是另外构建一个OSD，使其容量大小和其它节点相同，是否可以有偏差？猜测应该有一个可以接受的偏差范围，重构后的OSD节点树看起来像这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd tree</span><br><span class="line">ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY</span><br><span class="line">-1 7.25439 root default</span><br><span class="line">-2 1.81360     host ceph3</span><br><span class="line"> 2 1.81360         osd.2       up  1.00000          1.00000</span><br><span class="line">-3       0     host ceph4</span><br><span class="line">-4 3.62720     host ceph1</span><br><span class="line"> 0 1.81360         osd.0       up  1.00000          1.00000</span><br><span class="line"> 1 1.81360         osd.1       up  1.00000          1.00000</span><br><span class="line">-5 1.81360     host ceph2</span><br><span class="line"> 3 1.81360         osd.3       up  1.00000          1.00000</span><br></pre></td></tr></table></figure><p>ceph4节点被删除，重新加入了另一个OSD节点ceph2。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ceph -s</span><br><span class="line">    cluster 20ab1119-a072-4bdf-9402-9d0ce8c256f4</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e2: 2 mons at &#123;ceph2=192.168.17.21:6789/0,ceph4=192.168.17.23:6789/0&#125;</span><br><span class="line">            election epoch 26, quorum 0,1 ceph2,ceph4</span><br><span class="line">     osdmap e599: 4 osds: 4 up, 4 <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v155011: 100 pgs, 1 pools, 18628 bytes data, 1 objects</span><br><span class="line">            1129 MB used, 7427 GB / 7428 GB avail</span><br><span class="line">                 100 active+clean</span><br></pre></td></tr></table></figure><p>另外，为了满足HA的要求，OSD需要分散在不同的节点上，这里拷贝数量为3，则需要有三个OSD节点来承载这些OSD，如果三个OSD分布在两个OSD节点上，则依然可能会出现”active+undersized+degraded”的状态。</p><p>官方是这样说的：</p><blockquote><p>This, combined with the default CRUSH failure domain, ensures that replicas or erasure code shards are separated across hosts and a single host failure will not affect availability.</p></blockquote><p>理解如有错误还望能点醒。</p><hr><p>[1] <a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg47070.html" target="_blank" rel="noopener">https://www.mail-archive.com/ceph-users@lists.ceph.com/msg47070.html</a><br>[2] <a href="http://docs.ceph.com/docs/master/rados/operations/pg-states/" target="_blank" rel="noopener">http://docs.ceph.com/docs/master/rados/operations/pg-states/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;自己搭的3个OSD节点的集群的健康状态经常处在”WARN”状态，replicas设置为3，OSD节点数量大于3，存放的data数量也不多，&lt;strong&gt;ceph -s&lt;/strong&gt; 不是期待的health ok，而是&lt;strong&gt;active+undersized+
      
    
    </summary>
    
    
      <category term="Ceph" scheme="http://jungler.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Dashboard</title>
    <link href="http://jungler.com/2018/06/18/%E5%88%9B%E5%BB%BAKubernetes-UI/"/>
    <id>http://jungler.com/2018/06/18/创建Kubernetes-UI/</id>
    <published>2018-06-18T04:32:53.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p>创建Kubernets Dashboard(UI)比较简单也就是几个命令的事儿，记录在这里作为一个备忘：</p><ul><li>通过官网提供的yaml配置文件创建service, role, deployment等等:</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</span><br><span class="line"></span><br><span class="line">secret <span class="string">"kubernetes-dashboard-certs"</span> created</span><br><span class="line">serviceaccount <span class="string">"kubernetes-dashboard"</span> created</span><br><span class="line">role <span class="string">"kubernetes-dashboard-minimal"</span> created</span><br><span class="line">rolebinding <span class="string">"kubernetes-dashboard-minimal"</span> created</span><br><span class="line">deployment <span class="string">"kubernetes-dashboard"</span> created</span><br><span class="line">service <span class="string">"kubernetes-dashboard"</span> created</span><br></pre></td></tr></table></figure><ul><li>启动kubectl proxy服务，API server将监听在8001端口，apiserver将负责访问控制：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl proxy --address=<span class="string">'0.0.0.0'</span> --port=8001 --accept-hosts=<span class="string">'.*'</span> &amp;</span><br></pre></td></tr></table></figure><p><strong>–address=’0.0.0.0’</strong> 使得其它机器也可以访问8001端口，<strong>–accept-hosts</strong> 让apiserver接受其它所有机器的请求。</p><ul><li>赋予Dashboard的serviceaccount以admin权限，亦或理解为直接绕过访问控制？因为我的环境是直接通过kubeadm安装的，所以可以创建一个文件名为dashboard-admin.yaml，文件内容如下：</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: rbac.authorization.k8s.io/v1beta1</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">metadata:</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  labels:</span><br><span class="line">    k8s-app: kubernetes-dashboard</span><br><span class="line">roleRef:</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: cluster-admin</span><br><span class="line">subjects:</span><br><span class="line">- kind: ServiceAccount</span><br><span class="line">  name: kubernetes-dashboard</span><br><span class="line">  namespace: kube-system</span><br></pre></td></tr></table></figure><p>执行下面的命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kubectl create -f dashboard-admin.yaml</span><br><span class="line">clusterrolebinding <span class="string">"kubernetes-dashboard"</span> created</span><br></pre></td></tr></table></figure><p>接下来就可以直接8001直接访问UI了，对于出现的用户认证页面，直接skip就好了，看到的界面看起来像这样：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/k8s-ui.png" alt=""></p><hr><p>[1] <a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/</a><br>[2] <a href="https://github.com/kubernetes/dashboard/wiki/Access-control" target="_blank" rel="noopener">https://github.com/kubernetes/dashboard/wiki/Access-control</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;创建Kubernets Dashboard(UI)比较简单也就是几个命令的事儿，记录在这里作为一个备忘：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过官网提供的yaml配置文件创建service, role, deployment等等:&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="http://jungler.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>Python与协程</title>
    <link href="http://jungler.com/2018/05/26/Python%E4%B8%8E%E5%8D%8F%E7%A8%8B/"/>
    <id>http://jungler.com/2018/05/26/Python与协程/</id>
    <published>2018-05-26T09:36:10.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p>这篇博文对Python的协程(Coroutines)做个小结，记得多年之前在学校之时，学到的是进程，线程。即便工作之后用Python开始编写代码了，也很少会用到协程。那么协程是什么？提到协程我们得先去了解一下Python生成器(generator)与迭代器(iterator)。</p><p>简单来说迭代器(iterator)是一个你可以去顺序遍历的对象，是一个可以迭代(iterable)的Python类实例化的对象，这样的类一般来说是一个Python的容器类型，常见的容器类型如list，tuple，string都是可以迭代的(iterable)，也就是说通过它们可以实例化为迭代器，有点拗口，下面这张图可以帮助理解：</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/relationships.png" alt="" title="我的博客"></p><p>迭代器必须实现<code>__iter__</code>与<code>__next__</code>两种内置方法，迭代器的优势在于节省内存的开销，例如下面的例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">xrange</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, max)</span>:</span></span><br><span class="line">        self.i = <span class="number">0</span></span><br><span class="line">        self.max = max</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.i &lt; self.max:</span><br><span class="line">            i = self.i</span><br><span class="line">            self.i += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span> i</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration()</span><br></pre></td></tr></table></figure><p>每次调用<code>next()</code>会生成一个元素，这有点类似<code>C</code>语言里的指针，这对于无需一次访问整个列表而只访问某个元素来说是非常有益的，因为<em>内存</em>的占用始终是一个常数。如果返回的是一个列表类型的话，将会将所有的结果都保存在内存中，如果max特别大的话，消耗的内存将是一个必须考虑的问题。</p><p>生成器是一个特殊的迭代器，通过引入<code>yield</code>关键字简化了迭代器的代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xrange</span><span class="params">(max)</span>:</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; max:</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line">        i += <span class="number">1</span></span><br></pre></td></tr></table></figure><p>与上面的代码作用相同，每调用<code>next()</code>方法生成一个值直到值等于<code>max</code>为止。在某些场合下或者某些语言中，一个生成器可以理解为一个协程，但是又有细微的不同:</p><ul><li>生成器一般用来产生数据</li><li>协程一般用来消费数据</li></ul><p>但这是通常的使用场合的不同，实现上看不出来什么区别，也就是说如果我们需要对<code>yield</code>出来的数据做进一步的处理，则可以将其理解为协程。</p><p>协程有时和多线程可以用来实现相同的目的，那协程相对于线程来说有什么不同和优势呢？最明显的区别在于协程将控制权交由程序自己来控制，本质上就是一个线程，因为没有多线程场景下的CPU中断，也没有线程切换，自然系统的开销就要小的多，那么多核的平台上，又要并发，又要灵活的控制程序的执行，则可以将协程和多线程结合起来。下面来看一个协程的例子:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">grep</span><span class="params">(pattern)</span>:</span></span><br><span class="line">    print(<span class="string">"Searching for"</span>, pattern)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        line = (<span class="keyword">yield</span>)  <span class="comment"># 2</span></span><br><span class="line">        <span class="keyword">if</span> pattern <span class="keyword">in</span> line:</span><br><span class="line">            print(line)</span><br><span class="line"></span><br><span class="line">            </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>search = grep(<span class="string">"hello"</span>)  <span class="comment"># 0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>next(search)         <span class="comment"># 1</span></span><br><span class="line">(<span class="string">'Searching for'</span>, <span class="string">'hello'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>search.send(<span class="string">"i love you"</span>) <span class="comment"># 3</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>search.send(<span class="string">"helloworld"</span>)  <span class="comment"># 4</span></span><br><span class="line">helloworld</span><br></pre></td></tr></table></figure><p>0: 初始化此协程<br>1: 用<code>next()</code>方法来启动此协程，line此时被赋值为”hello”，且在“2”处暂停。<br>3：调用<code>send()</code>方法来匹配字符串”hello”，因为没有匹配上所以没有输出。<br>4：继续匹配“hello”，匹配成功，输出匹配结果。程序继续在2处暂停。</p><p><code>Python</code>语言中，对协程支持的并不是很好，目前看来，<code>greenlet</code>对<code>yield</code>进行封装实现对协程的支持，<code>Eventlet</code>再对<code>greenlet</code>进行了二次封装，后面有空再对<code>greenlet</code>和<code>Eventlet</code>做个小结吧。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;这篇博文对Python的协程(Coroutines)做个小结，记得多年之前在学校之时，学到的是进程，线程。即便工作之后用Python开始编写代码了，也很少会用到协程。那么协程是什么？提到协程我们得先去了解一下Python生成器(generator)与迭代器(iterator
      
    
    </summary>
    
    
      <category term="Python" scheme="http://jungler.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes Patch 学习小结</title>
    <link href="http://jungler.com/2018/05/20/kube-patch/"/>
    <id>http://jungler.com/2018/05/20/kube-patch/</id>
    <published>2018-05-20T02:00:12.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p>总结一下<code>Kubernetes Patch</code>的使用方法，详细的说明还要要去看官网[1]，K8S用这个命令来对运行中的应用进行动态跟新。总的来说<code>Patch</code>一个API对象有三种方式：</p><ul><li>使用JSON Patch来更新一个对象，没有看到具体的例子，看起来这是JSON的一个标准[2]，类似数据库的增删改查的方式对原来的JSON格式进行修改，不太清楚K8S对其支持如何。</li><li>使用JSON merge patch，这种方式需要定义一个完整的替换列表，也就是说，新的列表定义会替换原有的定义。</li><li>使用JSON strategic merge patch，这种补丁是以增量的形式来对已有的定义进行修改，可以理解为类似于<code>linux diff</code>创建的补丁。</li></ul><p>下面对第二种和第三种形式的更新，分别来举个栗子：</p><h4 id="JSON-merge-patch"><a href="#JSON-merge-patch" class="headerlink" title="JSON merge patch"></a>JSON merge patch</h4><p>下面的例子用来部署一个<code>nginx</code>应用，2份拷贝，后面在此基础上打补丁</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span> <span class="comment"># for versions before 1.9.0 use apps/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">patch-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">patch-demo-ctr</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">      tolerations:</span></span><br><span class="line"><span class="attr">      - effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">        key:</span> <span class="string">dedicated</span></span><br><span class="line"><span class="attr">        value:</span> <span class="string">test-team</span></span><br></pre></td></tr></table></figure><p>先部署这个应用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create -f patch_demo.yaml</span><br><span class="line">$ kubectl get pod -o wide</span><br><span class="line"></span><br><span class="line">NAME                         READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">patch-demo-576f89c99-mf4fj   1/1       Running   0          1m        10.244.2.118   k8s-node2</span><br><span class="line">patch-demo-576f89c99-tb97t   1/1       Running   0          1m        10.244.1.185   k8s-node1</span><br></pre></td></tr></table></figure><p>可以两个pod都已经跑了起来，接下来想对image进行修改，定义下面的patch:<br><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">patch-demo-ctr-3</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">gcr.io/google-samples/node-hello:1.0</span></span><br></pre></td></tr></table></figure></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl patch deployment patch-demo --<span class="built_in">type</span> merge --patch <span class="string">"<span class="variable">$(cat patch_image.yaml)</span>"</span></span><br><span class="line">$ kubectl get pod -o wide</span><br><span class="line">patch-demo-86c8577c88-bgd9s   1/1       Running   0          9m        10.244.2.119   k8s-node2</span><br><span class="line">patch-demo-86c8577c88-qqfrv   1/1       Running   0          10m       10.244.1.187   k8s-node1</span><br></pre></td></tr></table></figure><p>对比pod ID可见已有的pod已经被terminate了，并重新创建了两个新的 pod，可以进一步查看更新后的image：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get deployment patch-demo --output yaml | grep image</span><br><span class="line">      - image: gcr.io/google-samples/node-hello:1.0</span><br><span class="line">        imagePullPolicy: IfNotPresent</span><br></pre></td></tr></table></figure><h4 id="JSON-strategic-merge-patch"><a href="#JSON-strategic-merge-patch" class="headerlink" title="JSON strategic merge patch"></a>JSON strategic merge patch</h4><p>所谓策略性补丁，就是作为一个对已有配置的增量补丁，想想<code>diff</code>就好了，patch中没有定义的修改内容，则不会对原有配置产生影响，还是看下这个例子，基于原始版本新增一个新的<code>redis</code>的容器，定义好下面的补丁：</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">patch-demo-ctr-2</span></span><br><span class="line"><span class="attr">        image:</span> <span class="string">redis</span></span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl patch deployment patch-demo --patch <span class="string">"<span class="variable">$(cat patch_container.yaml)</span>"</span></span><br><span class="line">deployment <span class="string">"patch-demo"</span> patched</span><br><span class="line"></span><br><span class="line">$ kubectl get pod -o wide</span><br><span class="line">NAME                          READY     STATUS    RESTARTS   AGE       IP             NODE</span><br><span class="line">patch-demo-74b9844b77-hk2l7   2/2       Running   0          3m        10.244.2.120   k8s-node2</span><br><span class="line">patch-demo-74b9844b77-qjz6r   2/2       Running   0          5m        10.244.1.188   k8s-node1</span><br></pre></td></tr></table></figure><p><code>2/2</code>表示每个pod有两个容器，如果想再看细点，用下面命令来查看pod上跑的image:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod patch-demo-74b9844b77-hk2l7 --output yaml | grep image</span><br><span class="line">  - * image: redis *</span><br><span class="line">    imagePullPolicy: Always</span><br><span class="line">  - * image: gcr.io/google-samples/node-hello:1.0 *</span><br><span class="line">    imagePullPolicy: IfNotPresent</span><br><span class="line">    image: redis:latest</span><br><span class="line">    imageID: docker-pullable://redis@sha256:4aed8ea5a5fc4cf05c8d5341b4ae4a4f7c0f9301082a74f6f9a5f321140e0cd3</span><br><span class="line">    image: gcr.io/google-samples/node-hello:1.0</span><br><span class="line">    imageID: docker-pullable://gcr.io/google-samples/node-hello@sha256:d238d0ab54efb76ec0f7b1da666cefa9b40be59ef34346a761b8adc2dd45459b</span><br></pre></td></tr></table></figure><p>好了，先总结到这里，详细的介绍还是看官网吧，下周再来看看。</p><hr><p>[1] <a href="https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/" target="_blank" rel="noopener">https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/</a><br>[2] <a href="http://erosb.github.io/post/json-patch-vs-merge-patch/" target="_blank" rel="noopener">http://erosb.github.io/post/json-patch-vs-merge-patch/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;总结一下&lt;code&gt;Kubernetes Patch&lt;/code&gt;的使用方法，详细的说明还要要去看官网[1]，K8S用这个命令来对运行中的应用进行动态跟新。总的来说&lt;code&gt;Patch&lt;/code&gt;一个API对象有三种方式：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;使用JSON Pat
      
    
    </summary>
    
    
      <category term="Kubernetes" scheme="http://jungler.com/tags/Kubernetes/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack 与 SDN --- namespace</title>
    <link href="http://jungler.com/2018/04/30/OpenStack-SDN-namespace/"/>
    <id>http://jungler.com/2018/04/30/OpenStack-SDN-namespace/</id>
    <published>2018-04-30T08:20:33.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<h3 id="什么是network-namespace"><a href="#什么是network-namespace" class="headerlink" title="什么是network namespace"></a>什么是network namespace</h3><p>先来看看linux手册上对namespace的解释</p><blockquote><p>A network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices.<br>By default a process inherits its network namespace from its parent. Initially all the processes share the same default network namespace from the init process.</p></blockquote><p>翻译过来再加以理解，大致的意思就是network namespace实现了对网络资源的隔离，一个namespace有自己的路由，防火墙规则，以及网络设备。进程会从他的父进程那里继承network namespace。所有进程从共同的父进程init进程那里共享默认network namespace.</p><p>默认的network namespace有时也被称作root namespace，一个基本的原则是一个网络设备最终只能属于一个namespace，无论是物理设备或者虚拟设备。从实现上network namespace介于<strong>chroot</strong>与虚拟机VM之间，VM太重，而chroot不能有效实现网络设备隔离，如果想要实现的网络设备的隔离，那么优先考虑namespace。</p><p>从一张图来看或许更加清楚一些：<br><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/namespace.png" alt=""></p><h3 id="namespace与OpenStack"><a href="#namespace与OpenStack" class="headerlink" title="namespace与OpenStack"></a>namespace与OpenStack</h3><p>Neutron项目直接依赖于namespace，具体哪个版本引入不是太清楚了，如果你的环境是用<em>devstack</em>搭建起来的话，那么默认你是可以看到两个namespace的，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ip netns</span><br><span class="line">qrouter-a781c60f-3929-4a2b-b233-d724f3693d4e</span><br><span class="line">qdhcp-8c21785b-fdcf-49c6-8cf2-9ea56b9e8d35</span><br></pre></td></tr></table></figure><p>可以看到一个提供路由功能，另一个提供DHCP服务，创建多个子网并设置DHCP服务可以在系统上创建对个DHCP namespace。<br>进入到namesapce内部来一探究竟。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ sudo ip netns <span class="built_in">exec</span> qrouter-a781c60f-3929-4a2b-b233-d724f3693d4e ip a</span><br><span class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</span><br><span class="line">    inet 127.0.0.1/8 scope host lo</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 ::1/128 scope host</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">42: qr-2e14aa33-ac: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:88:7b:28 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 10.0.0.1/26 brd 10.0.0.63 scope global qr-2e14aa33-ac</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::f816:3eff:fe88:7b28/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">43: qg-9ad90b84-19: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:33:37:8e brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet 192.168.42.139/25 brd 192.168.42.255 scope global qg-9ad90b84-19</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet 192.168.42.131/32 brd 192.168.42.131 scope global qg-9ad90b84-19</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 2001:db8::b/64 scope global</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::f816:3eff:fe33:378e/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">44: qr-9166b961-68: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default qlen 1000</span><br><span class="line">    link/ether fa:16:3e:11:5f:29 brd ff:ff:ff:ff:ff:ff</span><br><span class="line">    inet6 fdbf:ae1d:a3f6::1/64 scope global</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br><span class="line">    inet6 fe80::f816:3eff:fe11:5f29/64 scope link</span><br><span class="line">       valid_lft forever preferred_lft forever</span><br></pre></td></tr></table></figure><p>对于router namespace来说<em>qr-2e14aa33-ac</em>其实就是一个内网的网关，<em>10.0.0.1/26</em>网段为内网网段，<em>qg-9ad90b84-19</em>则对应的是外网，其中<em>192.168.42.139</em>为外网网关，而其他IP地址则对应一个个用于访问外网的floating IP地址，他们共享一个虚拟的以太网地址或MAC地址。</p><p>dhcp namespace类似，其中除了一个回环设备外，剩下的就是一个IP地址就是DHCP服务器的IP地址。那么虚拟机是如何与他们产生联系的呢？答案是linux bridge或者OVS, 以OVS为例，这些虚拟的设备都会附加到<em>br-int</em> bridge上，对每一个虚拟机而言，又通过veth pair的方式附加到同一个bridge上以实现互联互通。不展开讨论。</p><p>OpenStack引入namespace的目的是为了解决多租户情况下的三层网络IP地址隔离，具体分析不用namespace的情况下可能会出现的问题可以参考这篇博文[1]</p><h3 id="namespace-访问外网"><a href="#namespace-访问外网" class="headerlink" title="namespace 访问外网"></a>namespace 访问外网</h3><p>谈到namespace对外网的访问，这里涉及到两个方面：</p><ol><li>namespace内部网络设备比方说之前提到的 <em>qg-9ad90b84-19</em>设备对外网的访问。</li><li>租户网络与外部网络之间的互访问。</li></ol><p>第一个问题的答案之前已经提及，主要是通过将网络设备绑定到linux bridge或者OVS bridge来实现。<br>第二个问题的答案是floating IP与iptable，floating IP的本质只是通过iptable实现的一些虚拟IP而非物理的绑定到网卡上的IP地址，那么通过查看router上的iptable定义的规则，我们可以很清楚的了解到floating IP的实现原理。</p><p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/namespace-iptables.png" alt=""></p><p>上图中DNAT用于从外部网络访问虚拟机内部网络所定义的地址转换规则；SNAT用于从虚拟机内部网络访问外网是所需要的地址转换规则。</p><p>留下一个问题，namespace与LXC以及container这三者之间的关系是什么？留待下次总结。</p><hr><p>[1] <a href="https://blog.csdn.net/cloudman6/article/details/52876889" target="_blank" rel="noopener">https://blog.csdn.net/cloudman6/article/details/52876889</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;什么是network-namespace&quot;&gt;&lt;a href=&quot;#什么是network-namespace&quot; class=&quot;headerlink&quot; title=&quot;什么是network namespace&quot;&gt;&lt;/a&gt;什么是network namespace&lt;/h3&gt;&lt;
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
  <entry>
    <title>我的数据在哪儿? - Ceph rbd image</title>
    <link href="http://jungler.com/2018/04/21/where-is-my-data/"/>
    <id>http://jungler.com/2018/04/21/where-is-my-data/</id>
    <published>2018-04-21T05:06:14.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://github.com/chendave/chendave.github.io/raw/master/css/images/sunshine.jpg" alt=""></p><p>Ceph的rbd image可以用来作为OpenStack的块存储，如果OpenStack配置Cinder存储后端为Ceph，实际上读写的就是Ceph的块存储设备，这里记录如何查看rbd image里的数据，以及数据存放在哪里。</p><p>首先来创建一个rbd image</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create rbdbench 100 100  <span class="comment">#创建一个名为rbdbench的pool，pg与pgp size均为100</span></span><br><span class="line"></span><br><span class="line">rbd create image01 --size 1024 --pool rbdbench --image-format 2  <span class="comment"># format 1已经deprecated了，format 2 包含了更多的特性。</span></span><br></pre></td></tr></table></figure><p>这里不需要做rbd的mapping操作，也无需mount rbd image，我们只想来看看rbd image里文件存放位置，如果需要做mapping，则需要修改ceph的主配置文件来忽略系统不支持的一些ceph的特性。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">'$a\rbd_default_features = 3'</span> /etc/ceph/ceph.conf</span><br></pre></td></tr></table></figure><p>接下来，我们通过rbd info查看image的一些详细信息：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ rbd -p rbdbench info image01</span><br><span class="line">rbd image <span class="string">'image01'</span>:</span><br><span class="line">        size 1024 MB <span class="keyword">in</span> 256 objects</span><br><span class="line">        order 22 (4096 kB objects)</span><br><span class="line">        block_name_prefix: rbd_data.4dde74b0dc51</span><br><span class="line">        format: 2</span><br><span class="line">        features: layering</span><br><span class="line">        flags:</span><br></pre></td></tr></table></figure></p><p>块设备里已经有256个object了，这些个object是什么，我们以后再看，通过block_name_prefix来查看pool里的objects.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ rados -p rbdbench ls | grep ^rbd_data.4dde74b0dc51</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000060</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000086</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000084</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000081</span><br><span class="line">rbd_data.4dde74b0dc51.00000000000000e0</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000083</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000000</span><br><span class="line">rbd_data.4dde74b0dc51.00000000000000a0</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000080</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000004</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000082</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000085</span><br><span class="line">rbd_data.4dde74b0dc51.00000000000000ff</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000087</span><br><span class="line">rbd_data.4dde74b0dc51.0000000000000020</span><br></pre></td></tr></table></figure></p><p>接下来就可以通过下面的命令来查找object所在的pg以及相应的OSD了。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd map rbdbench rbd_data.4dde74b0dc51.0000000000000086</span><br><span class="line">osdmap e505 pool <span class="string">'rbdbench'</span> (16) object <span class="string">'rbd_data.4dde74b0dc51.0000000000000086'</span> -&gt; pg 16.eabd8f8a (16.a) -&gt; up ([2,1], p2) acting ([2,1], p2)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd tree</span><br><span class="line">ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY</span><br><span class="line">-1 5.44080 root default</span><br><span class="line">-2 1.81360     host ceph3</span><br><span class="line"> 2 1.81360         osd.2       up  1.00000          1.00000</span><br><span class="line">-3       0     host ceph4</span><br><span class="line">-4 3.62720     host ceph1</span><br><span class="line"> 0 1.81360         osd.0       up  1.00000          1.00000</span><br><span class="line"> 1 1.81360         osd.1       up  1.00000          1.00000</span><br></pre></td></tr></table></figure><p>数据所在的主OSD为2， 从OSD为1， pg号为“16.a”，这样登录到OSD所在的机器，就可以查看到object的data文件了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@ceph3:/var/lib/ceph/osd/ceph-2/current/16.a_head<span class="comment"># file rbd\\udata.4dde74b0dc51.0000000000000086__head_EABD8F8A__10</span></span><br><span class="line">rbd\udata.4dde74b0dc51.0000000000000086__head_EABD8F8A__10: data</span><br></pre></td></tr></table></figure><p>有几个问题，rbd初始创建的object到底是什么？除了“block_name_prefix”指定的object之外，还有哪些objects? 可否通过这种方式创建一个image，然后写入一个文件，再去查看文件的存储位置，以及完整性校验等。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://github.com/chendave/chendave.github.io/raw/master/css/images/sunshine.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;Ceph的rbd image可以用来作为OpenStack
      
    
    </summary>
    
    
      <category term="Ceph" scheme="http://jungler.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack Queen 版本变更概述</title>
    <link href="http://jungler.com/2018/04/14/openstack-queen/"/>
    <id>http://jungler.com/2018/04/14/openstack-queen/</id>
    <published>2018-04-14T07:44:53.000Z</published>
    <updated>2022-06-19T11:49:19.353Z</updated>
    
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/chendave/initrepo/master/pic/banner.jpg" alt=""></p><p>毫无疑问，OpenStack正在经历它的低谷期，和芸芸众生一样，无法改变世界那就得改变自己来适应这个世界，真心的期待，曾经的王者有朝一日能再现昔日辉煌。</p><p>回过头来看Queen版本的一些主要变更，Mirantis的这篇文章[1] 总结的不错，</p><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>Queen版本之前，使用GPU做科学计算和机器学习，可以通过使用PCI pass through或者直接用Ironic来操作裸机，Queen版本增加了新的flavor，可以像其它flavor例如vCPUs一样来支持对GPU资源作出的请求。</p><h3 id="高可用性"><a href="#高可用性" class="headerlink" title="高可用性"></a>高可用性</h3><p>个人觉得这个特性对OpenStack的落地是个很大的提高，尤其是虚拟机层次的高可用性， 围绕这个特性，块存储以及裸机管理方面的相关功能也都值得期待。</p><h3 id="边缘计算"><a href="#边缘计算" class="headerlink" title="边缘计算"></a>边缘计算</h3><p>围绕边缘计算和容器化，我们可以看到新的项目如LOCI和OpenStack-Helm所作出的努力，OpenStack容器化已经有些年头，而一时间出来这么多和Container相关的项目也算是OpenStack受诟病的一个原因吧，能否多这么的项目做个整合，再比如早起的Kolla?</p><p>下个版本据说是要更加关注NFV了，祝OpenStack一路走好吧！</p><p>附原文：</p><p>OpenStack embraces the future with GPU, edge computing support</p><p>It wasn’t that long ago that OpenStack was the hot new kid on the infrastructure block. Lately, though, other technologies have been vying for that spot, making the open source cloud platform look downright stodgy in comparison. That just might change with the latest release of OpenStack, code-named Queens.</p><p>The Queens release makes it abundantly clear that the OpenStack community, far from resting on its laurels or burying its collective head in the digital sand, has been paying attention to what’s going on in the cloud space and adjusting its efforts accordingly. Queens includes capabilities that wouldn’t even have been possible when the OpenStack project started, let alone considered, such as GPU support (handy for scientific and machine learning/AI workloads) and a focus on Edge Computing that makes use of the current new kid on the block, Kubernetes.</p><p><strong>Optimization</strong></p><p>While OpenStack users have been able to utilize GPUs for scientific and machine learning purposes for some time, it has typically been through the use of either PCI passthrough or by using Ironic to manage an entire server as a single instance — neither of which was particularly convenient. Queens now makes it possible to provision virtual GPUs (vGPUs) using specific flavors, just as you would provision traditional vCPUs.</p><p>Queens also includes the debut of the Cyborg project, which provides a management framework for different types of accelerators such as GPUs, FPGA, NVMe/NOF, SSDs, DPDK, and so on. This capability is important not just for GPU-related use cases, but also for situations such as NFV.</p><p><strong>High Availability</strong></p><p>As OpenStack becomes more of an essential tool and less of a science project, the need for high availability has grown. The OpenStack Queens release addresses this need in several different ways.</p><p>The OpenStack Instances High Availability Service, or Masakari, provides an API to manage the automated rescue mechanism that recovers instances that fail because of process down, provisioning process down, or nova-compute host failure events.</p><p>While Masakari currently supports KVM-based VMs, Ironic bare metal nodes have always been more difficult to recover. Queens debuts the Ironic Rescue Mode (one of our favorite feature names of all time), which makes it possible to recover an Ironic node that has gone down.</p><p>Another way OpenStack Queens provides HA capabilities is through Cinder’s new volume multi-attach feature. The OpenStack Block Storage Service’s new capability makes it possible to attach a single volume to multiple VMs, so if one of those instances fails, traffic can be routed to an identical instance that is using the same storage.</p><p><strong>Edge Computing</strong></p><p>What’s become more than obvious, though, is that OpenStack has realized that the future doesn’t lay in just a few concentrated datacenters, but rather that workloads will be in a variety of diverse locations. Specifically, Edge Computing, in which we will see multiple smaller clouds closer to the user rather than a single centralized cloud, is coming into its own as service providers and others realize its importance.</p><p>To that end, OpenStack has been focused on several projects to adapt itself to that kind of environment, including LOCI and OpenStack-Helm.</p><p>OpenStack LOCI provides Lightweight OCI compatible images of OpenStack services so that they can be deployed by a container orchestration tool such as Kubernetes. As of the Queens release, images are available for Cinder, Glance, Heat, Horizon, Ironic, Keystone, Neutron and Nova.</p><p>And of course since orchestrating a containerized deployment of OpenStack isn’t necessarily any easier than deploying a non-containerized version, there’s OpenStack-Helm, a collection of Helm charts that install the various OpenStack services on a Kubernetes cluster.</p><p><strong>Other container-related advances</strong></p><p>If it seems like there’s a focus on integrating with container-based services, you’re right. Another way OpenStack has integrated with Kubernetes is through the Kuryr CNI plugin. The Container Network Interface (CNI) is a CNCF project that standardizes container networking operations, and the Kuryr CNI plugin makes it possible to use OpenStack Neutron within your Kubernetes cluster.</p><p>Also, if your container needs are more modest — maybe you don’t need an actual cluster, you just want the containers — the new Zun project makes it possible to run application containers on their own.</p><p><strong>Coming up next</strong></p><p>As always, it’s impossible to sum up 6 months of OpenStack work in a single blog post, but the general idea is that the OpenStack community is clearly thinking about the long term future and planning accordingly. While this release focused on making it possible to run OpenStack at the Edge, the next, code-named Rocky, will see a focus on NFV-related functionality such as minimum bandwidth requirements to ensure service quality.</p><p>What’s more, the community is also working on “mutable configuration across services”, which means that as we move into Intelligent Continuous Delivery (ICD) and potentially ever-changing and morphing infrastructure, we’ll be able to change service configurations without having to restart services.</p><hr><p>[1] <a href="https://www.mirantis.com/blog/openstack-embraces-the-future-with-gpu-edge-computing-support/" target="_blank" rel="noopener">https://www.mirantis.com/blog/openstack-embraces-the-future-with-gpu-edge-computing-support/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;https://raw.githubusercontent.com/chendave/initrepo/master/pic/banner.jpg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
&lt;p&gt;毫无疑问，OpenStack正在经历它的低谷期，和芸芸众生一样，无法改变世
      
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://jungler.com/tags/OpenStack/"/>
    
  </entry>
  
</feed>
